{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "import torch\n",
    "import math\n",
    "import random\n",
    "from model import gan\n",
    "from early_stopping import EarlyStopping\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "\n",
    "# Unique Naming\n",
    "from datetime import datetime\n",
    "import random, string\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_string(length=10):\n",
    "    \"\"\"\n",
    "        Generate a random string of given length. For safely storing produced images.\n",
    "    \"\"\"\n",
    "    letters = string.ascii_lowercase\n",
    "    return ''.join(random.choice(letters) for i in range(length))\n",
    "\n",
    "\n",
    "def get_model_id():\n",
    "    \"\"\"\n",
    "        Creates folder with unique ID in which everything related to a particular testrun can be saved.\n",
    "    :return: Unique folder identifier\n",
    "    \"\"\"\n",
    "    # Construct testrun identifier\n",
    "    TIME_STAMP = datetime.now().strftime(\"%Y_%d_%m__%H_%M_%S__%f_\")\n",
    "    model_folder_id = TIME_STAMP + '_' + random_string() + '/'\n",
    "\n",
    "    try:\n",
    "        os.makedirs(model_folder_id)\n",
    "    except Exception as e:\n",
    "        print('Exception occurred: ', e)\n",
    "\n",
    "    return model_folder_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "### VARIABLES & ADMINISTRATIVE STUFF ###\n",
    "# System\n",
    "#dataset_path = '/media/daniel/Elements/FastText_Data/'  # In case dataset is stored somewhere else, e.g. on hard-drive\n",
    "dataset_path = ''  # Data in same directory\n",
    "dictionary_path = ''  # Dictionaries in same directory\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "# Network\n",
    "embedding_dim = 300\n",
    "internal_dim = 300\n",
    "output_dim = 2\n",
    "\n",
    "# Train hyperparameters\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "vocab_size = 5000\n",
    "num_minibatches = vocab_size // batch_size\n",
    "real_label, fake_label = 1, 0\n",
    "languages = {'src': ['de', 'nl']\n",
    "             , 'trgt': ['en']}  # Target language to be indicated in last position\n",
    "checkpoint_frequency = 0  # 0 == Off; i > 0 == actual checkpoint frequency in epochs\n",
    "avg_grads = False  # Boolean indicating whether to average the grads of decoder & discriminator accumulated over nr of source languages by nr of source langs\n",
    "early_stop = False # Boolean indicating whether to stop early if loss won't decrease for a certain threshold\n",
    "eval_frequency = 10\n",
    "\n",
    "#testing parameters\n",
    "N = [1] # List of n nearest neighbors that will be performed in evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de\n",
      "nl\n",
      "en\n",
      "{'trgt': ['en'], 'src': ['de', 'nl']}\n"
     ]
    }
   ],
   "source": [
    "# Changed the way languages are stored. \n",
    "# For easy access to complete set of all included languages, just concatenate lists\n",
    "languages = {'src': ['de', 'nl'], 'trgt': ['en']}\n",
    "\n",
    "for lang in languages['src']+languages['trgt']:\n",
    "    print(lang)\n",
    "print(languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: ./2020_08_06__22_06_24__630394__qyzxfdpjtn/Final/\n",
      "Model ID: 2020_08_06__22_06_24__630394__qyzxfdpjtn/\n"
     ]
    }
   ],
   "source": [
    "# Set up saving paths\n",
    "data_storage_path = './'\n",
    "model_id = get_model_id()\n",
    "checkpoint_path = data_storage_path + model_id + 'Checkpoint/'\n",
    "final_state_path = data_storage_path + model_id + 'Final/'\n",
    "\n",
    "try:\n",
    "    if checkpoint_frequency > 0:\n",
    "        os.makedirs(checkpoint_path)\n",
    "        print('Created:', checkpoint_path)\n",
    "    os.makedirs(final_state_path)\n",
    "    print('Created:', final_state_path)\n",
    "except Exception as e:\n",
    "    raise Warning('Exception occurred: Cound not create dirs! Exception:', e)\n",
    "    \n",
    "print('Model ID:', model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_vocab(vocab):\n",
    "    # Returns the word embeddings and matching labels for the full vocabulary\n",
    "    words = vocab.words\n",
    "    vectors = [vocab[word] for word in words]\n",
    "    return vectors, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaned_vocab(vocab, vocab_size):\n",
    "    # Remove all punctuation tokens while valid nr of tokens is insufficient yet for having full vocab size\n",
    "    # TODO & possibly reserve testing vocab\n",
    "    # Return clean & restricted vocab\n",
    "    words = vocab.words[:vocab_size]              # Y (labels)\n",
    "    vects = [vocab[word] for word in words]       # X (input data)\n",
    "\n",
    "    return vects, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lang_to_vocab(lang_id, vocab_size, vocabs, full_vocabs):\n",
    "    # Get dataset\n",
    "    if dataset_path == './':\n",
    "        fasttext.util.download_model(lang_id)  # Download word embedding vector data if not available\n",
    "    vocab = fasttext.load_model(dataset_path + 'cc.' + lang_id + '.300.bin')  # Load language data\n",
    "\n",
    "    # Add train data (embedding-vectors) and labels (words) to vocab\n",
    "    X, Y = cleaned_vocab(vocab,500000)\n",
    "    x, y = cleaned_vocab(vocab, vocab_size)\n",
    "    vocabs[lang_id] = {'x': torch.tensor(x), 'y': y}\n",
    "    full_vocabs[lang_id] = {'X': X, 'Y': Y}\n",
    "\n",
    "    return vocabs, full_vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab(languages):\n",
    "    vocabs = {}\n",
    "    full_vocabs = {}\n",
    "    \n",
    "    for language in languages['src']+languages['trgt']:\n",
    "        vocabs, full_vocabs = add_lang_to_vocab(language, vocab_size, vocabs, full_vocabs)\n",
    "\n",
    "    print('Successfully loaded language models.')\n",
    "    return vocabs, full_vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded language models.\n"
     ]
    }
   ],
   "source": [
    "#load vocab (keep in independent cell for bugfixing purposes)\n",
    "vocabs, full_vocabs = load_vocab(languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_vocabs = {}\n",
    "source_full_vocabs = {}\n",
    "\n",
    "for source_language in languages['src']:\n",
    "    source_vocabs[source_language] = vocabs[source_language]\n",
    "    source_full_vocabs[source_language] = full_vocabs[source_language]\n",
    "target_full_vocabs = full_vocabs[languages['trgt'][0]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dictionary(dictionary_text):\n",
    "    # Converts an input dictionary text file to a python dictionary\n",
    "    dictionary = {}\n",
    "    source = True\n",
    "    source_word = ''\n",
    "    target_word = ''\n",
    "    \n",
    "    for character in dictionary_text:\n",
    "        if source is True:\n",
    "            if character is '\\t' or character is ' ':\n",
    "                source = False\n",
    "            else:\n",
    "                source_word = source_word + character\n",
    "        else:\n",
    "            if character is '\\n':\n",
    "                source = True\n",
    "                if source_word in dictionary:\n",
    "                    dictionary[source_word].append(target_word)\n",
    "                else:\n",
    "                    dictionary[source_word] = [target_word]\n",
    "                source_word = ''\n",
    "                target_word = ''\n",
    "            else:\n",
    "                target_word = target_word + character\n",
    "                \n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dictionaries(languages):\n",
    "    # Loads in the bilingual dictionaries\n",
    "    dictionaries = {}\n",
    "    \n",
    "    for source_language in languages['src']:\n",
    "        file = open(dictionary_path + source_language + '-' + languages['trgt'][0] + '.txt', 'r', errors='ignore')\n",
    "        dictionary_text = file.read()\n",
    "        dictionaries[source_language] = convert_dictionary(dictionary_text)\n",
    "    \n",
    "    return dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_translation_task(languages, source_full_vocabs, dictionaries):\n",
    "    # Creates a split in eval and training translation task\n",
    "    eval_words = {}\n",
    "    test_words = {}\n",
    "    \n",
    "    for source_language in languages['src']:\n",
    "        source_words = list(dictionaries[source_language].keys())\n",
    "#         random.shuffle(source_words)\n",
    "        eval_list = []\n",
    "        for source_word in source_words:\n",
    "            if source_word in source_full_vocabs[source_language]['Y']:\n",
    "                eval_list.append(source_word)\n",
    "            if len(eval_list) is 200:\n",
    "                eval_words[source_language] = eval_list\n",
    "                break\n",
    "#             eval_words[source_language] = source_words[0:50]\n",
    "#             test_words[source_language] = source_words[50:150]        \n",
    "#         eval_words[source_language] = source_words[0:int(len(source_words)/2)]\n",
    "#         test_words[source_language] = source_words[int(len(source_words)/2):len(source_words)]\n",
    "        \n",
    "    return eval_words, test_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in dictionaries (independent cell for bugfixing)\n",
    "dictionaries = load_dictionaries(languages)\n",
    "\n",
    "# split in train and evaluation\n",
    "eval_words, test_words = split_translation_task(languages, source_full_vocabs, dictionaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest neighbor fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_neighbors(N, languages, full_vocabs):\n",
    "    target_neighbors = {}\n",
    "    \n",
    "    for n in N:\n",
    "        target_neighbors[n] = NearestNeighbors(n_neighbors=n, metric='cosine').fit(full_vocabs[languages['trgt'][0]]['X'])\n",
    "        \n",
    "    return target_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = fit_neighbors(N, languages, full_vocabs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cosine(vector1, vector2):\n",
    "    # Computes the cosine simularity between two vectors\n",
    "    dot_product = np.dot(vector1, vector2)\n",
    "    norm_vector1 = np.linalg.norm(vector1)\n",
    "    norm_vector2 = np.linalg.norm(vector2)\n",
    "    return dot_product/(norm_vector1*norm_vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_translations(generator, language, source_vector, target_vocab, neighbors):\n",
    "    # Gets n possible translations, as given by the n nearest neighbors of the transformed\n",
    "    # source vector in the target embeddings space, we will use a package for this for optimization\n",
    "    # purposes. n is given in the nearest neighbor fitting stage.\n",
    "    #print(source_vector.numpy()[0])\n",
    "    transformed_source_embedding = generator(torch.as_tensor(source_vector), language).detach().numpy()\n",
    "\n",
    "    # only takes 2D arrays, hence the extra bracket [1][0] stands for select indices of\n",
    "    # the first input vector (the only one in this case)\n",
    "\n",
    "    vocab_indices = neighbors.kneighbors(np.array([transformed_source_embedding]))[1][0]\n",
    "    target_vectors = []\n",
    "    target_words = []\n",
    "    for index in vocab_indices:\n",
    "        target_vectors.append(target_vocab['X'][index])\n",
    "        target_words.append(target_vocab['Y'][index])\n",
    "\n",
    "    return target_vectors, target_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_translations_batch(generator, language, source_vectors, target_vocab, neighbors):\n",
    "    # Gets n possible translations, as given by the n nearest neighbors of the transformed\n",
    "    # source vector in the target embeddings space, we will use a package for this for optimization\n",
    "    # purposes. n is given in the nearest neighbor fitting stage.\n",
    "    #print(source_vector.numpy()[0])\n",
    "    transformed_source_embedding = generator(torch.as_tensor(source_vectors), language).detach().numpy()\n",
    "\n",
    "    # only takes 2D arrays, hence the extra bracket [1][0] stands for select indices of\n",
    "    # the first input vector (the only one in this case)\n",
    "\n",
    "    vocab_indices = neighbors.kneighbors(np.array(transformed_source_embedding))[1]\n",
    "    target_vectors = []\n",
    "    target_words = []\n",
    "    for target_indices in vocab_indices:\n",
    "        vectors = []\n",
    "        words = []\n",
    "        for index in target_indices:\n",
    "            vectors.append(target_vocab['X'][index])\n",
    "            words.append(target_vocab['Y'][index])\n",
    "        target_vectors.append(vectors)\n",
    "        target_words.append(words)\n",
    "\n",
    "    return target_vectors, target_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_cosine(generator, language, source_word_vectors, target_vocab, neighbors):\n",
    "    # Computes the average cosine simularity between the source words and their translations\n",
    "    sum_of_cosines = 0\n",
    "    for source_word_vector in source_word_vectors[:30]:\n",
    "        translated_word_vector = get_n_translations(generator, language, source_word_vector, target_vocab, neighbors[1])[0][0]\n",
    "        sum_of_cosines += compute_cosine(source_word_vector, translated_word_vector)\n",
    "    return sum_of_cosines/len(source_word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_cosine_batch(generator, language, source_word_vectors, target_vocab, neighbors):\n",
    "    # Computes the average cosine simularity between the source words and their translations\n",
    "    sum_of_cosines = 0\n",
    "    translated_word_vectors = get_n_translations_batch(generator, language, source_word_vectors, target_vocab, neighbors[1])[0]\n",
    "    for source_word_vector, translated_word_vector in zip(source_word_vectors, translated_word_vectors):\n",
    "        sum_of_cosines += compute_cosine(source_word_vector, translated_word_vector[0])\n",
    "    return sum_of_cosines/len(source_word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_translation_accuracy(generator, language, source_words, source_vocab, target_vocab, dictionary, neighbors):\n",
    "    # Compute the accuracy of translation over the given set of source words\n",
    "    correct_translations = 0\n",
    "    for source_word in source_words:\n",
    "        source_word_index = source_vocab['Y'].index(source_word)\n",
    "        source_word_vector = source_vocab['X'][source_word_index]\n",
    "        n_target_words = get_n_translations(generator, language, source_word_vector, target_vocab, neighbors)[1]\n",
    "        for target_word in n_target_words:\n",
    "            if target_word in dictionary[source_word]:\n",
    "                correct_translations += 1\n",
    "                break\n",
    "    return correct_translations/len(source_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_translation_accuracy_batch(generator, language, source_words, source_vocab, target_vocab, dictionary, neighbors):\n",
    "    # Compute the accuracy of translation over the given set of source words\n",
    "    correct_translations = 0\n",
    "    source_word_vectors = []\n",
    "    for source_word in source_words:\n",
    "        source_word_index = source_vocab['Y'].index(source_word)\n",
    "        source_word_vectors.append(source_vocab['X'][source_word_index])\n",
    "    target_words = get_n_translations_batch(generator, language, source_word_vectors, target_vocab, neighbors)[1]\n",
    "    for n_target_words in target_words:\n",
    "        for target_word in n_target_words:\n",
    "            if target_word in dictionary[source_word]:\n",
    "                correct_translations += 1\n",
    "                break\n",
    "    return correct_translations/len(source_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(generator, languages, source_training_vocabs, source_eval_words, source_full_vocabs, target_full_vocabs, dictionaries, neighbors, N):\n",
    "    # Evaluates the current model by using both an unsupervised cosine similiraty metric and a \n",
    "    # supervised translation accuracy metric. We have included both to see how they compare.\n",
    "    for source_language in languages['src']:\n",
    "        cosine_metric =  get_average_cosine_batch(generator, source_language, source_training_vocabs[source_language]['x'], target_full_vocabs, neighbors) #experimental\n",
    "#         cosine_metric = ''\n",
    "        accuracy_text = 'accuracies are '\n",
    "        for n in N:\n",
    "            accuracy = get_translation_accuracy_batch(generator, source_language, source_eval_words[source_language], source_full_vocabs[source_language], target_full_vocabs, dictionaries[source_language], neighbors[n])\n",
    "            accuracy_text = str(accuracy_text) + 'p@' + str(n) + '=' + str(accuracy) + ', '\n",
    "        \n",
    "        print('evaluation of source language ' + source_language + ': average cosine=',cosine_metric, accuracy_text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(generator, languages, source_test_words, source_full_vocabs, target_full_vocabs, dictionaries, neighbors, N):\n",
    "    # Testing based on translation accuracy on testing set\n",
    "    for source_language in languages['src']:\n",
    "        accuracy_text = ''\n",
    "        for n in N:\n",
    "            accuracy = get_translation_accuracy_batch(generator, source_language, source_test_words[source_language], source_full_vocabs[source_language], target_full_vocabs, dictionaries[source_language], neighbors[n])\n",
    "            accuracy_text = accuracy_text + 'p@' + n + '=' + accuracy + ', '\n",
    "        \n",
    "        print('Testing accuracies of source language ' + source_language + \": \" + accuracy_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define training related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(data, save):\n",
    "    if save:\n",
    "        torch.save(data, checkpoint_path + 'checkpoint_%d.pt' % data['epoch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_param(model):\n",
    "    return torch.mean(torch.cat([param.data.view(-1) for param in model.parameters()], 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_sample(lang, vocab, batch_size, include_y=False):\n",
    "    \"\"\"\n",
    "    This function draws batch_size-many training samples at random \n",
    "    from a vocab corresponding to queried language.  \n",
    "    \"\"\"\n",
    "    indices = torch.LongTensor(batch_size).random_(0, len(vocab))\n",
    "    if include_y:\n",
    "        return vocab['x'][indices], vocab['y'][indices]\n",
    "    return vocab['x'][indices]\n",
    "\n",
    "\n",
    "def get_train_data(languages, vocabs, batch_size, include_y=False):\n",
    "    \"\"\"\n",
    "    Returns one set of samples datapoints form a vocabulary for each provided language.\n",
    "    \"\"\"\n",
    "    x, y = {}, {}\n",
    "    \n",
    "    # Source languages\n",
    "    for lang in languages['src']+languages['trgt']:\n",
    "        if include_y:\n",
    "            x[lang], y[lang] = get_dataset_sample(lang, vocabs[lang], batch_size, include_y)\n",
    "        else:\n",
    "            x[lang] = get_dataset_sample(lang, vocabs[lang], batch_size)\n",
    "    \n",
    "    # Return\n",
    "    if include_y:\n",
    "        return x, y\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For debugging - Compute sum of abs(gradients) of model\n",
    "def get_summed_abs_grads(model):\n",
    "#     summed_abs = torch.tensor(0)\n",
    "    summed_abs = 0\n",
    "    for p in model.parameters():\n",
    "        summed_abs += torch.sum(torch.abs(p))\n",
    "    return summed_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nr source languages: 2\n",
      "Nr target languages: 1\n",
      "\n",
      " {'trgt': ['en'], 'src': ['de', 'nl']}\n",
      "Epoch  0 / 100\n",
      "Progress:  1838.4152 13.593496\n",
      "Summed abs weights Generator: 3923.126\n",
      "Summed abs weights Discrimi.: 1305.7596\n",
      "Epoch  1 / 100\n",
      "Progress:  1774.3591 1.6182866\n",
      "Summed abs weights Generator: 3776.2253\n",
      "Summed abs weights Discrimi.: 1376.2877\n",
      "Epoch  2 / 100\n",
      "Progress:  2583.4265 6.666801\n",
      "Summed abs weights Generator: 3823.8213\n",
      "Summed abs weights Discrimi.: 2498.179\n",
      "Epoch  3 / 100\n",
      "Progress:  1981.512 7.7556477\n",
      "Summed abs weights Generator: 3792.2073\n",
      "Summed abs weights Discrimi.: 2246.0012\n",
      "Epoch  4 / 100\n",
      "Progress:  1726.8434 6.4076304\n",
      "Summed abs weights Generator: 3773.465\n",
      "Summed abs weights Discrimi.: 2400.402\n",
      "Epoch  5 / 100\n",
      "Progress:  1740.5248 4.093757\n",
      "Summed abs weights Generator: 3775.1765\n",
      "Summed abs weights Discrimi.: 1907.8389\n",
      "Epoch  6 / 100\n",
      "Progress:  2038.2948 19.252428\n",
      "Summed abs weights Generator: 3822.95\n",
      "Summed abs weights Discrimi.: 2515.041\n",
      "Epoch  7 / 100\n",
      "Progress:  1826.5573 15.824627\n",
      "Summed abs weights Generator: 3818.5527\n",
      "Summed abs weights Discrimi.: 2238.8826\n",
      "Epoch  8 / 100\n",
      "Progress:  2620.1257 9.951024\n",
      "Summed abs weights Generator: 3805.566\n",
      "Summed abs weights Discrimi.: 1992.478\n",
      "Epoch  9 / 100\n",
      "Progress:  1660.0258 10.551536\n",
      "Summed abs weights Generator: 3802.7356\n",
      "Summed abs weights Discrimi.: 2484.5222\n",
      "Epoch  10 / 100\n",
      "Progress:  1809.6893 16.65757\n",
      "Summed abs weights Generator: 3818.2559\n",
      "Summed abs weights Discrimi.: 2579.8462\n",
      "evaluation of source language de: average cosine= 0.031911430115181064 accuracies are p@1=0.0, \n",
      "evaluation of source language nl: average cosine= 0.01127453315983039 accuracies are p@1=0.0, \n",
      "Epoch  11 / 100\n",
      "Progress:  1928.1984 15.565712\n",
      "Summed abs weights Generator: 3845.1675\n",
      "Summed abs weights Discrimi.: 2733.4055\n",
      "Epoch  12 / 100\n",
      "Progress:  2014.8289 12.257996\n",
      "Summed abs weights Generator: 3847.7148\n",
      "Summed abs weights Discrimi.: 2575.873\n",
      "Epoch  13 / 100\n",
      "Progress:  1846.5966 14.814937\n",
      "Summed abs weights Generator: 3843.254\n",
      "Summed abs weights Discrimi.: 2569.953\n",
      "Epoch  14 / 100\n",
      "Progress:  1652.2026 9.276005\n",
      "Summed abs weights Generator: 3833.9846\n",
      "Summed abs weights Discrimi.: 2293.897\n",
      "Epoch  15 / 100\n",
      "Progress:  1911.7905 10.409842\n",
      "Summed abs weights Generator: 3840.836\n",
      "Summed abs weights Discrimi.: 2162.1262\n",
      "Epoch  16 / 100\n",
      "Progress:  1917.3928 29.140596\n",
      "Summed abs weights Generator: 3860.7798\n",
      "Summed abs weights Discrimi.: 2241.6438\n",
      "Epoch  17 / 100\n",
      "Progress:  1343.7264 37.128212\n",
      "Summed abs weights Generator: 3845.9348\n",
      "Summed abs weights Discrimi.: 1918.6315\n",
      "Epoch  18 / 100\n",
      "Progress:  1388.0072 19.135475\n",
      "Summed abs weights Generator: 3832.0964\n",
      "Summed abs weights Discrimi.: 1958.0801\n",
      "Epoch  19 / 100\n",
      "Progress:  1281.3944 8.989563\n",
      "Summed abs weights Generator: 3808.6968\n",
      "Summed abs weights Discrimi.: 1765.3644\n",
      "Epoch  20 / 100\n",
      "Progress:  1148.0413 13.597761\n",
      "Summed abs weights Generator: 3782.966\n",
      "Summed abs weights Discrimi.: 1806.0411\n",
      "evaluation of source language de: average cosine= 0.031911430115181064 accuracies are p@1=0.0, \n",
      "evaluation of source language nl: average cosine= 0.01127453315983039 accuracies are p@1=0.0, \n",
      "Epoch  21 / 100\n",
      "Progress:  1403.7579 4.701895\n",
      "Summed abs weights Generator: 3753.9749\n",
      "Summed abs weights Discrimi.: 1529.737\n",
      "Epoch  22 / 100\n",
      "Progress:  1144.8677 29.391954\n",
      "Summed abs weights Generator: 3726.0676\n",
      "Summed abs weights Discrimi.: 1674.9623\n",
      "Epoch  23 / 100\n",
      "Progress:  1168.9547 24.987154\n",
      "Summed abs weights Generator: 3692.8508\n",
      "Summed abs weights Discrimi.: 1516.6677\n",
      "Epoch  24 / 100\n",
      "Progress:  764.5781 40.508198\n",
      "Summed abs weights Generator: 3657.6426\n",
      "Summed abs weights Discrimi.: 1478.7905\n",
      "Epoch  25 / 100\n",
      "Progress:  1120.6494 22.909904\n",
      "Summed abs weights Generator: 3621.962\n",
      "Summed abs weights Discrimi.: 1728.4601\n",
      "Epoch  26 / 100\n",
      "Progress:  715.14 42.61738\n",
      "Summed abs weights Generator: 3582.3489\n",
      "Summed abs weights Discrimi.: 1576.1793\n",
      "Epoch  27 / 100\n",
      "Progress:  840.41437 38.975807\n",
      "Summed abs weights Generator: 3540.9001\n",
      "Summed abs weights Discrimi.: 1381.74\n",
      "Epoch  28 / 100\n",
      "Progress:  679.54675 48.234165\n",
      "Summed abs weights Generator: 3498.5176\n",
      "Summed abs weights Discrimi.: 1135.7415\n",
      "Epoch  29 / 100\n",
      "Progress:  609.15485 62.771942\n",
      "Summed abs weights Generator: 3464.062\n",
      "Summed abs weights Discrimi.: 1257.6976\n",
      "Epoch  30 / 100\n",
      "Progress:  506.26636 63.88976\n",
      "Summed abs weights Generator: 3414.993\n",
      "Summed abs weights Discrimi.: 882.2947\n",
      "evaluation of source language de: average cosine= 0.031911430115181064 accuracies are p@1=0.0, \n",
      "evaluation of source language nl: average cosine= 0.01127453315983039 accuracies are p@1=0.0, \n",
      "Epoch  31 / 100\n",
      "Progress:  591.664 60.211857\n",
      "Summed abs weights Generator: 3371.2385\n",
      "Summed abs weights Discrimi.: 898.4014\n",
      "Epoch  32 / 100\n",
      "Progress:  649.8277 61.782387\n",
      "Summed abs weights Generator: 3336.1401\n",
      "Summed abs weights Discrimi.: 1387.0093\n",
      "Epoch  33 / 100\n",
      "Progress:  1109.6576 50.018883\n",
      "Summed abs weights Generator: 3305.9844\n",
      "Summed abs weights Discrimi.: 1617.6305\n",
      "Epoch  34 / 100\n",
      "Progress:  514.43317 66.37609\n",
      "Summed abs weights Generator: 3249.6516\n",
      "Summed abs weights Discrimi.: 904.97296\n",
      "Epoch  35 / 100\n",
      "Progress:  534.4376 59.938198\n",
      "Summed abs weights Generator: 3189.383\n",
      "Summed abs weights Discrimi.: 739.04193\n",
      "Epoch  36 / 100\n",
      "Progress:  493.3128 63.844936\n",
      "Summed abs weights Generator: 3126.7075\n",
      "Summed abs weights Discrimi.: 642.0377\n",
      "Epoch  37 / 100\n",
      "Progress:  521.77875 61.176266\n",
      "Summed abs weights Generator: 3061.865\n",
      "Summed abs weights Discrimi.: 564.6491\n",
      "Epoch  38 / 100\n",
      "Progress:  501.46008 64.03519\n",
      "Summed abs weights Generator: 2994.299\n",
      "Summed abs weights Discrimi.: 417.1201\n",
      "Epoch  39 / 100\n",
      "Progress:  484.58685 68.038506\n",
      "Summed abs weights Generator: 2937.3198\n",
      "Summed abs weights Discrimi.: 593.4128\n",
      "Epoch  40 / 100\n",
      "Progress:  598.80225 63.20503\n",
      "Summed abs weights Generator: 2870.415\n",
      "Summed abs weights Discrimi.: 950.5866\n",
      "evaluation of source language de: average cosine= 0.031911430115181064 accuracies are p@1=0.0, \n",
      "evaluation of source language nl: average cosine= 0.01127453315983039 accuracies are p@1=0.0, \n",
      "Epoch  41 / 100\n",
      "Progress:  489.83508 66.355896\n",
      "Summed abs weights Generator: 2798.3918\n",
      "Summed abs weights Discrimi.: 388.76517\n",
      "Epoch  42 / 100\n",
      "Progress:  484.71936 66.19836\n",
      "Summed abs weights Generator: 2721.7544\n",
      "Summed abs weights Discrimi.: 331.39563\n",
      "Epoch  43 / 100\n",
      "Progress:  529.4713 67.77607\n",
      "Summed abs weights Generator: 2647.484\n",
      "Summed abs weights Discrimi.: 1142.8774\n",
      "Epoch  44 / 100\n",
      "Progress:  481.86456 67.08123\n",
      "Summed abs weights Generator: 2570.7058\n",
      "Summed abs weights Discrimi.: 430.2936\n",
      "Epoch  45 / 100\n",
      "Progress:  481.24573 66.55105\n",
      "Summed abs weights Generator: 2496.4265\n",
      "Summed abs weights Discrimi.: 313.92294\n",
      "Epoch  46 / 100\n",
      "Progress:  552.53625 65.63446\n",
      "Summed abs weights Generator: 2422.6052\n",
      "Summed abs weights Discrimi.: 824.62976\n",
      "Epoch  47 / 100\n",
      "Progress:  675.9046 57.35755\n",
      "Summed abs weights Generator: 2362.2954\n",
      "Summed abs weights Discrimi.: 1110.0359\n",
      "Epoch  48 / 100\n",
      "Progress:  580.2847 62.111176\n",
      "Summed abs weights Generator: 2292.73\n",
      "Summed abs weights Discrimi.: 774.5249\n",
      "Epoch  49 / 100\n",
      "Progress:  510.2726 64.48597\n",
      "Summed abs weights Generator: 2208.9324\n",
      "Summed abs weights Discrimi.: 465.53674\n",
      "Epoch  50 / 100\n",
      "Progress:  509.68497 62.556393\n",
      "Summed abs weights Generator: 2124.463\n",
      "Summed abs weights Discrimi.: 423.97522\n",
      "evaluation of source language de: average cosine= 0.031911430115181064 accuracies are p@1=0.0, \n",
      "evaluation of source language nl: average cosine= 0.01127453315983039 accuracies are p@1=0.0, \n",
      "Epoch  51 / 100\n",
      "Progress:  487.77252 65.51386\n",
      "Summed abs weights Generator: 2047.1582\n",
      "Summed abs weights Discrimi.: 291.8024\n",
      "Epoch  52 / 100\n",
      "Progress:  493.76874 65.58101\n",
      "Summed abs weights Generator: 1967.6963\n",
      "Summed abs weights Discrimi.: 299.66452\n",
      "Epoch  53 / 100\n",
      "Progress:  512.2073 65.37497\n",
      "Summed abs weights Generator: 1885.2206\n",
      "Summed abs weights Discrimi.: 296.62683\n",
      "Epoch  54 / 100\n",
      "Progress:  489.3151 65.25238\n",
      "Summed abs weights Generator: 1800.9744\n",
      "Summed abs weights Discrimi.: 260.5321\n",
      "Epoch  55 / 100\n",
      "Progress:  489.2938 65.40835\n",
      "Summed abs weights Generator: 1717.4795\n",
      "Summed abs weights Discrimi.: 244.03206\n",
      "Epoch  56 / 100\n",
      "Progress:  487.70312 65.7752\n",
      "Summed abs weights Generator: 1634.9839\n",
      "Summed abs weights Discrimi.: 231.19794\n",
      "Epoch  57 / 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:  581.96564 65.05638\n",
      "Summed abs weights Generator: 1562.0239\n",
      "Summed abs weights Discrimi.: 649.3232\n",
      "Epoch  58 / 100\n",
      "Progress:  749.11273 64.20072\n",
      "Summed abs weights Generator: 1501.6559\n",
      "Summed abs weights Discrimi.: 1231.4569\n",
      "Epoch  59 / 100\n",
      "Progress:  567.41 63.02485\n",
      "Summed abs weights Generator: 1437.6227\n",
      "Summed abs weights Discrimi.: 820.7553\n",
      "Epoch  60 / 100\n",
      "Progress:  615.443 56.871536\n",
      "Summed abs weights Generator: 1371.3588\n",
      "Summed abs weights Discrimi.: 700.82947\n",
      "evaluation of source language de: average cosine= 0.031911430115181064 accuracies are p@1=0.0, \n",
      "evaluation of source language nl: average cosine= 0.01127453315983039 accuracies are p@1=0.0, \n",
      "Epoch  61 / 100\n",
      "Progress:  960.5567 53.764225\n",
      "Summed abs weights Generator: 1307.7753\n",
      "Summed abs weights Discrimi.: 821.4462\n",
      "Epoch  62 / 100\n",
      "Progress:  553.4995 58.203926\n",
      "Summed abs weights Generator: 1238.1863\n",
      "Summed abs weights Discrimi.: 618.1297\n",
      "Epoch  63 / 100\n",
      "Progress:  785.2429 59.83203\n",
      "Summed abs weights Generator: 1173.3304\n",
      "Summed abs weights Discrimi.: 914.4773\n",
      "Epoch  64 / 100\n",
      "Progress:  886.3983 58.64493\n",
      "Summed abs weights Generator: 1119.2582\n",
      "Summed abs weights Discrimi.: 1064.0912\n",
      "Epoch  65 / 100\n",
      "Progress:  2074.9277 60.327198\n",
      "Summed abs weights Generator: 1224.8336\n",
      "Summed abs weights Discrimi.: 1440.7278\n",
      "Epoch  66 / 100\n",
      "Progress:  2698.5461 49.145035\n",
      "Summed abs weights Generator: 1220.5347\n",
      "Summed abs weights Discrimi.: 950.6825\n",
      "Epoch  67 / 100\n",
      "Progress:  2546.391 113.991356\n",
      "Summed abs weights Generator: 1538.5005\n",
      "Summed abs weights Discrimi.: 1072.803\n",
      "Epoch  68 / 100\n",
      "Progress:  2480.4888 56.779392\n",
      "Summed abs weights Generator: 1504.4646\n",
      "Summed abs weights Discrimi.: 1213.1677\n",
      "Epoch  69 / 100\n",
      "Progress:  2643.9304 50.253628\n",
      "Summed abs weights Generator: 1528.352\n",
      "Summed abs weights Discrimi.: 984.15454\n",
      "Epoch  70 / 100\n",
      "Progress:  2074.0562 53.023853\n",
      "Summed abs weights Generator: 1496.8104\n",
      "Summed abs weights Discrimi.: 871.6265\n",
      "evaluation of source language de: average cosine= 0.009199678044636585 accuracies are p@1=0.0, \n",
      "evaluation of source language nl: average cosine= 0.006679684952684511 accuracies are p@1=0.0, \n",
      "Epoch  71 / 100\n",
      "Progress:  1598.9581 56.949223\n",
      "Summed abs weights Generator: 1503.0516\n",
      "Summed abs weights Discrimi.: 903.928\n",
      "Epoch  72 / 100\n",
      "Progress:  1314.1682 60.15369\n",
      "Summed abs weights Generator: 1475.9076\n",
      "Summed abs weights Discrimi.: 878.62494\n",
      "Epoch  73 / 100\n",
      "Progress:  547.8039 63.475315\n",
      "Summed abs weights Generator: 1457.5984\n",
      "Summed abs weights Discrimi.: 766.36926\n",
      "Epoch  74 / 100\n",
      "Progress:  529.9582 61.155666\n",
      "Summed abs weights Generator: 1435.1154\n",
      "Summed abs weights Discrimi.: 637.6526\n",
      "Epoch  75 / 100\n",
      "Progress:  562.01465 63.316246\n",
      "Summed abs weights Generator: 1413.4635\n",
      "Summed abs weights Discrimi.: 680.84393\n",
      "Epoch  76 / 100\n",
      "Progress:  494.39374 63.91974\n",
      "Summed abs weights Generator: 1385.0464\n",
      "Summed abs weights Discrimi.: 537.20856\n",
      "Epoch  77 / 100\n",
      "Progress:  797.9228 67.89206\n",
      "Summed abs weights Generator: 1368.4326\n",
      "Summed abs weights Discrimi.: 909.2399\n",
      "Epoch  78 / 100\n",
      "Progress:  764.8169 46.742207\n",
      "Summed abs weights Generator: 1340.1862\n",
      "Summed abs weights Discrimi.: 779.9913\n",
      "Epoch  79 / 100\n",
      "Progress:  931.56934 64.49629\n",
      "Summed abs weights Generator: 1319.8875\n",
      "Summed abs weights Discrimi.: 1037.9375\n",
      "Epoch  80 / 100\n",
      "Progress:  483.91144 65.71596\n",
      "Summed abs weights Generator: 1289.8236\n",
      "Summed abs weights Discrimi.: 637.0529\n",
      "evaluation of source language de: average cosine= 0.01104283980103928 accuracies are p@1=0.0, \n",
      "evaluation of source language nl: average cosine= 0.006796774744187497 accuracies are p@1=0.0, \n",
      "Epoch  81 / 100\n",
      "Progress:  492.29776 63.690044\n",
      "Summed abs weights Generator: 1259.1827\n",
      "Summed abs weights Discrimi.: 553.7588\n",
      "Epoch  82 / 100\n",
      "Progress:  502.5488 63.702023\n",
      "Summed abs weights Generator: 1228.4431\n",
      "Summed abs weights Discrimi.: 365.72034\n",
      "Epoch  83 / 100\n",
      "Progress:  553.672 72.413635\n",
      "Summed abs weights Generator: 1206.1177\n",
      "Summed abs weights Discrimi.: 518.14996\n",
      "Epoch  84 / 100\n",
      "Progress:  510.1134 62.858475\n",
      "Summed abs weights Generator: 1176.0027\n",
      "Summed abs weights Discrimi.: 401.6442\n",
      "Epoch  85 / 100\n",
      "Progress:  690.86 53.39133\n",
      "Summed abs weights Generator: 1144.214\n",
      "Summed abs weights Discrimi.: 545.1567\n",
      "Epoch  86 / 100\n",
      "Progress:  651.81616 59.73604\n",
      "Summed abs weights Generator: 1116.2153\n",
      "Summed abs weights Discrimi.: 582.91425\n",
      "Epoch  87 / 100\n",
      "Progress:  606.95984 57.48167\n",
      "Summed abs weights Generator: 1085.8165\n",
      "Summed abs weights Discrimi.: 526.58093\n",
      "Epoch  88 / 100\n",
      "Progress:  646.01636 48.832085\n",
      "Summed abs weights Generator: 1052.9287\n",
      "Summed abs weights Discrimi.: 601.30396\n",
      "Epoch  89 / 100\n",
      "Progress:  678.98444 52.06239\n",
      "Summed abs weights Generator: 1020.33276\n",
      "Summed abs weights Discrimi.: 537.6923\n",
      "Epoch  90 / 100\n",
      "Progress:  730.0027 61.87201\n",
      "Summed abs weights Generator: 994.1432\n",
      "Summed abs weights Discrimi.: 650.94727\n",
      "evaluation of source language de: average cosine= 0.012988728314082891 accuracies are p@1=0.0, \n",
      "evaluation of source language nl: average cosine= 0.007044971942427583 accuracies are p@1=0.0, \n",
      "Epoch  91 / 100\n",
      "Progress:  608.05963 59.989582\n",
      "Summed abs weights Generator: 968.5088\n",
      "Summed abs weights Discrimi.: 594.78485\n",
      "Epoch  92 / 100\n",
      "Progress:  637.2977 61.46615\n",
      "Summed abs weights Generator: 939.1504\n",
      "Summed abs weights Discrimi.: 703.2338\n",
      "Epoch  93 / 100\n",
      "Progress:  1090.2484 46.93524\n",
      "Summed abs weights Generator: 927.4981\n",
      "Summed abs weights Discrimi.: 1287.3367\n",
      "Epoch  94 / 100\n",
      "Progress:  676.2496 59.58734\n",
      "Summed abs weights Generator: 908.6027\n",
      "Summed abs weights Discrimi.: 1092.6455\n",
      "Epoch  95 / 100\n",
      "Progress:  626.70605 52.386356\n",
      "Summed abs weights Generator: 884.55963\n",
      "Summed abs weights Discrimi.: 960.9204\n",
      "Epoch  96 / 100\n",
      "Progress:  852.1151 37.3482\n",
      "Summed abs weights Generator: 856.84753\n",
      "Summed abs weights Discrimi.: 1062.6212\n",
      "Epoch  97 / 100\n",
      "Progress:  1175.0371 21.45472\n",
      "Summed abs weights Generator: 825.0933\n",
      "Summed abs weights Discrimi.: 905.3805\n",
      "Epoch  98 / 100\n",
      "Progress:  715.613 52.5008\n",
      "Summed abs weights Generator: 812.187\n",
      "Summed abs weights Discrimi.: 993.0642\n",
      "Epoch  99 / 100\n",
      "Progress:  555.35144 64.06006\n",
      "Summed abs weights Generator: 788.34015\n",
      "Summed abs weights Discrimi.: 697.2616\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \n",
    "    importlib.reload(gan)\n",
    "    \n",
    "    NLLLoss = torch.nn.NLLLoss()\n",
    "    nr_src_langs = len(languages['src'])\n",
    "    nr_trgt_langs = len(languages['trgt'])\n",
    "    nr_langs = nr_src_langs + nr_trgt_langs\n",
    "    print('Nr source languages:', nr_src_langs)\n",
    "    print('Nr target languages:', len(languages['trgt'])) \n",
    "    print('\\n', languages)\n",
    "    \n",
    "    if avg_grads:\n",
    "        avg_factor = 1/nr_src_langs\n",
    "        print('Decoder gradient averaging factor:', avg_factor, \"\\n\")\n",
    "    \n",
    "    # Get bilingual dictionary for evaluating train loss or at least testing\n",
    "    dicts = dict()\n",
    "    #TODO\n",
    "\n",
    "    # Set up model architecture\n",
    "    net = gan.GAN(embedding_dim, internal_dim, output_dim, languages['src'])\n",
    "\n",
    "    # Get optimizers; 1 per source language of encoder and 1 for discriminator\n",
    "    optimizers = {'gen': {}}\n",
    "    for lang in languages['src']:\n",
    "        optimizers['gen'][lang] = torch.optim.Adam([{'params': net.generator.encoders[lang].parameters()},\n",
    "                                                    {'params': net.generator.decoder.parameters()}\n",
    "                                                   ],\n",
    "                                                    lr=0.0001, betas=(0.9, 0.999), eps=1e-08, \n",
    "                                                    weight_decay=0.01, amsgrad=False)\n",
    "    optimizers['dis'] = torch.optim.Adam(net.discriminator.parameters(),\n",
    "                                         lr=0.001, betas=(0.9, 0.999), eps=1e-08, \n",
    "                                         weight_decay=0.01, amsgrad=False)\n",
    "    \n",
    "    # Train\n",
    "    train_loss_gen, train_loss_dis = [], []\n",
    "    eval_loss = [] # TODO: To be populated...\n",
    "    last_loss = -1\n",
    "    \n",
    "    es = EarlyStopping(patience=10) #patience = amount of epochs the loss has to stop decreasing in a row for it to early stop\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch ', epoch, '/', epochs)\n",
    "        loss_gen, loss_dis = 0., 0.\n",
    "\n",
    "        # Train #\n",
    "        for batch in range(num_minibatches):\n",
    "            #print('Epoch ', epoch, ', Batch ', batch, '/', num_minibatches)\n",
    "            \n",
    "            # Update discriminator #\n",
    "            net.discriminator.train()\n",
    "            net.generator.eval()\n",
    "            net.discriminator.zero_grad()\n",
    "            \n",
    "            # Retrieve data\n",
    "            x = get_train_data(languages, vocabs, batch_size)#.to(device) \n",
    "\n",
    "            # Init data-storage\n",
    "            y_preds = torch.zeros([nr_langs*batch_size, 2])\n",
    "            y_true = torch.zeros([nr_langs*batch_size]).long()\n",
    "            \n",
    "            y_true[0:batch_size] = real_label  # First elements are target embeddings\n",
    "\n",
    "            \n",
    "            # All-real minibatch\n",
    "            x_real = x[languages['trgt'][0]]  # Extract all-real data\n",
    "            y_preds[0:batch_size] = net.discriminator(x_real)\n",
    "            \n",
    "            # All-fake minibatches - One minibatch per source language\n",
    "            for i, language in enumerate(languages['src']):\n",
    "                idx_from = batch_size*i+batch_size*nr_trgt_langs\n",
    "                idx_to = batch_size*(i+1)+batch_size*nr_trgt_langs\n",
    "                x_trans = net.generator(x[language], language)  # Generate fake data aka translate\n",
    "                y_preds[idx_from:idx_to] = net.discriminator(x_trans)\n",
    "            #print('Preds:', y_preds)\n",
    "            \n",
    "            # Loss proportional to discriminator's probability of correctly distinguishing TP and FP\n",
    "            loss = NLLLoss(torch.log(y_preds+0.0000001), y_true)  # NLLLoss needs log(prob_distribution); adding small amount to avoid log(0)\n",
    "            loss.backward()    # Compute gradients only for discriminator\n",
    "            loss_dis += loss\n",
    "            \n",
    "            # Weight update for discriminator\n",
    "            optimizers['dis'].step() \n",
    "\n",
    "            \n",
    "            # Update generator #\n",
    "            net.generator.train()\n",
    "            net.discriminator.eval()\n",
    "            net.generator.zero_grad()\n",
    "            \n",
    "            # Retrieve data\n",
    "            x = get_train_data(languages, vocabs, batch_size)#.to(device)\n",
    "            \n",
    "            # All-real minibatch\n",
    "            x_real = x[languages['trgt'][0]]  # Extract all-real data\n",
    "            y_true = torch.full((batch_size,), fake_label).long()#.to(device)  # Pretend true targets were fake\n",
    "            y_pred = net.discriminator(x_real)\n",
    "            # Loss proportional to discriminator's probability of misclassifying TP and FP\n",
    "            loss_real = NLLLoss(torch.log(y_pred+0.0000001), y_true)  # NLLLoss needs log(prob_distribution); adding small amount to avoid log(0)\n",
    "            \n",
    "            # All-fake minibatches - One minibatch per source language\n",
    "            y_true = torch.full((batch_size,), real_label).long()#.to(device) # Try to fool the discriminator\n",
    "            for language in languages['src']:\n",
    "                x_src = x[language]\n",
    "                x_trans = net.generator(x_src, language)\n",
    "                y_pred = net.discriminator(x_trans)\n",
    "                # Loss proportional to discriminator's probability of misclassifying TP and FP\n",
    "                loss = NLLLoss(torch.log(y_pred+0.0000001), y_true)# + loss_real  # Add loss for real-misclassification here\n",
    "                loss.backward()    # Compute gradients only for discriminator\n",
    "                loss_gen += loss\n",
    "            \n",
    "            # Perform weight updates\n",
    "            for language in languages['src']:\n",
    "                optimizers['gen'][language].step()\n",
    "        \n",
    "        # Document accumulated losses per epoch\n",
    "        train_loss_gen.append(loss_gen.detach().numpy())\n",
    "        train_loss_dis.append(loss_dis.detach().numpy())\n",
    "        \n",
    "        #print('Mean: ', mean_param(net.generator.decoder))\n",
    "        print('Progress: ', loss_gen.detach().numpy(), \n",
    "                            loss_dis.detach().numpy())\n",
    "        \n",
    "        print('Summed abs weights Generator:', get_summed_abs_grads(net.generator).detach().numpy())\n",
    "        print('Summed abs weights Discrimi.:', get_summed_abs_grads(net.discriminator).detach().numpy())\n",
    "        \n",
    "        # Evaluation step\n",
    "        if epoch > 0 and epoch % eval_frequency is 0:\n",
    "            evaluation(net.generator, languages, source_vocabs, eval_words, source_full_vocabs, target_full_vocabs, dictionaries, neighbors, N)\n",
    "        \n",
    "        if early_stop: # if early stopping is enabled or not\n",
    "            if es.step(loss_gen.detach()): # using the real loss of the generator for now, maybe use something else later? e.g. evaluation loss?\n",
    "                print('early stopping')\n",
    "                break  # early stop criterion is met, stop the loop now\n",
    "        \n",
    "        # Save checkpoints\n",
    "        #print(loss_real_total_g.detach().numpy(), loss_fake_total_g.detach().numpy())\n",
    "        \n",
    "#        save = checkpoint_frequency > 0 and epoch % checkpoint_frequency == 0 and \\\n",
    "#            last_loss > loss_real_total_g+loss_fake_total_g  # Provisional: save when loss of generator has improved\n",
    "#        last_loss = loss_real_total_g+loss_fake_total_g\n",
    "#        save_checkpoint({'epoch': epoch,\n",
    "#                         'model_state_dict': net.state_dict(),\n",
    "#                         'optimizer_state_dicts': \n",
    "#                             {**{lang: optimizers['gen'][lang].state_dict() for lang in languages['src']}, \n",
    "#                              **{languages['trgt'][0]: optimizers['dis']}\n",
    "#                            },\n",
    "#                         'losses': {'train_loss_real_d': train_loss_real_d[-1],\n",
    "#                                    'train_loss_fake_d': train_loss_fake_d[-1],\n",
    "#                                    'train_loss_real_g': train_loss_real_g[-1],\n",
    "#                                    'train_loss_fake_g': train_loss_fake_g[-1],},\n",
    "#                         }, save)\n",
    "\n",
    "    # Final testing\n",
    "#     testing(net.generator, languages, test_words, source_full_vocabs, target_full_vocabs, dictionaries, neighbors, N)\n",
    "    \n",
    "    # Store model\n",
    "    torch.save(net.state_dict(), final_state_path + 'final_model%d.pt' % epoch)\n",
    "    \n",
    "    # Some provision for final eval\n",
    "    evaluation(net.generator, languages, source_vocabs, eval_words, source_full_vocabs, target_full_vocabs, dictionaries, neighbors, N)\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    # execute only if run as a script\n",
    "    main()\n",
    "    print('Done.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
