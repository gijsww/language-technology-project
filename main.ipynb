{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import fasttext.util\n",
    "import torch\n",
    "import math\n",
    "from model import gan\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### VARIABLES & ADMINISTRATIVE STUFF ###\n",
    "# System\n",
    "#dataset_path = '/media/daniel/Elements/FastText_Data/'  # In case dataset is stored somewhere else, e.g. on hard-drive\n",
    "dataset_path = ''  #data in same directory\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "# Network\n",
    "embedding_dim = 300\n",
    "internal_dim = 300\n",
    "hidden = 300\n",
    "\n",
    "# Train hyperparameters\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "vocab_size = 2000\n",
    "num_minibatches = vocab_size // batch_size\n",
    "real_label, fake_label = 1, 0\n",
    "languages = {'src': ['de', 'nl'], 'trgt': 'en'}  # Target language to be indicated in last position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaned_vocab(vocab, vocab_size):\n",
    "    # Remove all punctuation tokens while valid nr of tokens is insufficient yet for having full vocab size\n",
    "    # TODO & possibly reserve testing vocab\n",
    "\n",
    "    # Return clean & restricted vocab\n",
    "    words = vocab.words[:vocab_size]              # Y (labels)\n",
    "    vects = [vocab[word] for word in words]       # X (input data)\n",
    "\n",
    "    return vects, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lang_to_vocab(lang_type, lang_id, vocab_size, vocabs):\n",
    "    # Get dataset\n",
    "    if dataset_path == './':\n",
    "        fasttext.util.download_model(lang_id)  # Download word embedding vector data if not available\n",
    "    vocab = fasttext.load_model(dataset_path + 'cc.' + lang_id + '.300.bin')  # Load language data\n",
    "\n",
    "    # Add train data (embedding-vectors) and labels (words) to vocab\n",
    "    x, y = cleaned_vocab(vocab, vocab_size)\n",
    "    vocabs[lang_type][lang_id] = {'x': torch.tensor(x), 'y': y}\n",
    "\n",
    "    return vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab(languages):\n",
    "    nr_src_langs = len(languages)\n",
    "    vocabs = {'src': {}, 'trgt': {}}\n",
    "    \n",
    "    for language in languages['src']:\n",
    "        vocabs = add_lang_to_vocab('src', language, vocab_size, vocabs)\n",
    "        \n",
    "    language = languages['trgt']\n",
    "    vocabs = add_lang_to_vocab('trgt', language, vocab_size, vocabs)\n",
    "\n",
    "    print('Successfully loaded language models.')\n",
    "    return vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # load vocab\n",
    "    vocabs = load_vocab(languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Get bilingual dictionary for evaluating train loss or at least testing\n",
    "    dicts = dict()\n",
    "    #TODO\n",
    "\n",
    "    # Set up model architecture\n",
    "    net = gan.GAN(embedding_dim, internal_dim, hidden, languages['src'])\n",
    "\n",
    "    # Get optimizers; 1 per source language and 1 for target language\n",
    "    optims_g = {}\n",
    "    for language in languages['src']:\n",
    "\n",
    "        #params = net.generator.encoders[language].parameters() + net.generator.decoder.parameters()\n",
    "        optims_g[language] = torch.optim.Adam([{'params': net.generator.encoders[language].parameters()},{'params': net.generator.decoder.parameters()}],lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "\n",
    "    optim_d = torch.optim.Adam(net.discriminator.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "\n",
    "    # Train\n",
    "    train_loss_real_d, train_loss_fake_d, train_loss_fake_g = [], [], []\n",
    "    eval_loss = [], []\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch ', epoch, '/', epochs)\n",
    "        loss_real_d_total, loss_fake_d_total, loss_fake_g_total = 0., 0., 0.\n",
    "\n",
    "        # Shuffle data #\n",
    "        # Source languages\n",
    "        for lang in languages['src']:\n",
    "            vocabs['src'][lang]['x'], vocabs['src'][lang]['y'] = shuffle(np.array(vocabs['src'][lang]['x']), np.array(vocabs['src'][lang]['y']))\n",
    "            vocabs['src'][lang]['x'] = torch.from_numpy(vocabs['src'][lang]['x'])\n",
    "            #vocabs['src'][lang]['y'] = torch.from_numpy(vocabs['src'][lang]['y'])\n",
    "        # Target language\n",
    "        lang = languages['trgt']\n",
    "        vocabs['trgt'][lang]['x'], vocabs['trgt'][lang]['y'] = shuffle(np.array(vocabs['trgt'][lang]['x']), np.array(vocabs['trgt'][lang]['y']))\n",
    "        vocabs['trgt'][lang]['x'] = torch.from_numpy(vocabs['trgt'][lang]['x'])\n",
    "\n",
    "        # Train #\n",
    "        for batch in range(num_minibatches):\n",
    "            print('Epoch ', epoch, ', Batch ', batch, '/', num_minibatches)\n",
    "\n",
    "            # Update discriminator #\n",
    "            # All-real minibatch\n",
    "            net.discriminator.zero_grad()\n",
    "            x = vocabs['trgt'][languages['trgt']]['x'][batch * batch_size:(batch + 1) * batch_size].to(device)\n",
    "            y_true = torch.full((batch_size,), real_label, device=device)\n",
    "            y_pred = net.discriminator(x)\n",
    "            loss_real = net.loss(y_pred, y_true, 'dis')\n",
    "            loss_real.backward()\n",
    "            loss_real_d_total += loss_real\n",
    "\n",
    "            # One minibatch per source language\n",
    "            translations = {}\n",
    "            loss_fake_batch_avg = 0.\n",
    "            for language in languages['src']:\n",
    "                # All-real minibatch\n",
    "                net.discriminator.zero_grad()\n",
    "                x = vocabs['src'][language]['x'][batch * batch_size:(batch + 1) * batch_size].to(device)\n",
    "                x = net.generator(x, language)\n",
    "                translations[language] = x\n",
    "                y_true = torch.full((batch_size,), fake_label, device=device)\n",
    "                y_pred = net.discriminator(x.detach())      # Detach to avoid computing grads for generator\n",
    "                loss_fake = net.loss(y_pred, y_true, 'dis')\n",
    "                loss_fake_batch_avg += loss_fake\n",
    "                loss_fake.backward()    # Compute gradients only for discriminator\n",
    "            optim_d.step()              # Weight update\n",
    "            loss_fake_d_total += (loss_fake_batch_avg/nr_src_langs)\n",
    "\n",
    "            # Update generator #\n",
    "            # Compute gradients\n",
    "            loss_fake_batch_avg = 0.\n",
    "            for language in languages['src']:\n",
    "                net.generator.encoders[language].zero_grad()\n",
    "                x = translations[language]\n",
    "                y_true = torch.full((batch_size,), fake_label, device=device)\n",
    "                y_pred = net.discriminator(x)\n",
    "                loss_fake = net.loss(y_pred, y_true, 'gen')\n",
    "                loss_fake_batch_avg += loss_fake\n",
    "                loss_fake.backward()\n",
    "            loss_fake_g_total += (loss_fake_batch_avg / nr_src_langs)\n",
    "\n",
    "            # TODO: possibly average decoder's gradients\n",
    "\n",
    "            # Perform weight updates\n",
    "            for language in languages['src']:\n",
    "                optims_g[language].step()\n",
    "\n",
    "        # Document accumulated losses per epoch\n",
    "        train_loss_real_d.append(loss_real_d_total)\n",
    "        train_loss_fake_d.append(loss_fake_d_total)\n",
    "        train_loss_fake_g.append(loss_fake_g_total)\n",
    "\n",
    "        # TODO: evaluation per epoch?\n",
    "\n",
    "    # TODO: Final evaluation\n",
    "\n",
    "    # TODO: Store model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # execute only if run as a script\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
