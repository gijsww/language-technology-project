{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "import torch\n",
    "import math\n",
    "import random\n",
    "from model import gan\n",
    "from early_stopping import EarlyStopping\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "\n",
    "# Unique Naming\n",
    "from datetime import datetime\n",
    "import random, string\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_string(length=10):\n",
    "    \"\"\"\n",
    "        Generate a random string of given length. For safely storing produced images.\n",
    "    \"\"\"\n",
    "    letters = string.ascii_lowercase\n",
    "    return ''.join(random.choice(letters) for i in range(length))\n",
    "\n",
    "\n",
    "def get_model_id():\n",
    "    \"\"\"\n",
    "        Creates folder with unique ID in which everything related to a particular testrun can be saved.\n",
    "    :return: Unique folder identifier\n",
    "    \"\"\"\n",
    "    # Construct testrun identifier\n",
    "    TIME_STAMP = datetime.now().strftime(\"%Y_%d_%m__%H_%M_%S__%f_\")\n",
    "    model_folder_id = TIME_STAMP + '_' + random_string() + '/'\n",
    "\n",
    "    try:\n",
    "        os.makedirs(model_folder_id)\n",
    "    except Exception as e:\n",
    "        print('Exception occurred: ', e)\n",
    "\n",
    "    return model_folder_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### VARIABLES & ADMINISTRATIVE STUFF ###\n",
    "# System\n",
    "#dataset_path = '/media/daniel/Elements/FastText_Data/'  # In case dataset is stored somewhere else, e.g. on hard-drive\n",
    "dataset_path = ''  # Data in same directory\n",
    "dictionary_path = ''  # Dictionaries in same directory\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "# Network\n",
    "embedding_dim = 300\n",
    "internal_dim = 300\n",
    "hidden = 300\n",
    "\n",
    "# Train hyperparameters\n",
    "epochs = 20\n",
    "batch_size = 32\n",
    "vocab_size = 5000\n",
    "num_minibatches = vocab_size // batch_size\n",
    "real_label, fake_label = 1, 0\n",
    "languages = {'src': ['de', 'nl'], 'trgt': ['en']}  # Target language to be indicated in last position\n",
    "checkpoint_frequency = 0  # 0 == Off; i > 0 == actual checkpoint frequency in epochs\n",
    "avg_grads = False  # Boolean indicating whether to average the grads of decoder & discriminator accumulated over nr of source languages by nr of source langs\n",
    "early_stop = False # Boolean indicating whether to stop early if loss won't decrease for a certain threshold\n",
    "\n",
    "#testing parameters\n",
    "N = [1] # List of n nearest neighbors that will be performed in evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de\n",
      "nl\n",
      "en\n",
      "{'src': ['de', 'nl'], 'trgt': ['en']}\n"
     ]
    }
   ],
   "source": [
    "# Changed the way languages are stored. \n",
    "# For easy access to complete set of all included languages, just concatenate lists\n",
    "languages = {'src': ['de', 'nl'], 'trgt': ['en']}\n",
    "\n",
    "for lang in languages['src']+languages['trgt']:\n",
    "    print(lang)\n",
    "print(languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: ./2020_08_06__13_17_46__172572__carddelpjp/Final/\n",
      "Model ID: 2020_08_06__13_17_46__172572__carddelpjp/\n"
     ]
    }
   ],
   "source": [
    "# Set up saving paths\n",
    "data_storage_path = './'\n",
    "model_id = get_model_id()\n",
    "checkpoint_path = data_storage_path + model_id + 'Checkpoint/'\n",
    "final_state_path = data_storage_path + model_id + 'Final/'\n",
    "\n",
    "try:\n",
    "    if checkpoint_frequency > 0:\n",
    "        os.makedirs(checkpoint_path)\n",
    "        print('Created:', checkpoint_path)\n",
    "    os.makedirs(final_state_path)\n",
    "    print('Created:', final_state_path)\n",
    "except Exception as e:\n",
    "    raise Warning('Exception occurred: Cound not create dirs! Exception:', e)\n",
    "    \n",
    "print('Model ID:', model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_vocab(vocab):\n",
    "    # Returns the word embeddings and matching labels for the full vocabulary\n",
    "    words = vocab.words\n",
    "    vectors = [vocab[word] for word in words]\n",
    "    return vectors, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaned_vocab(vocab, vocab_size):\n",
    "    # Remove all punctuation tokens while valid nr of tokens is insufficient yet for having full vocab size\n",
    "    # TODO & possibly reserve testing vocab\n",
    "    # Return clean & restricted vocab\n",
    "    words = vocab.words[:vocab_size]              # Y (labels)\n",
    "    vects = [vocab[word] for word in words]       # X (input data)\n",
    "\n",
    "    return vects, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lang_to_vocab(lang_id, vocab_size, vocabs, full_vocabs):\n",
    "    # Get dataset\n",
    "    if dataset_path == './':\n",
    "        fasttext.util.download_model(lang_id)  # Download word embedding vector data if not available\n",
    "    vocab = fasttext.load_model(dataset_path + 'cc.' + lang_id + '.300.bin')  # Load language data\n",
    "\n",
    "    # Add train data (embedding-vectors) and labels (words) to vocab\n",
    "    X, Y = cleaned_vocab(vocab,500000)\n",
    "    x, y = cleaned_vocab(vocab, vocab_size)\n",
    "    vocabs[lang_id] = {'x': torch.tensor(x), 'y': y}\n",
    "    full_vocabs[lang_id] = {'X': X, 'Y': Y}\n",
    "\n",
    "    return vocabs, full_vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab(languages):\n",
    "    vocabs = {}\n",
    "    full_vocabs = {}\n",
    "    \n",
    "    for language in languages['src']+languages['trgt']:\n",
    "        vocabs, full_vocabs = add_lang_to_vocab(language, vocab_size, vocabs, full_vocabs)\n",
    "\n",
    "    print('Successfully loaded language models.')\n",
    "    return vocabs, full_vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded language models.\n"
     ]
    }
   ],
   "source": [
    "#load vocab (keep in independent cell for bugfixing purposes)\n",
    "vocabs, full_vocabs = load_vocab(languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_vocabs = {}\n",
    "source_full_vocabs = {}\n",
    "\n",
    "for source_language in languages['src']:\n",
    "    source_vocabs[source_language] = vocabs[source_language]\n",
    "    source_full_vocabs[source_language] = full_vocabs[source_language]\n",
    "target_full_vocabs = full_vocabs[languages['trgt'][0]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dictionary(dictionary_text):\n",
    "    # Converts an input dictionary text file to a python dictionary\n",
    "    dictionary = {}\n",
    "    source = True\n",
    "    source_word = ''\n",
    "    target_word = ''\n",
    "    \n",
    "    for character in dictionary_text:\n",
    "        if source is True:\n",
    "            if character is '\\t' or character is ' ':\n",
    "                source = False\n",
    "            else:\n",
    "                source_word = source_word + character\n",
    "        else:\n",
    "            if character is '\\n':\n",
    "                source = True\n",
    "                if source_word in dictionary:\n",
    "                    dictionary[source_word].append(target_word)\n",
    "                else:\n",
    "                    dictionary[source_word] = [target_word]\n",
    "                source_word = ''\n",
    "                target_word = ''\n",
    "            else:\n",
    "                target_word = target_word + character\n",
    "                \n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dictionaries(languages):\n",
    "    # Loads in the bilingual dictionaries\n",
    "    dictionaries = {}\n",
    "    \n",
    "    for source_language in languages['src']:\n",
    "        file = open(dictionary_path + source_language + '-' + languages['trgt'][0] + '.txt', 'r', errors='ignore')\n",
    "        dictionary_text = file.read()\n",
    "        dictionaries[source_language] = convert_dictionary(dictionary_text)\n",
    "    \n",
    "    return dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_translation_task(languages, source_full_vocabs, dictionaries):\n",
    "    # Creates a split in eval and training translation task\n",
    "    eval_words = {}\n",
    "    test_words = {}\n",
    "    \n",
    "    for source_language in languages['src']:\n",
    "        source_words = list(dictionaries[source_language].keys())\n",
    "#         random.shuffle(source_words)\n",
    "        eval_list = []\n",
    "        for source_word in source_words:\n",
    "            if source_word in source_full_vocabs[source_language]['Y']:\n",
    "                eval_list.append(source_word)\n",
    "            if len(eval_list) is 200:\n",
    "                eval_words[source_language] = eval_list\n",
    "                print(\"succes\")\n",
    "                break\n",
    "#             eval_words[source_language] = source_words[0:50]\n",
    "#             test_words[source_language] = source_words[50:150]        \n",
    "#         eval_words[source_language] = source_words[0:int(len(source_words)/2)]\n",
    "#         test_words[source_language] = source_words[int(len(source_words)/2):len(source_words)]\n",
    "        \n",
    "    return eval_words, test_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "succes\n",
      "succes\n"
     ]
    }
   ],
   "source": [
    "#load in dictionaries (independent cell for bugfixing)\n",
    "dictionaries = load_dictionaries(languages)\n",
    "\n",
    "# split in train and evaluation\n",
    "eval_words, test_words = split_translation_task(languages, source_full_vocabs, dictionaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest neighbor fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_neighbors(N, languages, full_vocabs):\n",
    "    target_neighbors = {}\n",
    "    \n",
    "    for n in N:\n",
    "        target_neighbors[n] = NearestNeighbors(n_neighbors=n, metric='cosine').fit(full_vocabs[languages['trgt'][0]]['X'])\n",
    "        \n",
    "    return target_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = fit_neighbors(N, languages, full_vocabs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cosine(vector1, vector2):\n",
    "    # Computes the cosine simularity between two vectors\n",
    "    dot_product = np.dot(vector1, vector2)\n",
    "    norm_vector1 = np.linalg.norm(vector1)\n",
    "    norm_vector2 = np.linalg.norm(vector2)\n",
    "    return dot_product/(norm_vector1*norm_vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_translations(generator, language, source_vector, target_vocab, neighbors):\n",
    "    # Gets n possible translations, as given by the n nearest neighbors of the transformed\n",
    "    # source vector in the target embeddings space, we will use a package for this for optimization\n",
    "    # purposes. n is given in the nearest neighbor fitting stage.\n",
    "    #print(source_vector.numpy()[0])\n",
    "    transformed_source_embedding = generator(torch.as_tensor(source_vector), language).detach().numpy()\n",
    "\n",
    "    # only takes 2D arrays, hence the extra bracket [1][0] stands for select indices of\n",
    "    # the first input vector (the only one in this case)\n",
    "\n",
    "    vocab_indices = neighbors.kneighbors(np.array([transformed_source_embedding]))[1][0]\n",
    "    target_vectors = []\n",
    "    target_words = []\n",
    "    for index in vocab_indices:\n",
    "        target_vectors.append(target_vocab['X'][index])\n",
    "        target_words.append(target_vocab['Y'][index])\n",
    "\n",
    "    return target_vectors, target_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_translations_batch(generator, language, source_vectors, target_vocab, neighbors):\n",
    "    # Gets n possible translations, as given by the n nearest neighbors of the transformed\n",
    "    # source vector in the target embeddings space, we will use a package for this for optimization\n",
    "    # purposes. n is given in the nearest neighbor fitting stage.\n",
    "    #print(source_vector.numpy()[0])\n",
    "    transformed_source_embedding = generator(torch.as_tensor(source_vectors), language).detach().numpy()\n",
    "\n",
    "    # only takes 2D arrays, hence the extra bracket [1][0] stands for select indices of\n",
    "    # the first input vector (the only one in this case)\n",
    "\n",
    "    vocab_indices = neighbors.kneighbors(np.array(transformed_source_embedding))[1]\n",
    "    target_vectors = []\n",
    "    target_words = []\n",
    "    for target_indices in vocab_indices:\n",
    "        vectors = []\n",
    "        words = []\n",
    "        for index in target_indices:\n",
    "            vectors.append(target_vocab['X'][index])\n",
    "            words.append(target_vocab['Y'][index])\n",
    "        target_vectors.append(vectors)\n",
    "        target_words.append(words)\n",
    "\n",
    "    return target_vectors, target_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_cosine(generator, language, source_word_vectors, target_vocab, neighbors):\n",
    "    # Computes the average cosine simularity between the source words and their translations\n",
    "    sum_of_cosines = 0\n",
    "    for source_word_vector in source_word_vectors[:30]:\n",
    "        translated_word_vector = get_n_translations(generator, language, source_word_vector, target_vocab, neighbors[1])[0][0]\n",
    "        sum_of_cosines += compute_cosine(source_word_vector, translated_word_vector)\n",
    "    return sum_of_cosines/len(source_word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_cosine_batch(generator, language, source_word_vectors, target_vocab, neighbors):\n",
    "    # Computes the average cosine simularity between the source words and their translations\n",
    "    sum_of_cosines = 0\n",
    "    translated_word_vectors = get_n_translations_batch(generator, language, source_word_vectors, target_vocab, neighbors[1])[0]\n",
    "    for source_word_vector, translated_word_vector in zip(source_word_vectors, translated_word_vectors):\n",
    "        sum_of_cosines += compute_cosine(source_word_vector, translated_word_vector[0])\n",
    "    return sum_of_cosines/len(source_word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_translation_accuracy(generator, language, source_words, source_vocab, target_vocab, dictionary, neighbors):\n",
    "    # Compute the accuracy of translation over the given set of source words\n",
    "    correct_translations = 0\n",
    "    for source_word in source_words:\n",
    "        source_word_index = source_vocab['Y'].index(source_word)\n",
    "        source_word_vector = source_vocab['X'][source_word_index]\n",
    "        n_target_words = get_n_translations(generator, language, source_word_vector, target_vocab, neighbors)[1]\n",
    "        for target_word in n_target_words:\n",
    "            if target_word in dictionary[source_word]:\n",
    "                correct_translations += 1\n",
    "                break\n",
    "    return correct_translations/len(source_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_translation_accuracy_batch(generator, language, source_words, source_vocab, target_vocab, dictionary, neighbors):\n",
    "    # Compute the accuracy of translation over the given set of source words\n",
    "    correct_translations = 0\n",
    "    source_word_vectors = []\n",
    "    for source_word in source_words:\n",
    "        source_word_index = source_vocab['Y'].index(source_word)\n",
    "        source_word_vectors.append(source_vocab['X'][source_word_index])\n",
    "    target_words = get_n_translations_batch(generator, language, source_word_vectors, target_vocab, neighbors)[1]\n",
    "    for n_target_words in target_words:\n",
    "        for target_word in n_target_words:\n",
    "            if target_word in dictionary[source_word]:\n",
    "                correct_translations += 1\n",
    "                break\n",
    "    return correct_translations/len(source_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(generator, languages, source_training_vocabs, source_eval_words, source_full_vocabs, target_full_vocabs, dictionaries, neighbors, N):\n",
    "    # Evaluates the current model by using both an unsupervised cosine similiraty metric and a \n",
    "    # supervised translation accuracy metric. We have included both to see how they compare.\n",
    "    for source_language in languages['src']:\n",
    "        cosine_metric =  get_average_cosine_batch(generator, source_language, source_training_vocabs[source_language]['x'], target_full_vocabs, neighbors) #experimental\n",
    "#         cosine_metric = ''\n",
    "        accuracy_text = 'accuracies are '\n",
    "        for n in N:\n",
    "            accuracy = get_translation_accuracy_batch(generator, source_language, source_eval_words[source_language], source_full_vocabs[source_language], target_full_vocabs, dictionaries[source_language], neighbors[n])\n",
    "            accuracy_text = str(accuracy_text) + 'p@' + str(n) + '=' + str(accuracy) + ', '\n",
    "        \n",
    "        print('evaluation of source language ' + source_language + ': average cosine=',cosine_metric, accuracy_text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(generator, languages, source_test_words, source_full_vocabs, target_full_vocabs, dictionaries, neighbors, N):\n",
    "    # Testing based on translation accuracy on testing set\n",
    "    for source_language in languages['src']:\n",
    "        accuracy_text = ''\n",
    "        for n in N:\n",
    "            accuracy = get_translation_accuracy_batch(generator, source_language, source_test_words[source_language], source_full_vocabs[source_language], target_full_vocabs, dictionaries[source_language], neighbors[n])\n",
    "            accuracy_text = accuracy_text + 'p@' + n + '=' + accuracy + ', '\n",
    "        \n",
    "        print('Testing accuracies of source language ' + source_language + \": \" + accuracy_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define training related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(data, save):\n",
    "    if save:\n",
    "        torch.save(data, checkpoint_path + 'checkpoint_%d.pt' % data['epoch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_param(model):\n",
    "    return torch.mean(torch.cat([param.data.view(-1) for param in model.parameters()], 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_sample(lang, vocab, batch_size, include_y=False):\n",
    "    \"\"\"\n",
    "    This function draws batch_size-many training samples at random \n",
    "    from a vocab corresponding to queried language.  \n",
    "    \"\"\"\n",
    "    indices = torch.LongTensor(batch_size).random_(0, len(vocab))\n",
    "    if include_y:\n",
    "        return vocab['x'][indices], vocab['y'][indices]\n",
    "    return vocab['x'][indices]\n",
    "\n",
    "\n",
    "def get_train_data(languages, vocabs, batch_size, include_y=False):\n",
    "    \"\"\"\n",
    "    Returns one set of samples datapoints form a vocabulary for each provided language.\n",
    "    \"\"\"\n",
    "    x, y = {}, {}\n",
    "    \n",
    "    # Source languages\n",
    "    for lang in languages['src']+languages['trgt']:\n",
    "        if include_y:\n",
    "            x[lang], y[lang] = get_dataset_sample(lang, vocabs[lang], batch_size, include_y)\n",
    "        else:\n",
    "            x[lang] = get_dataset_sample(lang, vocabs[lang], batch_size)\n",
    "    \n",
    "    # Return\n",
    "    if include_y:\n",
    "        return x, y\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For debugging - Compute sum of abs(gradients) of model\n",
    "def get_summed_abs_grads(model):\n",
    "#     summed_abs = torch.tensor(0)\n",
    "    summed_abs = 0\n",
    "    for p in model.parameters():\n",
    "        summed_abs += torch.sum(torch.abs(p))\n",
    "    return summed_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nr source languages: 2\n",
      "Nr target languages: 1\n",
      "\n",
      " {'src': ['de', 'nl'], 'trgt': ['en']}\n",
      "Epoch  0 / 20\n",
      "Progress:  420.21716 882.76807\n",
      "evaluation of source language de: average cosine= 0.0037752742459058936 accuracies are p@1=0.0, \n",
      "evaluation of source language nl: average cosine= -0.01946116616098407 accuracies are p@1=0.0, \n",
      "Epoch  1 / 20\n",
      "Progress:  377.1191 864.18616\n",
      "Epoch  2 / 20\n",
      "Progress:  406.84872 867.2186\n",
      "Epoch  3 / 20\n",
      "Progress:  312.46234 870.1067\n",
      "Epoch  4 / 20\n",
      "Progress:  335.59036 866.5885\n",
      "Epoch  5 / 20\n",
      "Progress:  402.1527 890.1033\n",
      "evaluation of source language de: average cosine= -0.015198492999531084 accuracies are p@1=0.0, \n",
      "evaluation of source language nl: average cosine= 0.011738179174637571 accuracies are p@1=0.0, \n",
      "Epoch  6 / 20\n",
      "Progress:  425.29053 860.6499\n",
      "Epoch  7 / 20\n",
      "Progress:  328.24036 898.61536\n",
      "Epoch  8 / 20\n",
      "Progress:  461.00717 889.3469\n",
      "Epoch  9 / 20\n",
      "Progress:  584.0207 884.15955\n",
      "Epoch  10 / 20\n",
      "Progress:  359.46045 888.92676\n",
      "evaluation of source language de: average cosine= -0.05489279715310304 accuracies are p@1=0.0, \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-918932e4b11f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[1;31m# execute only if run as a script\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Done.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-77-918932e4b11f>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[1;31m# Evaluation step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m5\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m             \u001b[0mevaluation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msource_vocabs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msource_full_vocabs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_full_vocabs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdictionaries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneighbors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mearly_stop\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# if early stopping is enabled or not\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-60b9e8ed8b8b>\u001b[0m in \u001b[0;36mevaluation\u001b[1;34m(generator, languages, source_training_vocabs, source_eval_words, source_full_vocabs, target_full_vocabs, dictionaries, neighbors, N)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m# supervised translation accuracy metric. We have included both to see how they compare.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0msource_language\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlanguages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'src'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mcosine_metric\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mget_average_cosine_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msource_language\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msource_training_vocabs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msource_language\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'x'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_full_vocabs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneighbors\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#experimental\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;31m#         cosine_metric = ''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0maccuracy_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'accuracies are '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-30c2166eb004>\u001b[0m in \u001b[0;36mget_average_cosine_batch\u001b[1;34m(generator, language, source_word_vectors, target_vocab, neighbors)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m# Computes the average cosine simularity between the source words and their translations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0msum_of_cosines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mtranslated_word_vectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_n_translations_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msource_word_vectors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_vocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneighbors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0msource_word_vector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranslated_word_vector\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_word_vectors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranslated_word_vectors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0msum_of_cosines\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mcompute_cosine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_word_vector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranslated_word_vector\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-94dfc1c7756f>\u001b[0m in \u001b[0;36mget_n_translations_batch\u001b[1;34m(generator, language, source_vectors, target_vocab, neighbors)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# the first input vector (the only one in this case)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mvocab_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mneighbors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformed_source_embedding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mtarget_vectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mtarget_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python3\\lib\\site-packages\\sklearn\\neighbors\\base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[1;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[0;32m    432\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce_func\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreduce_func\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m                 \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meffective_metric_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 434\u001b[1;33m                 **kwds))\n\u001b[0m\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_method\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'ball_tree'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'kd_tree'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36mpairwise_distances_chunked\u001b[1;34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[0m\n\u001b[0;32m   1446\u001b[0m             \u001b[0mX_chunk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msl\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1447\u001b[0m         D_chunk = pairwise_distances(X_chunk, Y, metric=metric,\n\u001b[1;32m-> 1448\u001b[1;33m                                      n_jobs=n_jobs, **kwds)\n\u001b[0m\u001b[0;32m   1449\u001b[0m         if ((X is Y or Y is None)\n\u001b[0;32m   1450\u001b[0m                 \u001b[1;32mand\u001b[0m \u001b[0mPAIRWISE_DISTANCE_FUNCTIONS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36mpairwise_distances\u001b[1;34m(X, Y, metric, n_jobs, **kwds)\u001b[0m\n\u001b[0;32m   1586\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdistance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcdist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1588\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_parallel_pairwise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1589\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36m_parallel_pairwise\u001b[1;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[0;32m   1204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1205\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0meffective_n_jobs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1206\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1208\u001b[0m     \u001b[1;31m# enforce a threading backend to prevent data communication overhead\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36mcosine_distances\u001b[1;34m(X, Y)\u001b[0m\n\u001b[0;32m    682\u001b[0m     \u001b[0mS\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    683\u001b[0m     \u001b[0mS\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 684\u001b[1;33m     \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    685\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mY\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mY\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    686\u001b[0m         \u001b[1;31m# Ensure that distances between vectors and themselves are set to 0.0.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mclip\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32md:\\python3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mclip\u001b[1;34m(a, a_min, a_max, out, **kwargs)\u001b[0m\n\u001b[0;32m   2035\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2036\u001b[0m     \"\"\"\n\u001b[1;32m-> 2037\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'clip'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_min\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2038\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2039\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python3\\lib\\site-packages\\numpy\\core\\_methods.py\u001b[0m in \u001b[0;36m_clip\u001b[1;34m(a, min, max, out, casting, **kwargs)\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m         return _clip_dep_invoke_with_casting(\n\u001b[1;32m--> 132\u001b[1;33m             um.clip, a, min, max, out=out, casting=casting, **kwargs)\n\u001b[0m\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python3\\lib\\site-packages\\numpy\\core\\_methods.py\u001b[0m in \u001b[0;36m_clip_dep_invoke_with_casting\u001b[1;34m(ufunc, out, casting, *args, **kwargs)\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;31m# try to deal with broken casting rules\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_exceptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_UFuncOutputCastingError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[1;31m# Numpy 1.17.0, 2019-02-24\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \n",
    "    importlib.reload(gan)\n",
    "    \n",
    "    NLLLoss = torch.nn.NLLLoss()\n",
    "    nr_src_langs = len(languages['src'])\n",
    "    nr_trgt_langs = len(languages['trgt'])\n",
    "    nr_langs = nr_src_langs + nr_trgt_langs\n",
    "    print('Nr source languages:', nr_src_langs)\n",
    "    print('Nr target languages:', len(languages['trgt'])) \n",
    "    print('\\n', languages)\n",
    "    \n",
    "    if avg_grads:\n",
    "        avg_factor = 1/nr_src_langs\n",
    "        print('Decoder gradient averaging factor:', avg_factor, \"\\n\")\n",
    "    \n",
    "    # Get bilingual dictionary for evaluating train loss or at least testing\n",
    "    dicts = dict()\n",
    "    #TODO\n",
    "\n",
    "    # Set up model architecture\n",
    "    net = gan.GAN(embedding_dim, internal_dim, hidden, languages['src'])\n",
    "\n",
    "    # Get optimizers; 1 per source language of encoder and 1 for discriminator\n",
    "    optimizers = {'gen': {}}\n",
    "    for lang in languages['src']:\n",
    "        optimizers['gen'][lang] = torch.optim.Adam([{'params': net.generator.encoders[lang].parameters()},\n",
    "                                                    {'params': net.generator.decoder.parameters()}],\n",
    "                                                    lr=0.0001, betas=(0.9, 0.999), eps=1e-08, \n",
    "                                                    weight_decay=0, amsgrad=False)\n",
    "    optimizers['dis'] = torch.optim.Adam(net.discriminator.parameters(),\n",
    "                                         lr=0.0001, betas=(0.9, 0.999), eps=1e-08, \n",
    "                                         weight_decay=0, amsgrad=False)\n",
    "    \n",
    "    # Train\n",
    "    train_loss_gen, train_loss_dis = [], []\n",
    "    eval_loss = [] # TODO: To be populated...\n",
    "    last_loss = -1\n",
    "    \n",
    "    es = EarlyStopping(patience=10) #patience = amount of epochs the loss has to stop decreasing in a row for it to early stop\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch ', epoch, '/', epochs)\n",
    "        loss_gen, loss_dis = 0., 0.\n",
    "\n",
    "        # Train #\n",
    "        for batch in range(num_minibatches):\n",
    "            #print('Epoch ', epoch, ', Batch ', batch, '/', num_minibatches)\n",
    "            \n",
    "            # Update discriminator #\n",
    "            net.discriminator.train()\n",
    "            net.generator.eval()\n",
    "            net.discriminator.zero_grad()\n",
    "            \n",
    "            # Retrieve data\n",
    "            x = get_train_data(languages, vocabs, batch_size)#.to(device)\n",
    "            \n",
    "\n",
    "            # Init data-storage\n",
    "            y_preds = torch.zeros([nr_langs*batch_size, 2])\n",
    "            y_true = torch.zeros([nr_langs*batch_size])\n",
    "\n",
    "            #print(\"ypreds\",y_preds.shape)\n",
    "            #print(\"ytrue\",y_true.shape)\n",
    "            \n",
    "            y_true = y_true.long()\n",
    "            y_true[0:batch_size] = real_label  # First elements are target embeddings\n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "            # All-real minibatch\n",
    "            x_real = x[languages['trgt'][0]]  # Extract all-real data\n",
    "            y_preds[0:batch_size] = net.discriminator(x_real)\n",
    "            ypreds = net.discriminator(x_real)\n",
    "            \n",
    "            # All-fake minibatches - One minibatch per source language\n",
    "            for i, language in enumerate(languages['src']):\n",
    "                idx_from = batch_size*i+nr_trgt_langs\n",
    "                idx_to = batch_size*(i+1)+nr_trgt_langs\n",
    "                x_trans = net.generator(x[lang], language)  # Generate fake data aka translate\n",
    "                y_preds[idx_from:idx_to] = net.discriminator(x_trans)\n",
    "            \n",
    "            # Loss proportional to discriminator's probability of correctly distinguishing TP and FP\n",
    "            loss = NLLLoss(torch.log(y_preds+0.0000001), y_true)  # NLLLoss needs log(prob_distribution); adding small amount to avoid log(0)\n",
    "            loss.backward()    # Compute gradients only for discriminator\n",
    "            loss_dis += loss\n",
    "            \n",
    "            # Weight update for discriminator\n",
    "            optimizers['dis'].step() \n",
    "\n",
    "            # Update generator #\n",
    "            net.generator.train()\n",
    "            net.discriminator.eval()\n",
    "            net.generator.zero_grad()\n",
    "            \n",
    "            # Retrieve data\n",
    "            x = get_train_data(languages, vocabs, batch_size)#.to(device)\n",
    "            \n",
    "            # All-real minibatch\n",
    "            x_real = x[languages['trgt'][0]]  # Extract all-real data\n",
    "            y_true = torch.full((batch_size,), fake_label).long()#.to(device)  # Pretend true targets were fake\n",
    "            y_pred = net.discriminator(x_real)\n",
    "            # Loss proportional to discriminator's probability of misclassifying TP and FP\n",
    "            loss_real = NLLLoss(torch.log(y_pred+0.0000001), y_true)  # NLLLoss needs log(prob_distribution); adding small amount to avoid log(0)\n",
    "            \n",
    "            # All-fake minibatches - One minibatch per source language\n",
    "            y_true = torch.full((batch_size,), real_label).long()#.to(device) # Try to fool the discriminator\n",
    "            for language in languages['src']:\n",
    "                x_src = x[lang]\n",
    "                x_trans = net.generator(x_src, language)\n",
    "                y_pred = net.discriminator(x_trans)\n",
    "                # Loss proportional to discriminator's probability of misclassifying TP and FP\n",
    "                loss = NLLLoss(torch.log(y_pred+0.0000001), y_true) + loss_real  # Add loss for real-misclassification here\n",
    "                loss.backward(retain_graph=True)    # Compute gradients only for discriminator\n",
    "                loss_gen += loss\n",
    "            \n",
    "            # Perform weight updates\n",
    "            for language in languages['src']:\n",
    "                optimizers['gen'][language].step()\n",
    "        \n",
    "        # Document accumulated losses per epoch\n",
    "        train_loss_gen.append(loss_gen)\n",
    "        train_loss_dis.append(loss_dis)\n",
    "        \n",
    "        #print('Mean: ', mean_param(net.generator.decoder))\n",
    "        print('Progress: ', loss_gen.detach().numpy(), \n",
    "                            loss_dis.detach().numpy())\n",
    "        \n",
    "        # Evaluation step\n",
    "        if epoch%5 is 0:\n",
    "            evaluation(net.generator, languages, source_vocabs, eval_words, source_full_vocabs, target_full_vocabs, dictionaries, neighbors, N)\n",
    "\n",
    "        if early_stop: # if early stopping is enabled or not\n",
    "            if es.step(loss_real_total_g.detach()): # using the real loss of the generator for now, maybe use something else later? e.g. evaluation loss?\n",
    "                print('early stopping')\n",
    "                break  # early stop criterion is met, stop the loop now\n",
    "        \n",
    "        # Save checkpoints\n",
    "        #print(loss_real_total_g.detach().numpy(), loss_fake_total_g.detach().numpy())\n",
    "        \n",
    "#        save = checkpoint_frequency > 0 and epoch % checkpoint_frequency == 0 and \\\n",
    "#            last_loss > loss_real_total_g+loss_fake_total_g  # Provisional: save when loss of generator has improved\n",
    "#        last_loss = loss_real_total_g+loss_fake_total_g\n",
    "#        save_checkpoint({'epoch': epoch,\n",
    "#                         'model_state_dict': net.state_dict(),\n",
    "#                         'optimizer_state_dicts': \n",
    "#                             {**{lang: optimizers['gen'][lang].state_dict() for lang in languages['src']}, \n",
    "#                              **{languages['trgt'][0]: optimizers['dis']}\n",
    "#                            },\n",
    "#                         'losses': {'train_loss_real_d': train_loss_real_d[-1],\n",
    "#                                    'train_loss_fake_d': train_loss_fake_d[-1],\n",
    "#                                    'train_loss_real_g': train_loss_real_g[-1],\n",
    "#                                    'train_loss_fake_g': train_loss_fake_g[-1],},\n",
    "#                         }, save)\n",
    "\n",
    "    # Final testing\n",
    "#     testing(net.generator, languages, test_words, source_full_vocabs, target_full_vocabs, dictionaries, neighbors, N)\n",
    "\n",
    "    # Store model\n",
    "    torch.save(net.state_dict(), final_state_path + 'final_model%d.pt' % epoch)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # execute only if run as a script\n",
    "    main()\n",
    "    print('Done.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
