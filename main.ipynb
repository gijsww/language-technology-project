{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "import torch\n",
    "import math\n",
    "import random\n",
    "from model import gan\n",
    "from early_stopping import EarlyStopping\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "\n",
    "# Unique Naming\n",
    "from datetime import datetime\n",
    "import random, string\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_string(length=10):\n",
    "    \"\"\"\n",
    "        Generate a random string of given length. For safely storing produced images.\n",
    "    \"\"\"\n",
    "    letters = string.ascii_lowercase\n",
    "    return ''.join(random.choice(letters) for i in range(length))\n",
    "\n",
    "\n",
    "def get_model_id():\n",
    "    \"\"\"\n",
    "        Creates folder with unique ID in which everything related to a particular testrun can be saved.\n",
    "    :return: Unique folder identifier\n",
    "    \"\"\"\n",
    "    # Construct testrun identifier\n",
    "    TIME_STAMP = datetime.now().strftime(\"%Y_%d_%m__%H_%M_%S__%f_\")\n",
    "    model_folder_id = TIME_STAMP + '_' + random_string() + '/'\n",
    "\n",
    "    try:\n",
    "        os.makedirs(model_folder_id)\n",
    "    except Exception as e:\n",
    "        print('Exception occurred: ', e)\n",
    "\n",
    "    return model_folder_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "### VARIABLES & ADMINISTRATIVE STUFF ###\n",
    "# System\n",
    "#dataset_path = '/media/daniel/Elements/FastText_Data/'  # In case dataset is stored somewhere else, e.g. on hard-drive\n",
    "dataset_path = '/media/daniel/Elements/FastText_Data/'  # Data in same directory\n",
    "dictionary_path = '/media/daniel/Elements/FastText_Data/'  # Dictionaries in same directory\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "# Network\n",
    "embedding_dim = 300\n",
    "internal_dim = 300\n",
    "output_dim = 2\n",
    "\n",
    "# Train hyperparameters\n",
    "epochs = 1000\n",
    "batch_size = 32\n",
    "vocab_size = 50\n",
    "num_minibatches = vocab_size // batch_size\n",
    "real_label, fake_label = 1, 0\n",
    "languages = {'src': ['de', 'nl'], 'trgt': ['en']}  # Target language to be indicated in last position\n",
    "checkpoint_frequency = 0  # 0 == Off; i > 0 == actual checkpoint frequency in epochs\n",
    "avg_grads = False  # Boolean indicating whether to average the grads of decoder & discriminator accumulated over nr of source languages by nr of source langs\n",
    "early_stop = False # Boolean indicating whether to stop early if loss won't decrease for a certain threshold\n",
    "eval_frequency = 200\n",
    "\n",
    "#testing parameters\n",
    "N = [1] # List of n nearest neighbors that will be performed in evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de\n",
      "nl\n",
      "en\n",
      "{'trgt': ['en'], 'src': ['de', 'nl']}\n"
     ]
    }
   ],
   "source": [
    "# Changed the way languages are stored. \n",
    "# For easy access to complete set of all included languages, just concatenate lists\n",
    "languages = {'src': ['de', 'nl'], 'trgt': ['en']}\n",
    "\n",
    "for lang in languages['src']+languages['trgt']:\n",
    "    print(lang)\n",
    "print(languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: ./2020_08_06__22_06_24__630394__qyzxfdpjtn/Final/\n",
      "Model ID: 2020_08_06__22_06_24__630394__qyzxfdpjtn/\n"
     ]
    }
   ],
   "source": [
    "# Set up saving paths\n",
    "data_storage_path = './'\n",
    "model_id = get_model_id()\n",
    "checkpoint_path = data_storage_path + model_id + 'Checkpoint/'\n",
    "final_state_path = data_storage_path + model_id + 'Final/'\n",
    "\n",
    "try:\n",
    "    if checkpoint_frequency > 0:\n",
    "        os.makedirs(checkpoint_path)\n",
    "        print('Created:', checkpoint_path)\n",
    "    os.makedirs(final_state_path)\n",
    "    print('Created:', final_state_path)\n",
    "except Exception as e:\n",
    "    raise Warning('Exception occurred: Cound not create dirs! Exception:', e)\n",
    "    \n",
    "print('Model ID:', model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_vocab(vocab):\n",
    "    # Returns the word embeddings and matching labels for the full vocabulary\n",
    "    words = vocab.words\n",
    "    vectors = [vocab[word] for word in words]\n",
    "    return vectors, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaned_vocab(vocab, vocab_size):\n",
    "    # Remove all punctuation tokens while valid nr of tokens is insufficient yet for having full vocab size\n",
    "    # TODO & possibly reserve testing vocab\n",
    "    # Return clean & restricted vocab\n",
    "    words = vocab.words[:vocab_size]              # Y (labels)\n",
    "    vects = [vocab[word] for word in words]       # X (input data)\n",
    "\n",
    "    return vects, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lang_to_vocab(lang_id, vocab_size, vocabs, full_vocabs):\n",
    "    # Get dataset\n",
    "    if dataset_path == './':\n",
    "        fasttext.util.download_model(lang_id)  # Download word embedding vector data if not available\n",
    "    vocab = fasttext.load_model(dataset_path + 'cc.' + lang_id + '.300.bin')  # Load language data\n",
    "\n",
    "    # Add train data (embedding-vectors) and labels (words) to vocab\n",
    "    X, Y = cleaned_vocab(vocab,500000)\n",
    "    x, y = cleaned_vocab(vocab, vocab_size)\n",
    "    vocabs[lang_id] = {'x': torch.tensor(x), 'y': y}\n",
    "    full_vocabs[lang_id] = {'X': X, 'Y': Y}\n",
    "\n",
    "    return vocabs, full_vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab(languages):\n",
    "    vocabs = {}\n",
    "    full_vocabs = {}\n",
    "    \n",
    "    for language in languages['src']+languages['trgt']:\n",
    "        vocabs, full_vocabs = add_lang_to_vocab(language, vocab_size, vocabs, full_vocabs)\n",
    "\n",
    "    print('Successfully loaded language models.')\n",
    "    return vocabs, full_vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded language models.\n"
     ]
    }
   ],
   "source": [
    "#load vocab (keep in independent cell for bugfixing purposes)\n",
    "vocabs, full_vocabs = load_vocab(languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_vocabs = {}\n",
    "source_full_vocabs = {}\n",
    "\n",
    "for source_language in languages['src']:\n",
    "    source_vocabs[source_language] = vocabs[source_language]\n",
    "    source_full_vocabs[source_language] = full_vocabs[source_language]\n",
    "target_full_vocabs = full_vocabs[languages['trgt'][0]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dictionary(dictionary_text):\n",
    "    # Converts an input dictionary text file to a python dictionary\n",
    "    dictionary = {}\n",
    "    source = True\n",
    "    source_word = ''\n",
    "    target_word = ''\n",
    "    \n",
    "    for character in dictionary_text:\n",
    "        if source is True:\n",
    "            if character is '\\t' or character is ' ':\n",
    "                source = False\n",
    "            else:\n",
    "                source_word = source_word + character\n",
    "        else:\n",
    "            if character is '\\n':\n",
    "                source = True\n",
    "                if source_word in dictionary:\n",
    "                    dictionary[source_word].append(target_word)\n",
    "                else:\n",
    "                    dictionary[source_word] = [target_word]\n",
    "                source_word = ''\n",
    "                target_word = ''\n",
    "            else:\n",
    "                target_word = target_word + character\n",
    "                \n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dictionaries(languages):\n",
    "    # Loads in the bilingual dictionaries\n",
    "    dictionaries = {}\n",
    "    \n",
    "    for source_language in languages['src']:\n",
    "        file = open(dictionary_path + source_language + '-' + languages['trgt'][0] + '.txt', 'r', errors='ignore')\n",
    "        dictionary_text = file.read()\n",
    "        dictionaries[source_language] = convert_dictionary(dictionary_text)\n",
    "    \n",
    "    return dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_translation_task(languages, source_full_vocabs, dictionaries):\n",
    "    # Creates a split in eval and training translation task\n",
    "    eval_words = {}\n",
    "    test_words = {}\n",
    "    \n",
    "    for source_language in languages['src']:\n",
    "        source_words = list(dictionaries[source_language].keys())\n",
    "#         random.shuffle(source_words)\n",
    "        eval_list = []\n",
    "        for source_word in source_words:\n",
    "            if source_word in source_full_vocabs[source_language]['Y']:\n",
    "                eval_list.append(source_word)\n",
    "            if len(eval_list) is 200:\n",
    "                eval_words[source_language] = eval_list\n",
    "                break\n",
    "#             eval_words[source_language] = source_words[0:50]\n",
    "#             test_words[source_language] = source_words[50:150]        \n",
    "#         eval_words[source_language] = source_words[0:int(len(source_words)/2)]\n",
    "#         test_words[source_language] = source_words[int(len(source_words)/2):len(source_words)]\n",
    "        \n",
    "    return eval_words, test_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in dictionaries (independent cell for bugfixing)\n",
    "dictionaries = load_dictionaries(languages)\n",
    "\n",
    "# split in train and evaluation\n",
    "eval_words, test_words = split_translation_task(languages, source_full_vocabs, dictionaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest neighbor fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_neighbors(N, languages, full_vocabs):\n",
    "    target_neighbors = {}\n",
    "    \n",
    "    for n in N:\n",
    "        target_neighbors[n] = NearestNeighbors(n_neighbors=n, metric='cosine').fit(full_vocabs[languages['trgt'][0]]['X'])\n",
    "        \n",
    "    return target_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = fit_neighbors(N, languages, full_vocabs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cosine(vector1, vector2):\n",
    "    # Computes the cosine simularity between two vectors\n",
    "    dot_product = np.dot(vector1, vector2)\n",
    "    norm_vector1 = np.linalg.norm(vector1)\n",
    "    norm_vector2 = np.linalg.norm(vector2)\n",
    "    return dot_product/(norm_vector1*norm_vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_translations(generator, language, source_vector, target_vocab, neighbors):\n",
    "    # Gets n possible translations, as given by the n nearest neighbors of the transformed\n",
    "    # source vector in the target embeddings space, we will use a package for this for optimization\n",
    "    # purposes. n is given in the nearest neighbor fitting stage.\n",
    "    #print(source_vector.numpy()[0])\n",
    "    transformed_source_embedding = generator(torch.as_tensor(source_vector), language).detach().numpy()\n",
    "\n",
    "    # only takes 2D arrays, hence the extra bracket [1][0] stands for select indices of\n",
    "    # the first input vector (the only one in this case)\n",
    "\n",
    "    vocab_indices = neighbors.kneighbors(np.array([transformed_source_embedding]))[1][0]\n",
    "    target_vectors = []\n",
    "    target_words = []\n",
    "    for index in vocab_indices:\n",
    "        target_vectors.append(target_vocab['X'][index])\n",
    "        target_words.append(target_vocab['Y'][index])\n",
    "\n",
    "    return target_vectors, target_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_translations_batch(generator, language, source_vectors, target_vocab, neighbors):\n",
    "    # Gets n possible translations, as given by the n nearest neighbors of the transformed\n",
    "    # source vector in the target embeddings space, we will use a package for this for optimization\n",
    "    # purposes. n is given in the nearest neighbor fitting stage.\n",
    "    #print(source_vector.numpy()[0])\n",
    "    transformed_source_embedding = generator(torch.as_tensor(source_vectors), language).detach().numpy()\n",
    "\n",
    "    # only takes 2D arrays, hence the extra bracket [1][0] stands for select indices of\n",
    "    # the first input vector (the only one in this case)\n",
    "\n",
    "    vocab_indices = neighbors.kneighbors(np.array(transformed_source_embedding))[1]\n",
    "    target_vectors = []\n",
    "    target_words = []\n",
    "    for target_indices in vocab_indices:\n",
    "        vectors = []\n",
    "        words = []\n",
    "        for index in target_indices:\n",
    "            vectors.append(target_vocab['X'][index])\n",
    "            words.append(target_vocab['Y'][index])\n",
    "        target_vectors.append(vectors)\n",
    "        target_words.append(words)\n",
    "\n",
    "    return target_vectors, target_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_cosine(generator, language, source_word_vectors, target_vocab, neighbors):\n",
    "    # Computes the average cosine simularity between the source words and their translations\n",
    "    sum_of_cosines = 0\n",
    "    for source_word_vector in source_word_vectors[:30]:\n",
    "        translated_word_vector = get_n_translations(generator, language, source_word_vector, target_vocab, neighbors[1])[0][0]\n",
    "        sum_of_cosines += compute_cosine(source_word_vector, translated_word_vector)\n",
    "    return sum_of_cosines/len(source_word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_cosine_batch(generator, language, source_word_vectors, target_vocab, neighbors):\n",
    "    # Computes the average cosine simularity between the source words and their translations\n",
    "    sum_of_cosines = 0\n",
    "    translated_word_vectors = get_n_translations_batch(generator, language, source_word_vectors, target_vocab, neighbors[1])[0]\n",
    "    for source_word_vector, translated_word_vector in zip(source_word_vectors, translated_word_vectors):\n",
    "        sum_of_cosines += compute_cosine(source_word_vector, translated_word_vector[0])\n",
    "    return sum_of_cosines/len(source_word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_translation_accuracy(generator, language, source_words, source_vocab, target_vocab, dictionary, neighbors):\n",
    "    # Compute the accuracy of translation over the given set of source words\n",
    "    correct_translations = 0\n",
    "    for source_word in source_words:\n",
    "        source_word_index = source_vocab['Y'].index(source_word)\n",
    "        source_word_vector = source_vocab['X'][source_word_index]\n",
    "        n_target_words = get_n_translations(generator, language, source_word_vector, target_vocab, neighbors)[1]\n",
    "        for target_word in n_target_words:\n",
    "            if target_word in dictionary[source_word]:\n",
    "                correct_translations += 1\n",
    "                break\n",
    "    return correct_translations/len(source_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_translation_accuracy_batch(generator, language, source_words, source_vocab, target_vocab, dictionary, neighbors):\n",
    "    # Compute the accuracy of translation over the given set of source words\n",
    "    correct_translations = 0\n",
    "    source_word_vectors = []\n",
    "    for source_word in source_words:\n",
    "        source_word_index = source_vocab['Y'].index(source_word)\n",
    "        source_word_vectors.append(source_vocab['X'][source_word_index])\n",
    "    target_words = get_n_translations_batch(generator, language, source_word_vectors, target_vocab, neighbors)[1]\n",
    "    for n_target_words in target_words:\n",
    "        for target_word in n_target_words:\n",
    "            if target_word in dictionary[source_word]:\n",
    "                correct_translations += 1\n",
    "                break\n",
    "    return correct_translations/len(source_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(generator, languages, source_training_vocabs, source_eval_words, source_full_vocabs, target_full_vocabs, dictionaries, neighbors, N):\n",
    "    # Evaluates the current model by using both an unsupervised cosine similiraty metric and a \n",
    "    # supervised translation accuracy metric. We have included both to see how they compare.\n",
    "    for source_language in languages['src']:\n",
    "        cosine_metric =  get_average_cosine_batch(generator, source_language, source_training_vocabs[source_language]['x'], target_full_vocabs, neighbors) #experimental\n",
    "#         cosine_metric = ''\n",
    "        accuracy_text = 'accuracies are '\n",
    "        for n in N:\n",
    "            accuracy = get_translation_accuracy_batch(generator, source_language, source_eval_words[source_language], source_full_vocabs[source_language], target_full_vocabs, dictionaries[source_language], neighbors[n])\n",
    "            accuracy_text = str(accuracy_text) + 'p@' + str(n) + '=' + str(accuracy) + ', '\n",
    "        \n",
    "        print('evaluation of source language ' + source_language + ': average cosine=',cosine_metric, accuracy_text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(generator, languages, source_test_words, source_full_vocabs, target_full_vocabs, dictionaries, neighbors, N):\n",
    "    # Testing based on translation accuracy on testing set\n",
    "    for source_language in languages['src']:\n",
    "        accuracy_text = ''\n",
    "        for n in N:\n",
    "            accuracy = get_translation_accuracy_batch(generator, source_language, source_test_words[source_language], source_full_vocabs[source_language], target_full_vocabs, dictionaries[source_language], neighbors[n])\n",
    "            accuracy_text = accuracy_text + 'p@' + n + '=' + accuracy + ', '\n",
    "        \n",
    "        print('Testing accuracies of source language ' + source_language + \": \" + accuracy_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define training related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(data, save):\n",
    "    if save:\n",
    "        torch.save(data, checkpoint_path + 'checkpoint_%d.pt' % data['epoch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_param(model):\n",
    "    return torch.mean(torch.cat([param.data.view(-1) for param in model.parameters()], 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_sample(lang, vocab, batch_size, include_y=False):\n",
    "    \"\"\"\n",
    "    This function draws batch_size-many training samples at random \n",
    "    from a vocab corresponding to queried language.  \n",
    "    \"\"\"\n",
    "    indices = torch.LongTensor(batch_size).random_(0, len(vocab))\n",
    "    if include_y:\n",
    "        return vocab['x'][indices], vocab['y'][indices]\n",
    "    return vocab['x'][indices]\n",
    "\n",
    "\n",
    "def get_train_data(languages, vocabs, batch_size, include_y=False):\n",
    "    \"\"\"\n",
    "    Returns one set of samples datapoints form a vocabulary for each provided language.\n",
    "    \"\"\"\n",
    "    x, y = {}, {}\n",
    "    \n",
    "    # Source languages\n",
    "    for lang in languages['src']+languages['trgt']:\n",
    "        if include_y:\n",
    "            x[lang], y[lang] = get_dataset_sample(lang, vocabs[lang], batch_size, include_y)\n",
    "        else:\n",
    "            x[lang] = get_dataset_sample(lang, vocabs[lang], batch_size)\n",
    "    \n",
    "    # Return\n",
    "    if include_y:\n",
    "        return x, y\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For debugging - Compute sum of abs(gradients) of model\n",
    "def get_summed_abs_grads(model):\n",
    "#     summed_abs = torch.tensor(0)\n",
    "    summed_abs = 0\n",
    "    for p in model.parameters():\n",
    "        summed_abs += torch.sum(torch.abs(p))\n",
    "    return summed_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nr source languages: 2\n",
      "Nr target languages: 1\n",
      "\n",
      " {'trgt': ['en'], 'src': ['de', 'nl']}\n",
      "Epoch  0 / 1000\n",
      "Progress:  2.7978802 0.6904523\n",
      "Epoch  1 / 1000\n",
      "Progress:  2.8078442 0.68881506\n",
      "Epoch  2 / 1000\n",
      "Progress:  2.8237314 0.68502575\n",
      "Epoch  3 / 1000\n",
      "Progress:  2.8262534 0.68289846\n",
      "Epoch  4 / 1000\n",
      "Progress:  2.8251662 0.67986727\n",
      "Epoch  5 / 1000\n",
      "Progress:  2.8482802 0.67673093\n",
      "Epoch  6 / 1000\n",
      "Progress:  2.8592534 0.67234105\n",
      "Epoch  7 / 1000\n",
      "Progress:  2.8823152 0.67123485\n",
      "Epoch  8 / 1000\n",
      "Progress:  2.871921 0.67038965\n",
      "Epoch  9 / 1000\n",
      "Progress:  2.915812 0.6679189\n",
      "Epoch  10 / 1000\n",
      "Progress:  2.9034603 0.6616455\n",
      "Epoch  11 / 1000\n",
      "Progress:  2.9221044 0.6578126\n",
      "Epoch  12 / 1000\n",
      "Progress:  2.940775 0.6579758\n",
      "Epoch  13 / 1000\n",
      "Progress:  2.9399943 0.65598077\n",
      "Epoch  14 / 1000\n",
      "Progress:  2.9722185 0.65462744\n",
      "Epoch  15 / 1000\n",
      "Progress:  2.9615703 0.65223897\n",
      "Epoch  16 / 1000\n",
      "Progress:  2.9952235 0.6491081\n",
      "Epoch  17 / 1000\n",
      "Progress:  2.978819 0.64463735\n",
      "Epoch  18 / 1000\n",
      "Progress:  2.9991984 0.6442275\n",
      "Epoch  19 / 1000\n",
      "Progress:  3.000258 0.6433398\n",
      "Epoch  20 / 1000\n",
      "Progress:  3.0366175 0.6386662\n",
      "Epoch  21 / 1000\n",
      "Progress:  3.0110416 0.63320476\n",
      "Epoch  22 / 1000\n",
      "Progress:  3.0490656 0.6300208\n",
      "Epoch  23 / 1000\n",
      "Progress:  3.048867 0.63540345\n",
      "Epoch  24 / 1000\n",
      "Progress:  3.0659733 0.62818855\n",
      "Epoch  25 / 1000\n",
      "Progress:  3.0586944 0.6274754\n",
      "Epoch  26 / 1000\n",
      "Progress:  3.0942078 0.6277014\n",
      "Epoch  27 / 1000\n",
      "Progress:  3.0929565 0.618462\n",
      "Epoch  28 / 1000\n",
      "Progress:  3.1039836 0.61962646\n",
      "Epoch  29 / 1000\n",
      "Progress:  3.1150432 0.61637586\n",
      "Epoch  30 / 1000\n",
      "Progress:  3.1608539 0.61307144\n",
      "Epoch  31 / 1000\n",
      "Progress:  3.1163027 0.6106825\n",
      "Epoch  32 / 1000\n",
      "Progress:  3.15639 0.6093159\n",
      "Epoch  33 / 1000\n",
      "Progress:  3.1682215 0.60493696\n",
      "Epoch  34 / 1000\n",
      "Progress:  3.1721249 0.60458076\n",
      "Epoch  35 / 1000\n",
      "Progress:  3.1915543 0.6106833\n",
      "Epoch  36 / 1000\n",
      "Progress:  3.1872158 0.59673125\n",
      "Epoch  37 / 1000\n",
      "Progress:  3.1819963 0.6041336\n",
      "Epoch  38 / 1000\n",
      "Progress:  3.252551 0.5952683\n",
      "Epoch  39 / 1000\n",
      "Progress:  3.2653584 0.5974447\n",
      "Epoch  40 / 1000\n",
      "Progress:  3.2250438 0.5860187\n",
      "Epoch  41 / 1000\n",
      "Progress:  3.2639513 0.5893908\n",
      "Epoch  42 / 1000\n",
      "Progress:  3.2394285 0.5917954\n",
      "Epoch  43 / 1000\n",
      "Progress:  3.3080282 0.5799347\n",
      "Epoch  44 / 1000\n",
      "Progress:  3.253488 0.5799286\n",
      "Epoch  45 / 1000\n",
      "Progress:  3.3346443 0.5762751\n",
      "Epoch  46 / 1000\n",
      "Progress:  3.3281128 0.57637614\n",
      "Epoch  47 / 1000\n",
      "Progress:  3.3310065 0.5752382\n",
      "Epoch  48 / 1000\n",
      "Progress:  3.3549623 0.57282704\n",
      "Epoch  49 / 1000\n",
      "Progress:  3.3904662 0.5637621\n",
      "Epoch  50 / 1000\n",
      "Progress:  3.3381202 0.5693052\n",
      "Epoch  51 / 1000\n",
      "Progress:  3.3737636 0.564166\n",
      "Epoch  52 / 1000\n",
      "Progress:  3.3989115 0.564508\n",
      "Epoch  53 / 1000\n",
      "Progress:  3.353489 0.56349415\n",
      "Epoch  54 / 1000\n",
      "Progress:  3.4752772 0.5610883\n",
      "Epoch  55 / 1000\n",
      "Progress:  3.4658232 0.5470107\n",
      "Epoch  56 / 1000\n",
      "Progress:  3.4680853 0.5518027\n",
      "Epoch  57 / 1000\n",
      "Progress:  3.5599887 0.55830616\n",
      "Epoch  58 / 1000\n",
      "Progress:  3.4579425 0.5544166\n",
      "Epoch  59 / 1000\n",
      "Progress:  3.5387871 0.5474213\n",
      "Epoch  60 / 1000\n",
      "Progress:  3.513598 0.5356721\n",
      "Epoch  61 / 1000\n",
      "Progress:  3.487012 0.5456582\n",
      "Epoch  62 / 1000\n",
      "Progress:  3.4731627 0.5448198\n",
      "Epoch  63 / 1000\n",
      "Progress:  3.5730388 0.52638763\n",
      "Epoch  64 / 1000\n",
      "Progress:  3.6175942 0.5431736\n",
      "Epoch  65 / 1000\n",
      "Progress:  3.6040606 0.5342381\n",
      "Epoch  66 / 1000\n",
      "Progress:  3.6049073 0.52684206\n",
      "Epoch  67 / 1000\n",
      "Progress:  3.6206434 0.52100545\n",
      "Epoch  68 / 1000\n",
      "Progress:  3.6676018 0.5167813\n",
      "Epoch  69 / 1000\n",
      "Progress:  3.6526597 0.5176006\n",
      "Epoch  70 / 1000\n",
      "Progress:  3.74976 0.5064931\n",
      "Epoch  71 / 1000\n",
      "Progress:  3.734987 0.50894517\n",
      "Epoch  72 / 1000\n",
      "Progress:  3.6519523 0.5116191\n",
      "Epoch  73 / 1000\n",
      "Progress:  3.6166954 0.5125989\n",
      "Epoch  74 / 1000\n",
      "Progress:  3.7363088 0.5082818\n",
      "Epoch  75 / 1000\n",
      "Progress:  3.735711 0.5021003\n",
      "Epoch  76 / 1000\n",
      "Progress:  3.8069365 0.50317436\n",
      "Epoch  77 / 1000\n",
      "Progress:  3.8804855 0.49505973\n",
      "Epoch  78 / 1000\n",
      "Progress:  3.8246412 0.49613342\n",
      "Epoch  79 / 1000\n",
      "Progress:  3.8428807 0.4992517\n",
      "Epoch  80 / 1000\n",
      "Progress:  3.8608525 0.48907414\n",
      "Epoch  81 / 1000\n",
      "Progress:  3.8396702 0.4922572\n",
      "Epoch  82 / 1000\n",
      "Progress:  3.9376028 0.48973942\n",
      "Epoch  83 / 1000\n",
      "Progress:  3.9566522 0.47935367\n",
      "Epoch  84 / 1000\n",
      "Progress:  3.9140637 0.48263466\n",
      "Epoch  85 / 1000\n",
      "Progress:  3.9329276 0.47609547\n",
      "Epoch  86 / 1000\n",
      "Progress:  3.887504 0.47748396\n",
      "Epoch  87 / 1000\n",
      "Progress:  3.90548 0.4748918\n",
      "Epoch  88 / 1000\n",
      "Progress:  4.055008 0.47032323\n",
      "Epoch  89 / 1000\n",
      "Progress:  3.9192977 0.46365044\n",
      "Epoch  90 / 1000\n",
      "Progress:  4.0729904 0.46517625\n",
      "Epoch  91 / 1000\n",
      "Progress:  4.069706 0.47697362\n",
      "Epoch  92 / 1000\n",
      "Progress:  4.1361504 0.46406528\n",
      "Epoch  93 / 1000\n",
      "Progress:  4.227352 0.45533678\n",
      "Epoch  94 / 1000\n",
      "Progress:  4.105919 0.4505905\n",
      "Epoch  95 / 1000\n",
      "Progress:  4.150052 0.45219836\n",
      "Epoch  96 / 1000\n",
      "Progress:  4.317066 0.45381257\n",
      "Epoch  97 / 1000\n",
      "Progress:  4.0422764 0.44486785\n",
      "Epoch  98 / 1000\n",
      "Progress:  4.186678 0.4487041\n",
      "Epoch  99 / 1000\n",
      "Progress:  4.283261 0.44398966\n",
      "Epoch  100 / 1000\n",
      "Progress:  4.356735 0.4435145\n",
      "Epoch  101 / 1000\n",
      "Progress:  4.352444 0.4388429\n",
      "Epoch  102 / 1000\n",
      "Progress:  4.374801 0.43413106\n",
      "Epoch  103 / 1000\n",
      "Progress:  4.3439884 0.4337484\n",
      "Epoch  104 / 1000\n",
      "Progress:  4.2574406 0.4333357\n",
      "Epoch  105 / 1000\n",
      "Progress:  4.3604407 0.4350783\n",
      "Epoch  106 / 1000\n",
      "Progress:  4.464659 0.43040958\n",
      "Epoch  107 / 1000\n",
      "Progress:  4.319498 0.43001685\n",
      "Epoch  108 / 1000\n",
      "Progress:  4.3972135 0.41665053\n",
      "Epoch  109 / 1000\n",
      "Progress:  4.3904905 0.4053292\n",
      "Epoch  110 / 1000\n",
      "Progress:  4.4407024 0.41370848\n",
      "Epoch  111 / 1000\n",
      "Progress:  4.579768 0.41335335\n",
      "Epoch  112 / 1000\n",
      "Progress:  4.54388 0.41311228\n",
      "Epoch  113 / 1000\n",
      "Progress:  4.5661936 0.4171486\n",
      "Epoch  114 / 1000\n",
      "Progress:  4.5290575 0.4210936\n",
      "Epoch  115 / 1000\n",
      "Progress:  4.5510254 0.40329316\n",
      "Epoch  116 / 1000\n",
      "Progress:  4.726327 0.40068266\n",
      "Epoch  117 / 1000\n",
      "Progress:  4.5962443 0.3960584\n",
      "Epoch  118 / 1000\n",
      "Progress:  4.6184034 0.39138913\n",
      "Epoch  119 / 1000\n",
      "Progress:  4.6740513 0.38888657\n",
      "Epoch  120 / 1000\n",
      "Progress:  4.7917185 0.39310554\n",
      "Epoch  121 / 1000\n",
      "Progress:  4.656435 0.38403103\n",
      "Epoch  122 / 1000\n",
      "Progress:  4.8725986 0.3882183\n",
      "Epoch  123 / 1000\n",
      "Progress:  4.669942 0.38578913\n",
      "Epoch  124 / 1000\n",
      "Progress:  4.889617 0.3832068\n",
      "Epoch  125 / 1000\n",
      "Progress:  4.781973 0.38292646\n",
      "Epoch  126 / 1000\n",
      "Progress:  4.7720118 0.3848597\n",
      "Epoch  127 / 1000\n",
      "Progress:  5.064334 0.3694035\n",
      "Epoch  128 / 1000\n",
      "Progress:  4.7844462 0.3713677\n",
      "Epoch  129 / 1000\n",
      "Progress:  4.8083034 0.37547317\n",
      "Epoch  130 / 1000\n",
      "Progress:  4.969444 0.37087128\n",
      "Epoch  131 / 1000\n",
      "Progress:  5.0281005 0.3685085\n",
      "Epoch  132 / 1000\n",
      "Progress:  5.0541124 0.35951844\n",
      "Epoch  133 / 1000\n",
      "Progress:  5.042095 0.35929084\n",
      "Epoch  134 / 1000\n",
      "Progress:  4.9973245 0.35266343\n",
      "Epoch  135 / 1000\n",
      "Progress:  5.0214934 0.35683835\n",
      "Epoch  136 / 1000\n",
      "Progress:  4.973852 0.356729\n",
      "Epoch  137 / 1000\n",
      "Progress:  5.21675 0.34345737\n",
      "Epoch  138 / 1000\n",
      "Progress:  5.2774487 0.35834822\n",
      "Epoch  139 / 1000\n",
      "Progress:  5.2673607 0.34330186\n",
      "Epoch  140 / 1000\n",
      "Progress:  5.145568 0.32190776\n",
      "Epoch  141 / 1000\n",
      "Progress:  5.2449813 0.34728208\n",
      "Epoch  142 / 1000\n",
      "Progress:  5.232315 0.33229193\n",
      "Epoch  143 / 1000\n",
      "Progress:  5.182585 0.34692836\n",
      "Epoch  144 / 1000\n",
      "Progress:  5.28417 0.33388355\n",
      "Epoch  145 / 1000\n",
      "Progress:  5.2319813 0.333933\n",
      "Epoch  146 / 1000\n",
      "Progress:  5.2973623 0.32961324\n",
      "Epoch  147 / 1000\n",
      "Progress:  5.2841797 0.32327095\n",
      "Epoch  148 / 1000\n",
      "Progress:  5.307455 0.32932395\n",
      "Epoch  149 / 1000\n",
      "Progress:  5.412361 0.33345488\n",
      "Epoch  150 / 1000\n",
      "Progress:  5.358081 0.31878617\n",
      "Epoch  151 / 1000\n",
      "Progress:  5.4639344 0.3165638\n",
      "Epoch  152 / 1000\n",
      "Progress:  5.5310235 0.31864604\n",
      "Epoch  153 / 1000\n",
      "Progress:  5.556609 0.31219396\n",
      "Epoch  154 / 1000\n",
      "Progress:  5.543174 0.31636176\n",
      "Epoch  155 / 1000\n",
      "Progress:  5.6517096 0.31208643\n",
      "Epoch  156 / 1000\n",
      "Progress:  5.7616625 0.30171344\n",
      "Epoch  157 / 1000\n",
      "Progress:  5.621095 0.30377835\n",
      "Epoch  158 / 1000\n",
      "Progress:  5.690861 0.30560663\n",
      "Epoch  159 / 1000\n",
      "Progress:  5.633178 0.2995128\n",
      "Epoch  160 / 1000\n",
      "Progress:  5.6153603 0.30361804\n",
      "Epoch  161 / 1000\n",
      "Progress:  5.729 0.29364243\n",
      "Epoch  162 / 1000\n",
      "Progress:  5.62552 0.2813888\n",
      "Epoch  163 / 1000\n",
      "Progress:  5.9571443 0.2873228\n",
      "Epoch  164 / 1000\n",
      "Progress:  5.764621 0.2912849\n",
      "Epoch  165 / 1000\n",
      "Progress:  5.745406 0.2972807\n",
      "Epoch  166 / 1000\n",
      "Progress:  5.907906 0.27749735\n",
      "Epoch  167 / 1000\n",
      "Progress:  5.7542872 0.28537908\n",
      "Epoch  168 / 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:  6.052578 0.28343984\n",
      "Epoch  169 / 1000\n",
      "Progress:  5.9444914 0.2855443\n",
      "Epoch  170 / 1000\n",
      "Progress:  5.971283 0.28337416\n",
      "Epoch  171 / 1000\n",
      "Progress:  5.8606863 0.2776939\n",
      "Epoch  172 / 1000\n",
      "Progress:  5.653369 0.2758764\n",
      "Epoch  173 / 1000\n",
      "Progress:  6.14488 0.27191833\n",
      "Epoch  174 / 1000\n",
      "Progress:  6.0321326 0.27756166\n",
      "Epoch  175 / 1000\n",
      "Progress:  6.0121136 0.27383286\n",
      "Epoch  176 / 1000\n",
      "Progress:  6.1823635 0.2682749\n",
      "Epoch  177 / 1000\n",
      "Progress:  6.208598 0.25876993\n",
      "Epoch  178 / 1000\n",
      "Progress:  6.137473 0.2645544\n",
      "Epoch  179 / 1000\n",
      "Progress:  6.168501 0.260738\n",
      "Epoch  180 / 1000\n",
      "Progress:  5.9991894 0.26277477\n",
      "Epoch  181 / 1000\n",
      "Progress:  6.5124826 0.25537118\n",
      "Epoch  182 / 1000\n",
      "Progress:  6.199836 0.25001326\n",
      "Epoch  183 / 1000\n",
      "Progress:  6.3220134 0.25745758\n",
      "Epoch  184 / 1000\n",
      "Progress:  6.3502474 0.25180915\n",
      "Epoch  185 / 1000\n",
      "Progress:  6.6305704 0.24277382\n",
      "Epoch  186 / 1000\n",
      "Progress:  6.308431 0.24655692\n",
      "Epoch  187 / 1000\n",
      "Progress:  6.385908 0.24688073\n",
      "Epoch  188 / 1000\n",
      "Progress:  6.4114723 0.23948173\n",
      "Epoch  189 / 1000\n",
      "Progress:  6.43924 0.24529701\n",
      "Epoch  190 / 1000\n",
      "Progress:  6.6190157 0.23984772\n",
      "Epoch  191 / 1000\n",
      "Progress:  6.598768 0.24360834\n",
      "Epoch  192 / 1000\n",
      "Progress:  6.5233917 0.23824887\n",
      "Epoch  193 / 1000\n",
      "Progress:  6.5510864 0.22600622\n",
      "Epoch  194 / 1000\n",
      "Progress:  6.5770106 0.22969465\n",
      "Epoch  195 / 1000\n",
      "Progress:  6.814776 0.23370306\n",
      "Epoch  196 / 1000\n",
      "Progress:  6.631977 0.22996663\n",
      "Epoch  197 / 1000\n",
      "Progress:  6.608369 0.23536706\n",
      "Epoch  198 / 1000\n",
      "Progress:  6.903402 0.22865058\n",
      "Epoch  199 / 1000\n",
      "Progress:  6.6083794 0.22512598\n",
      "Epoch  200 / 1000\n",
      "Progress:  6.960474 0.22711742\n",
      "evaluation of source language de: average cosine= 0.017757567017767678 accuracies are p@1=0.0, \n",
      "evaluation of source language nl: average cosine= 0.0023177191522518116 accuracies are p@1=0.0, \n",
      "Epoch  201 / 1000\n",
      "Progress:  6.657881 0.21887569\n",
      "Epoch  202 / 1000\n",
      "Progress:  6.852192 0.23470807\n",
      "Epoch  203 / 1000\n",
      "Progress:  6.9323983 0.22612827\n",
      "Epoch  204 / 1000\n",
      "Progress:  7.016386 0.22116464\n",
      "Epoch  205 / 1000\n",
      "Progress:  7.0998325 0.21936029\n",
      "Epoch  206 / 1000\n",
      "Progress:  6.8497396 0.223346\n",
      "Epoch  207 / 1000\n",
      "Progress:  6.98953 0.21830982\n",
      "Epoch  208 / 1000\n",
      "Progress:  7.3540773 0.21314323\n",
      "Epoch  209 / 1000\n",
      "Progress:  6.877017 0.21188587\n",
      "Epoch  210 / 1000\n",
      "Progress:  7.0146136 0.2153477\n",
      "Epoch  211 / 1000\n",
      "Progress:  6.9837255 0.20890151\n",
      "Epoch  212 / 1000\n",
      "Progress:  7.069715 0.21237125\n",
      "Epoch  213 / 1000\n",
      "Progress:  6.871916 0.20304339\n",
      "Epoch  214 / 1000\n",
      "Progress:  7.011966 0.20135514\n",
      "Epoch  215 / 1000\n",
      "Progress:  7.149745 0.20630729\n",
      "Epoch  216 / 1000\n",
      "Progress:  7.3475876 0.20823818\n",
      "Epoch  217 / 1000\n",
      "Progress:  7.4360924 0.19918466\n",
      "Epoch  218 / 1000\n",
      "Progress:  7.517968 0.20706785\n",
      "Epoch  219 / 1000\n",
      "Progress:  6.9718914 0.2041216\n",
      "Epoch  220 / 1000\n",
      "Progress:  6.939164 0.19790316\n",
      "Epoch  221 / 1000\n",
      "Progress:  7.662841 0.18768738\n",
      "Epoch  222 / 1000\n",
      "Progress:  7.225663 0.18896554\n",
      "Epoch  223 / 1000\n",
      "Progress:  7.597862 0.18334217\n",
      "Epoch  224 / 1000\n",
      "Progress:  7.6289935 0.19902836\n",
      "Epoch  225 / 1000\n",
      "Progress:  7.599681 0.19312888\n",
      "Epoch  226 / 1000\n",
      "Progress:  7.447383 0.18843727\n",
      "Epoch  227 / 1000\n",
      "Progress:  7.7711563 0.18926775\n",
      "Epoch  228 / 1000\n",
      "Progress:  7.381733 0.1878408\n",
      "Epoch  229 / 1000\n",
      "Progress:  7.7080474 0.18797348\n",
      "Epoch  230 / 1000\n",
      "Progress:  7.79224 0.1793084\n",
      "Epoch  231 / 1000\n",
      "Progress:  7.8252573 0.18372874\n",
      "Epoch  232 / 1000\n",
      "Progress:  7.61349 0.17972738\n",
      "Epoch  233 / 1000\n",
      "Progress:  7.8756323 0.18489426\n",
      "Epoch  234 / 1000\n",
      "Progress:  7.7816486 0.17747651\n",
      "Epoch  235 / 1000\n",
      "Progress:  7.8096085 0.18202174\n",
      "Epoch  236 / 1000\n",
      "Progress:  7.71966 0.17929268\n",
      "Epoch  237 / 1000\n",
      "Progress:  7.864772 0.17521827\n",
      "Epoch  238 / 1000\n",
      "Progress:  7.6487007 0.17556892\n",
      "Epoch  239 / 1000\n",
      "Progress:  7.5483904 0.17429154\n",
      "Epoch  240 / 1000\n",
      "Progress:  7.9456987 0.16930778\n",
      "Epoch  241 / 1000\n",
      "Progress:  7.7881002 0.17076802\n",
      "Epoch  242 / 1000\n",
      "Progress:  7.874965 0.1710958\n",
      "Epoch  243 / 1000\n",
      "Progress:  8.0861435 0.16849683\n",
      "Epoch  244 / 1000\n",
      "Progress:  7.8008122 0.17522335\n",
      "Epoch  245 / 1000\n",
      "Progress:  8.136822 0.16642044\n",
      "Epoch  246 / 1000\n",
      "Progress:  8.224745 0.16517553\n",
      "Epoch  247 / 1000\n",
      "Progress:  8.1318 0.16418613\n",
      "Epoch  248 / 1000\n",
      "Progress:  7.970496 0.16705902\n",
      "Epoch  249 / 1000\n",
      "Progress:  7.930127 0.16747202\n",
      "Epoch  250 / 1000\n",
      "Progress:  8.403292 0.1609426\n",
      "Epoch  251 / 1000\n",
      "Progress:  8.1672 0.15482824\n",
      "Epoch  252 / 1000\n",
      "Progress:  8.193297 0.15634641\n",
      "Epoch  253 / 1000\n",
      "Progress:  8.039445 0.16063724\n",
      "Epoch  254 / 1000\n",
      "Progress:  8.121065 0.15700203\n",
      "Epoch  255 / 1000\n",
      "Progress:  8.336114 0.16241582\n",
      "Epoch  256 / 1000\n",
      "Progress:  8.054267 0.15610427\n",
      "Epoch  257 / 1000\n",
      "Progress:  8.203112 0.15249278\n",
      "Epoch  258 / 1000\n",
      "Progress:  8.789398 0.15303011\n",
      "Epoch  259 / 1000\n",
      "Progress:  8.6883955 0.15917338\n",
      "Epoch  260 / 1000\n",
      "Progress:  8.402531 0.1538524\n",
      "Epoch  261 / 1000\n",
      "Progress:  8.555269 0.14992294\n",
      "Epoch  262 / 1000\n",
      "Progress:  8.330252 0.15258466\n",
      "Epoch  263 / 1000\n",
      "Progress:  8.290522 0.14714016\n",
      "Epoch  264 / 1000\n",
      "Progress:  8.381529 0.14960077\n",
      "Epoch  265 / 1000\n",
      "Progress:  8.717877 0.14309302\n",
      "Epoch  266 / 1000\n",
      "Progress:  8.306648 0.13761263\n",
      "Epoch  267 / 1000\n",
      "Progress:  8.831049 0.14791356\n",
      "Epoch  268 / 1000\n",
      "Progress:  8.67066 0.14464448\n",
      "Epoch  269 / 1000\n",
      "Progress:  8.885486 0.13976373\n",
      "Epoch  270 / 1000\n",
      "Progress:  8.399931 0.14402543\n",
      "Epoch  271 / 1000\n",
      "Progress:  8.7451725 0.14014205\n",
      "Epoch  272 / 1000\n",
      "Progress:  8.516966 0.14259776\n",
      "Epoch  273 / 1000\n",
      "Progress:  8.607352 0.14247322\n",
      "Epoch  274 / 1000\n",
      "Progress:  8.691229 0.13748936\n",
      "Epoch  275 / 1000\n",
      "Progress:  8.784788 0.13864997\n",
      "Epoch  276 / 1000\n",
      "Progress:  9.00028 0.13572007\n",
      "Epoch  277 / 1000\n",
      "Progress:  9.0299425 0.1405764\n",
      "Epoch  278 / 1000\n",
      "Progress:  9.046974 0.1365562\n",
      "Epoch  279 / 1000\n",
      "Progress:  9.268461 0.13847056\n",
      "Epoch  280 / 1000\n",
      "Progress:  8.972763 0.12762673\n",
      "Epoch  281 / 1000\n",
      "Progress:  8.932298 0.13193516\n",
      "Epoch  282 / 1000\n",
      "Progress:  9.214987 0.13741189\n",
      "Epoch  283 / 1000\n",
      "Progress:  8.787813 0.13349472\n",
      "Epoch  284 / 1000\n",
      "Progress:  9.0024185 0.13244873\n",
      "Epoch  285 / 1000\n",
      "Progress:  8.7685995 0.1276204\n",
      "Epoch  286 / 1000\n",
      "Progress:  9.118963 0.12716302\n",
      "Epoch  287 / 1000\n",
      "Progress:  9.269279 0.13000485\n",
      "Epoch  288 / 1000\n",
      "Progress:  8.976799 0.12559941\n",
      "Epoch  289 / 1000\n",
      "Progress:  8.866207 0.12538148\n",
      "Epoch  290 / 1000\n",
      "Progress:  9.0817585 0.12524839\n",
      "Epoch  291 / 1000\n",
      "Progress:  9.503966 0.12525819\n",
      "Epoch  292 / 1000\n",
      "Progress:  9.39588 0.12739044\n",
      "Epoch  293 / 1000\n",
      "Progress:  9.022584 0.12754995\n",
      "Epoch  294 / 1000\n",
      "Progress:  9.175467 0.12436498\n",
      "Epoch  295 / 1000\n",
      "Progress:  9.204037 0.12611195\n",
      "Epoch  296 / 1000\n",
      "Progress:  9.098978 0.12196826\n",
      "Epoch  297 / 1000\n",
      "Progress:  9.185744 0.124682866\n",
      "Epoch  298 / 1000\n",
      "Progress:  9.216003 0.12209901\n",
      "Epoch  299 / 1000\n",
      "Progress:  9.568849 0.122056924\n",
      "Epoch  300 / 1000\n",
      "Progress:  9.25224 0.117970936\n",
      "Epoch  301 / 1000\n",
      "Progress:  9.283252 0.11912608\n",
      "Epoch  302 / 1000\n",
      "Progress:  9.298417 0.121536344\n",
      "Epoch  303 / 1000\n",
      "Progress:  9.472273 0.11327959\n",
      "Epoch  304 / 1000\n",
      "Progress:  9.748399 0.120857544\n",
      "Epoch  305 / 1000\n",
      "Progress:  9.697836 0.12200176\n",
      "Epoch  306 / 1000\n",
      "Progress:  9.593633 0.115252964\n",
      "Epoch  307 / 1000\n",
      "Progress:  9.553153 0.116202354\n",
      "Epoch  308 / 1000\n",
      "Progress:  9.770978 0.11848106\n",
      "Epoch  309 / 1000\n",
      "Progress:  9.67642 0.11915288\n",
      "Epoch  310 / 1000\n",
      "Progress:  9.493992 0.11577111\n",
      "Epoch  311 / 1000\n",
      "Progress:  9.444857 0.10758326\n",
      "Epoch  312 / 1000\n",
      "Progress:  9.736972 0.11246394\n",
      "Epoch  313 / 1000\n",
      "Progress:  9.886534 0.11384825\n",
      "Epoch  314 / 1000\n",
      "Progress:  9.910384 0.107605726\n",
      "Epoch  315 / 1000\n",
      "Progress:  9.80554 0.10883968\n",
      "Epoch  316 / 1000\n",
      "Progress:  9.885687 0.113954835\n",
      "Epoch  317 / 1000\n",
      "Progress:  9.578239 0.11097119\n",
      "Epoch  318 / 1000\n",
      "Progress:  9.929022 0.11341286\n",
      "Epoch  319 / 1000\n",
      "Progress:  9.694139 0.10702885\n",
      "Epoch  320 / 1000\n",
      "Progress:  9.72196 0.10791243\n",
      "Epoch  321 / 1000\n",
      "Progress:  9.940662 0.10840409\n",
      "Epoch  322 / 1000\n",
      "Progress:  9.6253805 0.10902039\n",
      "Epoch  323 / 1000\n",
      "Progress:  9.976196 0.102865964\n",
      "Epoch  324 / 1000\n",
      "Progress:  10.12635 0.10387098\n",
      "Epoch  325 / 1000\n",
      "Progress:  9.961936 0.10964874\n",
      "Epoch  326 / 1000\n",
      "Progress:  9.590971 0.102704965\n",
      "Epoch  327 / 1000\n",
      "Progress:  10.002136 0.10439981\n",
      "Epoch  328 / 1000\n",
      "Progress:  10.028763 0.1037567\n",
      "Epoch  329 / 1000\n",
      "Progress:  9.856112 0.10372739\n",
      "Epoch  330 / 1000\n",
      "Progress:  10.119286 0.10214702\n",
      "Epoch  331 / 1000\n",
      "Progress:  9.900938 0.10423386\n",
      "Epoch  332 / 1000\n",
      "Progress:  9.790251 0.100005716\n",
      "Epoch  333 / 1000\n",
      "Progress:  9.858626 0.10069444\n",
      "Epoch  334 / 1000\n",
      "Progress:  10.272295 0.099822946\n",
      "Epoch  335 / 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:  10.420068 0.10157868\n",
      "Epoch  336 / 1000\n",
      "Progress:  10.130104 0.09682845\n",
      "Epoch  337 / 1000\n",
      "Progress:  9.8980255 0.098262966\n",
      "Epoch  338 / 1000\n",
      "Progress:  10.174983 0.10079909\n",
      "Epoch  339 / 1000\n",
      "Progress:  10.198662 0.09602233\n",
      "Epoch  340 / 1000\n",
      "Progress:  10.355782 0.09762322\n",
      "Epoch  341 / 1000\n",
      "Progress:  10.309566 0.096177764\n",
      "Epoch  342 / 1000\n",
      "Progress:  10.577058 0.095574945\n",
      "Epoch  343 / 1000\n",
      "Progress:  10.659878 0.096191965\n",
      "Epoch  344 / 1000\n",
      "Progress:  10.373066 0.094225325\n",
      "Epoch  345 / 1000\n",
      "Progress:  10.454596 0.095009156\n",
      "Epoch  346 / 1000\n",
      "Progress:  10.282398 0.09399998\n",
      "Epoch  347 / 1000\n",
      "Progress:  10.362335 0.091837436\n",
      "Epoch  348 / 1000\n",
      "Progress:  10.198611 0.092046976\n",
      "Epoch  349 / 1000\n",
      "Progress:  10.290011 0.093071334\n",
      "Epoch  350 / 1000\n",
      "Progress:  10.745592 0.08977934\n",
      "Epoch  351 / 1000\n",
      "Progress:  10.458462 0.09211901\n",
      "Epoch  352 / 1000\n",
      "Progress:  10.7716675 0.09380721\n",
      "Epoch  353 / 1000\n",
      "Progress:  10.365324 0.092080444\n",
      "Epoch  354 / 1000\n",
      "Progress:  10.580066 0.094896995\n",
      "Epoch  355 / 1000\n",
      "Progress:  10.722643 0.08799761\n",
      "Epoch  356 / 1000\n",
      "Progress:  10.870041 0.088969864\n",
      "Epoch  357 / 1000\n",
      "Progress:  10.572443 0.087942176\n",
      "Epoch  358 / 1000\n",
      "Progress:  10.920095 0.086790144\n",
      "Epoch  359 / 1000\n",
      "Progress:  10.740553 0.08627037\n",
      "Epoch  360 / 1000\n",
      "Progress:  10.708897 0.08910789\n",
      "Epoch  361 / 1000\n",
      "Progress:  10.853828 0.08637518\n",
      "Epoch  362 / 1000\n",
      "Progress:  10.814177 0.08673633\n",
      "Epoch  363 / 1000\n",
      "Progress:  10.700851 0.08694575\n",
      "Epoch  364 / 1000\n",
      "Progress:  10.536145 0.084714115\n",
      "Epoch  365 / 1000\n",
      "Progress:  10.6768 0.086930156\n",
      "Epoch  366 / 1000\n",
      "Progress:  11.097398 0.08428537\n",
      "Epoch  367 / 1000\n",
      "Progress:  10.848423 0.08638716\n",
      "Epoch  368 / 1000\n",
      "Progress:  10.862638 0.08486972\n",
      "Epoch  369 / 1000\n",
      "Progress:  10.698517 0.08203216\n",
      "Epoch  370 / 1000\n",
      "Progress:  11.165814 0.08120281\n",
      "Epoch  371 / 1000\n",
      "Progress:  10.80217 0.08356484\n",
      "Epoch  372 / 1000\n",
      "Progress:  10.882147 0.08129428\n",
      "Epoch  373 / 1000\n",
      "Progress:  10.966567 0.08171121\n",
      "Epoch  374 / 1000\n",
      "Progress:  11.058165 0.08278205\n",
      "Epoch  375 / 1000\n",
      "Progress:  11.456422 0.08108794\n",
      "Epoch  376 / 1000\n",
      "Progress:  11.213909 0.07937867\n",
      "Epoch  377 / 1000\n",
      "Progress:  11.367895 0.08080683\n",
      "Epoch  378 / 1000\n",
      "Progress:  11.063868 0.079799145\n",
      "Epoch  379 / 1000\n",
      "Progress:  11.522778 0.07925946\n",
      "Epoch  380 / 1000\n",
      "Progress:  11.0350895 0.08045437\n",
      "Epoch  381 / 1000\n",
      "Progress:  11.054484 0.076030225\n",
      "Epoch  382 / 1000\n",
      "Progress:  11.652592 0.07883532\n",
      "Epoch  383 / 1000\n",
      "Progress:  11.477354 0.077956386\n",
      "Epoch  384 / 1000\n",
      "Progress:  10.9322605 0.079435445\n",
      "Epoch  385 / 1000\n",
      "Progress:  11.074274 0.07916476\n",
      "Epoch  386 / 1000\n",
      "Progress:  10.887344 0.075205356\n",
      "Epoch  387 / 1000\n",
      "Progress:  10.983769 0.08065497\n",
      "Epoch  388 / 1000\n",
      "Progress:  11.327416 0.07680324\n",
      "Epoch  389 / 1000\n",
      "Progress:  11.145166 0.07583457\n",
      "Epoch  390 / 1000\n",
      "Progress:  11.354065 0.07774433\n",
      "Epoch  391 / 1000\n",
      "Progress:  11.168041 0.07739052\n",
      "Epoch  392 / 1000\n",
      "Progress:  11.134748 0.07458449\n",
      "Epoch  393 / 1000\n",
      "Progress:  11.208252 0.0757073\n",
      "Epoch  394 / 1000\n",
      "Progress:  11.56921 0.07475494\n",
      "Epoch  395 / 1000\n",
      "Progress:  11.264763 0.07654701\n",
      "Epoch  396 / 1000\n",
      "Progress:  11.268267 0.073141724\n",
      "Epoch  397 / 1000\n",
      "Progress:  11.422485 0.07205596\n",
      "Epoch  398 / 1000\n",
      "Progress:  11.6185055 0.07380992\n",
      "Epoch  399 / 1000\n",
      "Progress:  11.268496 0.07261067\n",
      "Epoch  400 / 1000\n",
      "Progress:  11.218155 0.07304558\n",
      "evaluation of source language de: average cosine= 0.018806216609719286 accuracies are p@1=0.0, \n",
      "evaluation of source language nl: average cosine= -0.0017857098412474442 accuracies are p@1=0.0, \n",
      "Epoch  401 / 1000\n",
      "Progress:  11.381586 0.07165325\n",
      "Epoch  402 / 1000\n",
      "Progress:  11.574537 0.07361408\n",
      "Epoch  403 / 1000\n",
      "Progress:  11.545812 0.07529514\n",
      "Epoch  404 / 1000\n",
      "Progress:  11.6107 0.071935885\n",
      "Epoch  405 / 1000\n",
      "Progress:  11.499173 0.07406016\n",
      "Epoch  406 / 1000\n",
      "Progress:  11.592551 0.07027107\n",
      "Epoch  407 / 1000\n",
      "Progress:  11.467239 0.06988987\n",
      "Epoch  408 / 1000\n",
      "Progress:  11.427574 0.07430576\n",
      "Epoch  409 / 1000\n",
      "Progress:  11.883429 0.07049383\n",
      "Epoch  410 / 1000\n",
      "Progress:  11.526443 0.06946076\n",
      "Epoch  411 / 1000\n",
      "Progress:  11.338175 0.070977665\n",
      "Epoch  412 / 1000\n",
      "Progress:  11.633115 0.06895425\n",
      "Epoch  413 / 1000\n",
      "Progress:  11.640799 0.07164536\n",
      "Epoch  414 / 1000\n",
      "Progress:  11.771117 0.06922915\n",
      "Epoch  415 / 1000\n",
      "Progress:  11.800659 0.06796483\n",
      "Epoch  416 / 1000\n",
      "Progress:  11.695761 0.06713543\n",
      "Epoch  417 / 1000\n",
      "Progress:  11.762776 0.069043405\n",
      "Epoch  418 / 1000\n",
      "Progress:  12.051613 0.06813091\n",
      "Epoch  419 / 1000\n",
      "Progress:  11.551303 0.067794144\n",
      "Epoch  420 / 1000\n",
      "Progress:  11.626787 0.06489524\n",
      "Epoch  421 / 1000\n",
      "Progress:  11.917719 0.06713837\n",
      "Epoch  422 / 1000\n",
      "Progress:  11.863989 0.06729539\n",
      "Epoch  423 / 1000\n",
      "Progress:  12.054958 0.06811003\n",
      "Epoch  424 / 1000\n",
      "Progress:  11.836988 0.06597574\n",
      "Epoch  425 / 1000\n",
      "Progress:  12.166506 0.0654314\n",
      "Epoch  426 / 1000\n",
      "Progress:  11.669958 0.06636624\n",
      "Epoch  427 / 1000\n",
      "Progress:  11.814308 0.06618344\n",
      "Epoch  428 / 1000\n",
      "Progress:  11.89508 0.06481052\n",
      "Epoch  429 / 1000\n",
      "Progress:  11.964483 0.066143274\n",
      "Epoch  430 / 1000\n",
      "Progress:  12.318613 0.066213675\n",
      "Epoch  431 / 1000\n",
      "Progress:  12.175783 0.06481262\n",
      "Epoch  432 / 1000\n",
      "Progress:  11.973014 0.06527675\n",
      "Epoch  433 / 1000\n",
      "Progress:  12.226224 0.0661073\n",
      "Epoch  434 / 1000\n",
      "Progress:  12.112849 0.06459838\n",
      "Epoch  435 / 1000\n",
      "Progress:  12.005341 0.06550532\n",
      "Epoch  436 / 1000\n",
      "Progress:  12.294743 0.06441094\n",
      "Epoch  437 / 1000\n",
      "Progress:  12.300203 0.06341697\n",
      "Epoch  438 / 1000\n",
      "Progress:  12.107344 0.06275729\n",
      "Epoch  439 / 1000\n",
      "Progress:  12.072306 0.062041555\n",
      "Epoch  440 / 1000\n",
      "Progress:  12.070747 0.0642868\n",
      "Epoch  441 / 1000\n",
      "Progress:  12.215385 0.062370796\n",
      "Epoch  442 / 1000\n",
      "Progress:  12.129759 0.06502881\n",
      "Epoch  443 / 1000\n",
      "Progress:  12.565872 0.062069565\n",
      "Epoch  444 / 1000\n",
      "Progress:  12.530546 0.06302253\n",
      "Epoch  445 / 1000\n",
      "Progress:  12.346227 0.06077953\n",
      "Epoch  446 / 1000\n",
      "Progress:  12.35897 0.061783787\n",
      "Epoch  447 / 1000\n",
      "Progress:  12.516882 0.061271604\n",
      "Epoch  448 / 1000\n",
      "Progress:  12.195652 0.05972882\n",
      "Epoch  449 / 1000\n",
      "Progress:  12.088106 0.061562937\n",
      "Epoch  450 / 1000\n",
      "Progress:  12.292866 0.06175753\n",
      "Epoch  451 / 1000\n",
      "Progress:  12.102889 0.062133867\n",
      "Epoch  452 / 1000\n",
      "Progress:  12.516218 0.06051613\n",
      "Epoch  453 / 1000\n",
      "Progress:  12.217724 0.05947857\n",
      "Epoch  454 / 1000\n",
      "Progress:  12.497026 0.060957357\n",
      "Epoch  455 / 1000\n",
      "Progress:  12.407374 0.0610172\n",
      "Epoch  456 / 1000\n",
      "Progress:  12.271734 0.06125188\n",
      "Epoch  457 / 1000\n",
      "Progress:  12.520067 0.059449363\n",
      "Epoch  458 / 1000\n",
      "Progress:  12.475905 0.06093091\n",
      "Epoch  459 / 1000\n",
      "Progress:  12.302259 0.05801994\n",
      "Epoch  460 / 1000\n",
      "Progress:  12.320796 0.06055883\n",
      "Epoch  461 / 1000\n",
      "Progress:  12.403395 0.059212092\n",
      "Epoch  462 / 1000\n",
      "Progress:  12.585661 0.060884506\n",
      "Epoch  463 / 1000\n",
      "Progress:  12.3632765 0.058353204\n",
      "Epoch  464 / 1000\n",
      "Progress:  12.497772 0.059508473\n",
      "Epoch  465 / 1000\n",
      "Progress:  12.586134 0.059271898\n",
      "Epoch  466 / 1000\n",
      "Progress:  12.701699 0.057601284\n",
      "Epoch  467 / 1000\n",
      "Progress:  12.663389 0.058751937\n",
      "Epoch  468 / 1000\n",
      "Progress:  12.23869 0.05802537\n",
      "Epoch  469 / 1000\n",
      "Progress:  12.387646 0.057079718\n",
      "Epoch  470 / 1000\n",
      "Progress:  12.70708 0.05663688\n",
      "Epoch  471 / 1000\n",
      "Progress:  12.638203 0.057510313\n",
      "Epoch  472 / 1000\n",
      "Progress:  12.797272 0.058050234\n",
      "Epoch  473 / 1000\n",
      "Progress:  12.766498 0.05730195\n",
      "Epoch  474 / 1000\n",
      "Progress:  12.652671 0.05717087\n",
      "Epoch  475 / 1000\n",
      "Progress:  12.830999 0.055918057\n",
      "Epoch  476 / 1000\n",
      "Progress:  12.804062 0.055160772\n",
      "Epoch  477 / 1000\n",
      "Progress:  12.6700115 0.05587228\n",
      "Epoch  478 / 1000\n",
      "Progress:  12.618509 0.05567341\n",
      "Epoch  479 / 1000\n",
      "Progress:  12.621925 0.05629313\n",
      "Epoch  480 / 1000\n",
      "Progress:  12.399456 0.05624708\n",
      "Epoch  481 / 1000\n",
      "Progress:  12.607419 0.05671602\n",
      "Epoch  482 / 1000\n",
      "Progress:  12.802963 0.053867683\n",
      "Epoch  483 / 1000\n",
      "Progress:  13.079657 0.05656873\n",
      "Epoch  484 / 1000\n",
      "Progress:  12.712894 0.0556055\n",
      "Epoch  485 / 1000\n",
      "Progress:  12.782692 0.05744609\n",
      "Epoch  486 / 1000\n",
      "Progress:  12.872721 0.05569528\n",
      "Epoch  487 / 1000\n",
      "Progress:  12.61782 0.05359836\n",
      "Epoch  488 / 1000\n",
      "Progress:  13.145807 0.054989714\n",
      "Epoch  489 / 1000\n",
      "Progress:  12.920233 0.05518231\n",
      "Epoch  490 / 1000\n",
      "Progress:  13.221016 0.05381875\n",
      "Epoch  491 / 1000\n",
      "Progress:  12.995174 0.053974044\n",
      "Epoch  492 / 1000\n",
      "Progress:  12.746014 0.05367498\n",
      "Epoch  493 / 1000\n",
      "Progress:  13.000362 0.0550189\n",
      "Epoch  494 / 1000\n",
      "Progress:  13.037536 0.05439912\n",
      "Epoch  495 / 1000\n",
      "Progress:  12.765361 0.054529443\n",
      "Epoch  496 / 1000\n",
      "Progress:  13.191884 0.052480724\n",
      "Epoch  497 / 1000\n",
      "Progress:  13.048428 0.054197047\n",
      "Epoch  498 / 1000\n",
      "Progress:  12.802685 0.053438827\n",
      "Epoch  499 / 1000\n",
      "Progress:  13.106918 0.05514003\n",
      "Epoch  500 / 1000\n",
      "Progress:  13.158945 0.05401355\n",
      "Epoch  501 / 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:  13.187643 0.053464785\n",
      "Epoch  502 / 1000\n",
      "Progress:  12.935348 0.053700354\n",
      "Epoch  503 / 1000\n",
      "Progress:  13.157127 0.054158423\n",
      "Epoch  504 / 1000\n",
      "Progress:  12.976543 0.054386634\n",
      "Epoch  505 / 1000\n",
      "Progress:  12.979776 0.051262215\n",
      "Epoch  506 / 1000\n",
      "Progress:  13.008888 0.05343077\n",
      "Epoch  507 / 1000\n",
      "Progress:  12.923284 0.052905962\n",
      "Epoch  508 / 1000\n",
      "Progress:  12.866987 0.05314349\n",
      "Epoch  509 / 1000\n",
      "Progress:  13.20101 0.05129783\n",
      "Epoch  510 / 1000\n",
      "Progress:  13.106085 0.051640317\n",
      "Epoch  511 / 1000\n",
      "Progress:  13.036948 0.051633347\n",
      "Epoch  512 / 1000\n",
      "Progress:  13.185152 0.052738983\n",
      "Epoch  513 / 1000\n",
      "Progress:  13.004602 0.051901776\n",
      "Epoch  514 / 1000\n",
      "Progress:  13.085854 0.052028134\n",
      "Epoch  515 / 1000\n",
      "Progress:  13.322951 0.05232362\n",
      "Epoch  516 / 1000\n",
      "Progress:  12.968729 0.052332472\n",
      "Epoch  517 / 1000\n",
      "Progress:  12.987437 0.05178757\n",
      "Epoch  518 / 1000\n",
      "Progress:  12.876876 0.05173297\n",
      "Epoch  519 / 1000\n",
      "Progress:  13.141197 0.050658237\n",
      "Epoch  520 / 1000\n",
      "Progress:  13.2702465 0.051685583\n",
      "Epoch  521 / 1000\n",
      "Progress:  13.104082 0.05192973\n",
      "Epoch  522 / 1000\n",
      "Progress:  13.231693 0.050257172\n",
      "Epoch  523 / 1000\n",
      "Progress:  13.250243 0.05118616\n",
      "Epoch  524 / 1000\n",
      "Progress:  13.648899 0.05035615\n",
      "Epoch  525 / 1000\n",
      "Progress:  13.0863085 0.051463116\n",
      "Epoch  526 / 1000\n",
      "Progress:  13.341057 0.05080707\n",
      "Epoch  527 / 1000\n",
      "Progress:  13.0728855 0.051803064\n",
      "Epoch  528 / 1000\n",
      "Progress:  13.193271 0.049057126\n",
      "Epoch  529 / 1000\n",
      "Progress:  13.269605 0.049577642\n",
      "Epoch  530 / 1000\n",
      "Progress:  13.196503 0.048256043\n",
      "Epoch  531 / 1000\n",
      "Progress:  13.352409 0.050094873\n",
      "Epoch  532 / 1000\n",
      "Progress:  13.054218 0.05018981\n",
      "Epoch  533 / 1000\n",
      "Progress:  13.16475 0.048810467\n",
      "Epoch  534 / 1000\n",
      "Progress:  13.640968 0.050212786\n",
      "Epoch  535 / 1000\n",
      "Progress:  13.825848 0.049025074\n",
      "Epoch  536 / 1000\n",
      "Progress:  13.414052 0.04921383\n",
      "Epoch  537 / 1000\n",
      "Progress:  13.794424 0.050254498\n",
      "Epoch  538 / 1000\n",
      "Progress:  13.29117 0.049376205\n",
      "Epoch  539 / 1000\n",
      "Progress:  13.752751 0.049906597\n",
      "Epoch  540 / 1000\n",
      "Progress:  13.71759 0.049128722\n",
      "Epoch  541 / 1000\n",
      "Progress:  13.641321 0.04899027\n",
      "Epoch  542 / 1000\n",
      "Progress:  13.57412 0.049782474\n",
      "Epoch  543 / 1000\n",
      "Progress:  13.797612 0.04844867\n",
      "Epoch  544 / 1000\n",
      "Progress:  13.390053 0.04890464\n",
      "Epoch  545 / 1000\n",
      "Progress:  13.491629 0.047519345\n",
      "Epoch  546 / 1000\n",
      "Progress:  13.299481 0.04908212\n",
      "Epoch  547 / 1000\n",
      "Progress:  13.615544 0.048591267\n",
      "Epoch  548 / 1000\n",
      "Progress:  13.578922 0.048683364\n",
      "Epoch  549 / 1000\n",
      "Progress:  13.591948 0.04810555\n",
      "Epoch  550 / 1000\n",
      "Progress:  13.58255 0.048353147\n",
      "Epoch  551 / 1000\n",
      "Progress:  13.517357 0.047757253\n",
      "Epoch  552 / 1000\n",
      "Progress:  13.788225 0.0503311\n",
      "Epoch  553 / 1000\n",
      "Progress:  13.6340065 0.047392514\n",
      "Epoch  554 / 1000\n",
      "Progress:  13.631271 0.04857096\n",
      "Epoch  555 / 1000\n",
      "Progress:  13.659473 0.04889497\n",
      "Epoch  556 / 1000\n",
      "Progress:  13.668312 0.048726793\n",
      "Epoch  557 / 1000\n",
      "Progress:  13.611614 0.047381777\n",
      "Epoch  558 / 1000\n",
      "Progress:  13.617616 0.046862897\n",
      "Epoch  559 / 1000\n",
      "Progress:  13.933993 0.04748793\n",
      "Epoch  560 / 1000\n",
      "Progress:  13.500328 0.047750514\n",
      "Epoch  561 / 1000\n",
      "Progress:  13.78621 0.04649007\n",
      "Epoch  562 / 1000\n",
      "Progress:  13.477939 0.04809232\n",
      "Epoch  563 / 1000\n",
      "Progress:  13.62501 0.04742745\n",
      "Epoch  564 / 1000\n",
      "Progress:  13.272468 0.047786597\n",
      "Epoch  565 / 1000\n",
      "Progress:  13.79331 0.04826163\n",
      "Epoch  566 / 1000\n",
      "Progress:  13.844093 0.04707211\n",
      "Epoch  567 / 1000\n",
      "Progress:  13.6316395 0.047250543\n",
      "Epoch  568 / 1000\n",
      "Progress:  13.635506 0.047628567\n",
      "Epoch  569 / 1000\n",
      "Progress:  13.509434 0.04699667\n",
      "Epoch  570 / 1000\n",
      "Progress:  13.599998 0.04613548\n",
      "Epoch  571 / 1000\n",
      "Progress:  13.903102 0.04692118\n",
      "Epoch  572 / 1000\n",
      "Progress:  13.949096 0.047638223\n",
      "Epoch  573 / 1000\n",
      "Progress:  13.677403 0.04732153\n",
      "Epoch  574 / 1000\n",
      "Progress:  13.808514 0.047226105\n",
      "Epoch  575 / 1000\n",
      "Progress:  13.826036 0.04719725\n",
      "Epoch  576 / 1000\n",
      "Progress:  13.839647 0.046514813\n",
      "Epoch  577 / 1000\n",
      "Progress:  13.3584385 0.04749162\n",
      "Epoch  578 / 1000\n",
      "Progress:  13.89225 0.048536554\n",
      "Epoch  579 / 1000\n",
      "Progress:  13.785269 0.046107616\n",
      "Epoch  580 / 1000\n",
      "Progress:  13.631497 0.04733492\n",
      "Epoch  581 / 1000\n",
      "Progress:  14.0926485 0.04571873\n",
      "Epoch  582 / 1000\n",
      "Progress:  13.537931 0.046702076\n",
      "Epoch  583 / 1000\n",
      "Progress:  13.889597 0.046253562\n",
      "Epoch  584 / 1000\n",
      "Progress:  13.512888 0.046250854\n",
      "Epoch  585 / 1000\n",
      "Progress:  13.731354 0.045474704\n",
      "Epoch  586 / 1000\n",
      "Progress:  13.99998 0.0455296\n",
      "Epoch  587 / 1000\n",
      "Progress:  14.003453 0.04581189\n",
      "Epoch  588 / 1000\n",
      "Progress:  13.860168 0.045822904\n",
      "Epoch  589 / 1000\n",
      "Progress:  13.747442 0.046115074\n",
      "Epoch  590 / 1000\n",
      "Progress:  13.70669 0.045642123\n",
      "Epoch  591 / 1000\n",
      "Progress:  14.316313 0.04661387\n",
      "Epoch  592 / 1000\n",
      "Progress:  13.974388 0.045140196\n",
      "Epoch  593 / 1000\n",
      "Progress:  14.074341 0.045879457\n",
      "Epoch  594 / 1000\n",
      "Progress:  13.912099 0.045972858\n",
      "Epoch  595 / 1000\n",
      "Progress:  14.015989 0.045361076\n",
      "Epoch  596 / 1000\n",
      "Progress:  13.686785 0.045283288\n",
      "Epoch  597 / 1000\n",
      "Progress:  13.941781 0.046068072\n",
      "Epoch  598 / 1000\n",
      "Progress:  14.084612 0.045628145\n",
      "Epoch  599 / 1000\n",
      "Progress:  13.577121 0.046369508\n",
      "Epoch  600 / 1000\n",
      "Progress:  13.725618 0.04524827\n",
      "evaluation of source language de: average cosine= 0.01672640680968443 accuracies are p@1=0.0, \n",
      "evaluation of source language nl: average cosine= 0.0008085201264886564 accuracies are p@1=0.0, \n",
      "Epoch  601 / 1000\n",
      "Progress:  14.013758 0.04505953\n",
      "Epoch  602 / 1000\n",
      "Progress:  14.058357 0.045047\n",
      "Epoch  603 / 1000\n",
      "Progress:  14.238976 0.046313617\n",
      "Epoch  604 / 1000\n",
      "Progress:  13.799739 0.04512091\n",
      "Epoch  605 / 1000\n",
      "Progress:  13.972177 0.044512976\n",
      "Epoch  606 / 1000\n",
      "Progress:  13.506367 0.045670096\n",
      "Epoch  607 / 1000\n",
      "Progress:  13.868479 0.044627648\n",
      "Epoch  608 / 1000\n",
      "Progress:  13.956998 0.045532316\n",
      "Epoch  609 / 1000\n",
      "Progress:  14.151388 0.045405477\n",
      "Epoch  610 / 1000\n",
      "Progress:  14.305627 0.04514402\n",
      "Epoch  611 / 1000\n",
      "Progress:  13.934558 0.045511533\n",
      "Epoch  612 / 1000\n",
      "Progress:  14.389623 0.04468286\n",
      "Epoch  613 / 1000\n",
      "Progress:  14.430404 0.044788953\n",
      "Epoch  614 / 1000\n",
      "Progress:  14.218216 0.04515989\n",
      "Epoch  615 / 1000\n",
      "Progress:  13.869421 0.043681607\n",
      "Epoch  616 / 1000\n",
      "Progress:  14.245258 0.04479894\n",
      "Epoch  617 / 1000\n",
      "Progress:  14.16539 0.044774204\n",
      "Epoch  618 / 1000\n",
      "Progress:  14.234029 0.045480505\n",
      "Epoch  619 / 1000\n",
      "Progress:  13.825429 0.04423679\n",
      "Epoch  620 / 1000\n",
      "Progress:  14.005953 0.044078186\n",
      "Epoch  621 / 1000\n",
      "Progress:  14.253523 0.045364782\n",
      "Epoch  622 / 1000\n",
      "Progress:  14.069311 0.04473151\n",
      "Epoch  623 / 1000\n",
      "Progress:  13.99429 0.043164972\n",
      "Epoch  624 / 1000\n",
      "Progress:  13.919445 0.045722205\n",
      "Epoch  625 / 1000\n",
      "Progress:  13.973175 0.043787133\n",
      "Epoch  626 / 1000\n",
      "Progress:  14.111653 0.044428527\n",
      "Epoch  627 / 1000\n",
      "Progress:  14.22637 0.044758078\n",
      "Epoch  628 / 1000\n",
      "Progress:  14.16 0.044804458\n",
      "Epoch  629 / 1000\n",
      "Progress:  14.367988 0.044653844\n",
      "Epoch  630 / 1000\n",
      "Progress:  14.097515 0.045572508\n",
      "Epoch  631 / 1000\n",
      "Progress:  14.3227825 0.045104858\n",
      "Epoch  632 / 1000\n",
      "Progress:  14.309782 0.04483901\n",
      "Epoch  633 / 1000\n",
      "Progress:  14.500774 0.04548107\n",
      "Epoch  634 / 1000\n",
      "Progress:  14.396515 0.044328634\n",
      "Epoch  635 / 1000\n",
      "Progress:  14.29746 0.04413751\n",
      "Epoch  636 / 1000\n",
      "Progress:  14.215176 0.043879956\n",
      "Epoch  637 / 1000\n",
      "Progress:  14.458208 0.043542232\n",
      "Epoch  638 / 1000\n",
      "Progress:  14.064872 0.043520033\n",
      "Epoch  639 / 1000\n",
      "Progress:  14.41872 0.044076335\n",
      "Epoch  640 / 1000\n",
      "Progress:  14.321646 0.04440452\n",
      "Epoch  641 / 1000\n",
      "Progress:  14.422157 0.04457968\n",
      "Epoch  642 / 1000\n",
      "Progress:  14.361521 0.043838512\n",
      "Epoch  643 / 1000\n",
      "Progress:  14.53952 0.044257443\n",
      "Epoch  644 / 1000\n",
      "Progress:  14.307802 0.04315373\n",
      "Epoch  645 / 1000\n",
      "Progress:  14.385803 0.044268575\n",
      "Epoch  646 / 1000\n",
      "Progress:  14.240419 0.043523267\n",
      "Epoch  647 / 1000\n",
      "Progress:  14.1730175 0.044451524\n",
      "Epoch  648 / 1000\n",
      "Progress:  14.346992 0.04375273\n",
      "Epoch  649 / 1000\n",
      "Progress:  14.556562 0.043778766\n",
      "Epoch  650 / 1000\n",
      "Progress:  13.846748 0.043226887\n",
      "Epoch  651 / 1000\n",
      "Progress:  14.34127 0.04375102\n",
      "Epoch  652 / 1000\n",
      "Progress:  14.469477 0.04326506\n",
      "Epoch  653 / 1000\n",
      "Progress:  14.422985 0.042892713\n",
      "Epoch  654 / 1000\n",
      "Progress:  14.244053 0.04450868\n",
      "Epoch  655 / 1000\n",
      "Progress:  14.664778 0.044139966\n",
      "Epoch  656 / 1000\n",
      "Progress:  14.789875 0.043238413\n",
      "Epoch  657 / 1000\n",
      "Progress:  14.152917 0.044944778\n",
      "Epoch  658 / 1000\n",
      "Progress:  14.454033 0.043090206\n",
      "Epoch  659 / 1000\n",
      "Progress:  14.420643 0.041972894\n",
      "Epoch  660 / 1000\n",
      "Progress:  14.206104 0.043704335\n",
      "Epoch  661 / 1000\n",
      "Progress:  14.363858 0.04428348\n",
      "Epoch  662 / 1000\n",
      "Progress:  14.277985 0.042902097\n",
      "Epoch  663 / 1000\n",
      "Progress:  14.297258 0.042742524\n",
      "Epoch  664 / 1000\n",
      "Progress:  14.465429 0.04495454\n",
      "Epoch  665 / 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:  13.928995 0.04391007\n",
      "Epoch  666 / 1000\n",
      "Progress:  14.352275 0.044425685\n",
      "Epoch  667 / 1000\n",
      "Progress:  14.680357 0.04357821\n",
      "Epoch  668 / 1000\n",
      "Progress:  14.783454 0.043067455\n",
      "Epoch  669 / 1000\n",
      "Progress:  14.373749 0.04406683\n",
      "Epoch  670 / 1000\n",
      "Progress:  14.186407 0.04362668\n",
      "Epoch  671 / 1000\n",
      "Progress:  14.249035 0.04319488\n",
      "Epoch  672 / 1000\n",
      "Progress:  14.461113 0.04426982\n",
      "Epoch  673 / 1000\n",
      "Progress:  14.527557 0.042992648\n",
      "Epoch  674 / 1000\n",
      "Progress:  14.640118 0.044877574\n",
      "Epoch  675 / 1000\n",
      "Progress:  14.526541 0.043826133\n",
      "Epoch  676 / 1000\n",
      "Progress:  14.601173 0.043299768\n",
      "Epoch  677 / 1000\n",
      "Progress:  14.445819 0.043397326\n",
      "Epoch  678 / 1000\n",
      "Progress:  14.511651 0.044402476\n",
      "Epoch  679 / 1000\n",
      "Progress:  14.72421 0.044145893\n",
      "Epoch  680 / 1000\n",
      "Progress:  14.484047 0.043211695\n",
      "Epoch  681 / 1000\n",
      "Progress:  14.501548 0.044250738\n",
      "Epoch  682 / 1000\n",
      "Progress:  14.341645 0.043059345\n",
      "Epoch  683 / 1000\n",
      "Progress:  14.577433 0.042795684\n",
      "Epoch  684 / 1000\n",
      "Progress:  14.458569 0.04442283\n",
      "Epoch  685 / 1000\n",
      "Progress:  14.966231 0.044187706\n",
      "Epoch  686 / 1000\n",
      "Progress:  14.610631 0.043094706\n",
      "Epoch  687 / 1000\n",
      "Progress:  14.251844 0.042797837\n",
      "Epoch  688 / 1000\n",
      "Progress:  14.43342 0.043163132\n",
      "Epoch  689 / 1000\n",
      "Progress:  14.941282 0.041867387\n",
      "Epoch  690 / 1000\n",
      "Progress:  14.718069 0.042445626\n",
      "Epoch  691 / 1000\n",
      "Progress:  14.413718 0.04360585\n",
      "Epoch  692 / 1000\n",
      "Progress:  14.820493 0.04407515\n",
      "Epoch  693 / 1000\n",
      "Progress:  14.6344185 0.042644154\n",
      "Epoch  694 / 1000\n",
      "Progress:  14.751654 0.044371698\n",
      "Epoch  695 / 1000\n",
      "Progress:  14.690472 0.04316482\n",
      "Epoch  696 / 1000\n",
      "Progress:  14.533997 0.043462064\n",
      "Epoch  697 / 1000\n",
      "Progress:  14.663491 0.043191824\n",
      "Epoch  698 / 1000\n",
      "Progress:  14.732441 0.043716606\n",
      "Epoch  699 / 1000\n",
      "Progress:  14.481342 0.043135688\n",
      "Epoch  700 / 1000\n",
      "Progress:  14.552874 0.043409813\n",
      "Epoch  701 / 1000\n",
      "Progress:  14.231466 0.043695975\n",
      "Epoch  702 / 1000\n",
      "Progress:  14.598398 0.04386361\n",
      "Epoch  703 / 1000\n",
      "Progress:  14.688742 0.042935316\n",
      "Epoch  704 / 1000\n",
      "Progress:  14.569219 0.043466244\n",
      "Epoch  705 / 1000\n",
      "Progress:  14.436232 0.04454902\n",
      "Epoch  706 / 1000\n",
      "Progress:  14.680327 0.04429846\n",
      "Epoch  707 / 1000\n",
      "Progress:  14.816404 0.04130118\n",
      "Epoch  708 / 1000\n",
      "Progress:  14.589471 0.043626364\n",
      "Epoch  709 / 1000\n",
      "Progress:  14.811045 0.04160684\n",
      "Epoch  710 / 1000\n",
      "Progress:  14.410932 0.043896604\n",
      "Epoch  711 / 1000\n",
      "Progress:  14.730749 0.04315084\n",
      "Epoch  712 / 1000\n",
      "Progress:  14.770308 0.04385765\n",
      "Epoch  713 / 1000\n",
      "Progress:  14.827734 0.04350947\n",
      "Epoch  714 / 1000\n",
      "Progress:  14.441351 0.04289721\n",
      "Epoch  715 / 1000\n",
      "Progress:  14.7181 0.043282356\n",
      "Epoch  716 / 1000\n",
      "Progress:  14.500558 0.044210825\n",
      "Epoch  717 / 1000\n",
      "Progress:  14.90169 0.043302607\n",
      "Epoch  718 / 1000\n",
      "Progress:  14.512831 0.042896613\n",
      "Epoch  719 / 1000\n",
      "Progress:  14.6535845 0.04297723\n",
      "Epoch  720 / 1000\n",
      "Progress:  14.899807 0.044297073\n",
      "Epoch  721 / 1000\n",
      "Progress:  14.599205 0.041897286\n",
      "Epoch  722 / 1000\n",
      "Progress:  14.431883 0.043170612\n",
      "Epoch  723 / 1000\n",
      "Progress:  14.764957 0.0437697\n",
      "Epoch  724 / 1000\n",
      "Progress:  14.777263 0.04216964\n",
      "Epoch  725 / 1000\n",
      "Progress:  15.056694 0.04380159\n",
      "Epoch  726 / 1000\n",
      "Progress:  14.720103 0.04287824\n",
      "Epoch  727 / 1000\n",
      "Progress:  14.729686 0.04359017\n",
      "Epoch  728 / 1000\n",
      "Progress:  14.717768 0.042667035\n",
      "Epoch  729 / 1000\n",
      "Progress:  15.076203 0.043550286\n",
      "Epoch  730 / 1000\n",
      "Progress:  14.883776 0.043264687\n",
      "Epoch  731 / 1000\n",
      "Progress:  14.937576 0.0416131\n",
      "Epoch  732 / 1000\n",
      "Progress:  14.8156395 0.043109123\n",
      "Epoch  733 / 1000\n",
      "Progress:  14.953136 0.042981807\n",
      "Epoch  734 / 1000\n",
      "Progress:  14.838708 0.043443363\n",
      "Epoch  735 / 1000\n",
      "Progress:  14.463139 0.041749552\n",
      "Epoch  736 / 1000\n",
      "Progress:  14.865644 0.043785423\n",
      "Epoch  737 / 1000\n",
      "Progress:  15.056265 0.043372825\n",
      "Epoch  738 / 1000\n",
      "Progress:  14.667371 0.04303829\n",
      "Epoch  739 / 1000\n",
      "Progress:  14.960308 0.042544503\n",
      "Epoch  740 / 1000\n",
      "Progress:  14.723004 0.04326442\n",
      "Epoch  741 / 1000\n",
      "Progress:  14.861906 0.04269908\n",
      "Epoch  742 / 1000\n",
      "Progress:  14.720493 0.042482015\n",
      "Epoch  743 / 1000\n",
      "Progress:  15.107586 0.04322731\n",
      "Epoch  744 / 1000\n",
      "Progress:  15.113623 0.042671937\n",
      "Epoch  745 / 1000\n",
      "Progress:  14.861561 0.0432206\n",
      "Epoch  746 / 1000\n",
      "Progress:  14.804922 0.042682145\n",
      "Epoch  747 / 1000\n",
      "Progress:  14.732536 0.043746457\n",
      "Epoch  748 / 1000\n",
      "Progress:  14.953493 0.04197078\n",
      "Epoch  749 / 1000\n",
      "Progress:  14.658074 0.042588938\n",
      "Epoch  750 / 1000\n",
      "Progress:  14.801544 0.042492192\n",
      "Epoch  751 / 1000\n",
      "Progress:  14.592861 0.042811077\n",
      "Epoch  752 / 1000\n",
      "Progress:  14.805058 0.043768216\n",
      "Epoch  753 / 1000\n",
      "Progress:  14.954065 0.044132173\n",
      "Epoch  754 / 1000\n",
      "Progress:  14.719323 0.04303286\n",
      "Epoch  755 / 1000\n",
      "Progress:  15.262043 0.044478793\n",
      "Epoch  756 / 1000\n",
      "Progress:  15.017109 0.04250901\n",
      "Epoch  757 / 1000\n",
      "Progress:  14.906502 0.04197018\n",
      "Epoch  758 / 1000\n",
      "Progress:  14.903889 0.04266697\n",
      "Epoch  759 / 1000\n",
      "Progress:  14.830656 0.043709114\n",
      "Epoch  760 / 1000\n",
      "Progress:  15.199462 0.042791266\n",
      "Epoch  761 / 1000\n",
      "Progress:  14.665137 0.042304043\n",
      "Epoch  762 / 1000\n",
      "Progress:  14.830513 0.04341987\n",
      "Epoch  763 / 1000\n",
      "Progress:  14.990881 0.042827044\n",
      "Epoch  764 / 1000\n",
      "Progress:  15.134552 0.042750765\n",
      "Epoch  765 / 1000\n",
      "Progress:  15.229264 0.042751133\n",
      "Epoch  766 / 1000\n",
      "Progress:  14.656931 0.042236265\n",
      "Epoch  767 / 1000\n",
      "Progress:  14.968321 0.04170939\n",
      "Epoch  768 / 1000\n",
      "Progress:  15.051386 0.04225749\n",
      "Epoch  769 / 1000\n",
      "Progress:  14.994145 0.04283699\n",
      "Epoch  770 / 1000\n",
      "Progress:  14.796571 0.042719137\n",
      "Epoch  771 / 1000\n",
      "Progress:  15.048833 0.0424594\n",
      "Epoch  772 / 1000\n",
      "Progress:  15.047006 0.04363991\n",
      "Epoch  773 / 1000\n",
      "Progress:  14.824375 0.04198702\n",
      "Epoch  774 / 1000\n",
      "Progress:  14.726095 0.04158451\n",
      "Epoch  775 / 1000\n",
      "Progress:  14.941601 0.042133674\n",
      "Epoch  776 / 1000\n",
      "Progress:  15.21118 0.04160928\n",
      "Epoch  777 / 1000\n",
      "Progress:  14.991079 0.04213716\n",
      "Epoch  778 / 1000\n",
      "Progress:  15.2182455 0.04288811\n",
      "Epoch  779 / 1000\n",
      "Progress:  15.07518 0.0430954\n",
      "Epoch  780 / 1000\n",
      "Progress:  15.052568 0.043114882\n",
      "Epoch  781 / 1000\n",
      "Progress:  15.093813 0.04304256\n",
      "Epoch  782 / 1000\n",
      "Progress:  15.146656 0.043088954\n",
      "Epoch  783 / 1000\n",
      "Progress:  15.366537 0.041760605\n",
      "Epoch  784 / 1000\n",
      "Progress:  14.818614 0.04288586\n",
      "Epoch  785 / 1000\n",
      "Progress:  15.222279 0.042368848\n",
      "Epoch  786 / 1000\n",
      "Progress:  14.806536 0.042740833\n",
      "Epoch  787 / 1000\n",
      "Progress:  15.139389 0.041325513\n",
      "Epoch  788 / 1000\n",
      "Progress:  15.069831 0.04232323\n",
      "Epoch  789 / 1000\n",
      "Progress:  15.132873 0.041941572\n",
      "Epoch  790 / 1000\n",
      "Progress:  15.099449 0.04152378\n",
      "Epoch  791 / 1000\n",
      "Progress:  14.751237 0.042139973\n",
      "Epoch  792 / 1000\n",
      "Progress:  15.18327 0.043221373\n",
      "Epoch  793 / 1000\n",
      "Progress:  15.055525 0.043247182\n",
      "Epoch  794 / 1000\n",
      "Progress:  14.91547 0.04311341\n",
      "Epoch  795 / 1000\n",
      "Progress:  14.8032 0.041960508\n",
      "Epoch  796 / 1000\n",
      "Progress:  14.804832 0.04356168\n",
      "Epoch  797 / 1000\n",
      "Progress:  15.2635565 0.043006938\n",
      "Epoch  798 / 1000\n",
      "Progress:  15.053717 0.042121083\n",
      "Epoch  799 / 1000\n",
      "Progress:  15.232027 0.04189527\n",
      "Epoch  800 / 1000\n",
      "Progress:  14.993841 0.04231113\n",
      "evaluation of source language de: average cosine= 0.016263566577274197 accuracies are p@1=0.0, \n",
      "evaluation of source language nl: average cosine= 0.002499935253811418 accuracies are p@1=0.0, \n",
      "Epoch  801 / 1000\n",
      "Progress:  14.972307 0.042958513\n",
      "Epoch  802 / 1000\n",
      "Progress:  15.326659 0.041850284\n",
      "Epoch  803 / 1000\n",
      "Progress:  15.117422 0.04291438\n",
      "Epoch  804 / 1000\n",
      "Progress:  15.251519 0.042645205\n",
      "Epoch  805 / 1000\n",
      "Progress:  15.014263 0.041431997\n",
      "Epoch  806 / 1000\n",
      "Progress:  15.242905 0.042088833\n",
      "Epoch  807 / 1000\n",
      "Progress:  14.750339 0.041869536\n",
      "Epoch  808 / 1000\n",
      "Progress:  15.282543 0.041822568\n",
      "Epoch  809 / 1000\n",
      "Progress:  15.324006 0.042289052\n",
      "Epoch  810 / 1000\n",
      "Progress:  15.362818 0.041464437\n",
      "Epoch  811 / 1000\n",
      "Progress:  15.209759 0.041767184\n",
      "Epoch  812 / 1000\n",
      "Progress:  15.425003 0.041270006\n",
      "Epoch  813 / 1000\n",
      "Progress:  15.099255 0.04155962\n",
      "Epoch  814 / 1000\n",
      "Progress:  15.423082 0.04175521\n",
      "Epoch  815 / 1000\n",
      "Progress:  15.277955 0.042556565\n",
      "Epoch  816 / 1000\n",
      "Progress:  14.784176 0.04325223\n",
      "Epoch  817 / 1000\n",
      "Progress:  15.139484 0.042921457\n",
      "Epoch  818 / 1000\n",
      "Progress:  15.309829 0.042816605\n",
      "Epoch  819 / 1000\n",
      "Progress:  15.178834 0.043518756\n",
      "Epoch  820 / 1000\n",
      "Progress:  15.282266 0.04119488\n",
      "Epoch  821 / 1000\n",
      "Progress:  15.2892685 0.041973878\n",
      "Epoch  822 / 1000\n",
      "Progress:  15.244376 0.041469615\n",
      "Epoch  823 / 1000\n",
      "Progress:  15.040482 0.042130236\n",
      "Epoch  824 / 1000\n",
      "Progress:  14.989165 0.040582996\n",
      "Epoch  825 / 1000\n",
      "Progress:  15.42848 0.04196145\n",
      "Epoch  826 / 1000\n",
      "Progress:  15.42148 0.0408829\n",
      "Epoch  827 / 1000\n",
      "Progress:  15.318671 0.042834103\n",
      "Epoch  828 / 1000\n",
      "Progress:  15.290669 0.042596564\n",
      "Epoch  829 / 1000\n",
      "Progress:  15.252547 0.04276749\n",
      "Epoch  830 / 1000\n",
      "Progress:  15.0348015 0.0412041\n",
      "Epoch  831 / 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:  15.32365 0.041222494\n",
      "Epoch  832 / 1000\n",
      "Progress:  15.33811 0.04125854\n",
      "Epoch  833 / 1000\n",
      "Progress:  15.10354 0.04249735\n",
      "Epoch  834 / 1000\n",
      "Progress:  15.135586 0.041321587\n",
      "Epoch  835 / 1000\n",
      "Progress:  15.271593 0.04254968\n",
      "Epoch  836 / 1000\n",
      "Progress:  15.080326 0.04128231\n",
      "Epoch  837 / 1000\n",
      "Progress:  15.254029 0.042000085\n",
      "Epoch  838 / 1000\n",
      "Progress:  15.205343 0.041406497\n",
      "Epoch  839 / 1000\n",
      "Progress:  15.250182 0.042302653\n",
      "Epoch  840 / 1000\n",
      "Progress:  15.229713 0.042199194\n",
      "Epoch  841 / 1000\n",
      "Progress:  15.065575 0.040415067\n",
      "Epoch  842 / 1000\n",
      "Progress:  15.298212 0.041946907\n",
      "Epoch  843 / 1000\n",
      "Progress:  15.302725 0.04050635\n",
      "Epoch  844 / 1000\n",
      "Progress:  15.080395 0.04273894\n",
      "Epoch  845 / 1000\n",
      "Progress:  15.318881 0.04086825\n",
      "Epoch  846 / 1000\n",
      "Progress:  15.287977 0.04053329\n",
      "Epoch  847 / 1000\n",
      "Progress:  15.271564 0.042799342\n",
      "Epoch  848 / 1000\n",
      "Progress:  15.091356 0.0420447\n",
      "Epoch  849 / 1000\n",
      "Progress:  15.104284 0.04077611\n",
      "Epoch  850 / 1000\n",
      "Progress:  15.17013 0.041511696\n",
      "Epoch  851 / 1000\n",
      "Progress:  15.701787 0.040176146\n",
      "Epoch  852 / 1000\n",
      "Progress:  15.295294 0.04100227\n",
      "Epoch  853 / 1000\n",
      "Progress:  15.072379 0.04281321\n",
      "Epoch  854 / 1000\n",
      "Progress:  14.973128 0.041944664\n",
      "Epoch  855 / 1000\n",
      "Progress:  15.078435 0.041843057\n",
      "Epoch  856 / 1000\n",
      "Progress:  15.183078 0.041274928\n",
      "Epoch  857 / 1000\n",
      "Progress:  15.307521 0.042549774\n",
      "Epoch  858 / 1000\n",
      "Progress:  15.09199 0.041762244\n",
      "Epoch  859 / 1000\n",
      "Progress:  15.400312 0.041265357\n",
      "Epoch  860 / 1000\n",
      "Progress:  15.431288 0.042130902\n",
      "Epoch  861 / 1000\n",
      "Progress:  15.192864 0.042017743\n",
      "Epoch  862 / 1000\n",
      "Progress:  15.372566 0.040606316\n",
      "Epoch  863 / 1000\n",
      "Progress:  15.094984 0.040227737\n",
      "Epoch  864 / 1000\n",
      "Progress:  15.543494 0.04095516\n",
      "Epoch  865 / 1000\n",
      "Progress:  15.231748 0.04090598\n",
      "Epoch  866 / 1000\n",
      "Progress:  15.281778 0.040748235\n",
      "Epoch  867 / 1000\n",
      "Progress:  15.481158 0.041404203\n",
      "Epoch  868 / 1000\n",
      "Progress:  15.35853 0.040192116\n",
      "Epoch  869 / 1000\n",
      "Progress:  15.27186 0.04062751\n",
      "Epoch  870 / 1000\n",
      "Progress:  15.494901 0.041401986\n",
      "Epoch  871 / 1000\n",
      "Progress:  15.425716 0.039850574\n",
      "Epoch  872 / 1000\n",
      "Progress:  15.499826 0.040634405\n",
      "Epoch  873 / 1000\n",
      "Progress:  15.404227 0.040584996\n",
      "Epoch  874 / 1000\n",
      "Progress:  15.324449 0.040447827\n",
      "Epoch  875 / 1000\n",
      "Progress:  15.192635 0.040247347\n",
      "Epoch  876 / 1000\n",
      "Progress:  15.215877 0.040537592\n",
      "Epoch  877 / 1000\n",
      "Progress:  15.20001 0.0401549\n",
      "Epoch  878 / 1000\n",
      "Progress:  15.535347 0.041838016\n",
      "Epoch  879 / 1000\n",
      "Progress:  15.11478 0.040888574\n",
      "Epoch  880 / 1000\n",
      "Progress:  15.504881 0.03924885\n",
      "Epoch  881 / 1000\n",
      "Progress:  15.543065 0.039872047\n",
      "Epoch  882 / 1000\n",
      "Progress:  15.194864 0.041464694\n",
      "Epoch  883 / 1000\n",
      "Progress:  15.390285 0.042123914\n",
      "Epoch  884 / 1000\n",
      "Progress:  15.196171 0.039648477\n",
      "Epoch  885 / 1000\n",
      "Progress:  15.417105 0.038598567\n",
      "Epoch  886 / 1000\n",
      "Progress:  15.630518 0.040328976\n",
      "Epoch  887 / 1000\n",
      "Progress:  15.297057 0.040145796\n",
      "Epoch  888 / 1000\n",
      "Progress:  15.586744 0.038967606\n",
      "Epoch  889 / 1000\n",
      "Progress:  15.407864 0.041085426\n",
      "Epoch  890 / 1000\n",
      "Progress:  15.360373 0.039019335\n",
      "Epoch  891 / 1000\n",
      "Progress:  15.421505 0.040008698\n",
      "Epoch  892 / 1000\n",
      "Progress:  15.589405 0.039533325\n",
      "Epoch  893 / 1000\n",
      "Progress:  15.167263 0.03941873\n",
      "Epoch  894 / 1000\n",
      "Progress:  15.3546 0.03844248\n",
      "Epoch  895 / 1000\n",
      "Progress:  15.469221 0.041054178\n",
      "Epoch  896 / 1000\n",
      "Progress:  15.4968605 0.040351156\n",
      "Epoch  897 / 1000\n",
      "Progress:  15.397614 0.038159575\n",
      "Epoch  898 / 1000\n",
      "Progress:  15.550605 0.039756957\n",
      "Epoch  899 / 1000\n",
      "Progress:  15.571169 0.04024643\n",
      "Epoch  900 / 1000\n",
      "Progress:  15.390255 0.041204244\n",
      "Epoch  901 / 1000\n",
      "Progress:  15.469539 0.039985586\n",
      "Epoch  902 / 1000\n",
      "Progress:  15.671688 0.03962577\n",
      "Epoch  903 / 1000\n",
      "Progress:  15.389073 0.039997943\n",
      "Epoch  904 / 1000\n",
      "Progress:  15.577866 0.039113253\n",
      "Epoch  905 / 1000\n",
      "Progress:  15.623858 0.04028042\n",
      "Epoch  906 / 1000\n",
      "Progress:  15.761016 0.03931329\n",
      "Epoch  907 / 1000\n",
      "Progress:  15.581903 0.038702708\n",
      "Epoch  908 / 1000\n",
      "Progress:  15.17921 0.03958972\n",
      "Epoch  909 / 1000\n",
      "Progress:  15.568106 0.03888413\n",
      "Epoch  910 / 1000\n",
      "Progress:  15.623968 0.040718842\n",
      "Epoch  911 / 1000\n",
      "Progress:  15.532894 0.0394879\n",
      "Epoch  912 / 1000\n",
      "Progress:  15.264465 0.038683634\n",
      "Epoch  913 / 1000\n",
      "Progress:  15.634214 0.0393414\n",
      "Epoch  914 / 1000\n",
      "Progress:  15.627503 0.03853055\n",
      "Epoch  915 / 1000\n",
      "Progress:  15.497287 0.038327605\n",
      "Epoch  916 / 1000\n",
      "Progress:  15.300642 0.038485963\n",
      "Epoch  917 / 1000\n",
      "Progress:  15.466936 0.040584907\n",
      "Epoch  918 / 1000\n",
      "Progress:  15.616186 0.038412787\n",
      "Epoch  919 / 1000\n",
      "Progress:  15.402165 0.0387989\n",
      "Epoch  920 / 1000\n",
      "Progress:  15.700958 0.03994908\n",
      "Epoch  921 / 1000\n",
      "Progress:  15.607704 0.040266663\n",
      "Epoch  922 / 1000\n",
      "Progress:  15.53993 0.0393843\n",
      "Epoch  923 / 1000\n",
      "Progress:  15.595293 0.03968953\n",
      "Epoch  924 / 1000\n",
      "Progress:  15.489259 0.039838035\n",
      "Epoch  925 / 1000\n",
      "Progress:  15.5322695 0.0383775\n",
      "Epoch  926 / 1000\n",
      "Progress:  15.252388 0.039943658\n",
      "Epoch  927 / 1000\n",
      "Progress:  15.616558 0.040608723\n",
      "Epoch  928 / 1000\n",
      "Progress:  15.614441 0.037071496\n",
      "Epoch  929 / 1000\n",
      "Progress:  15.280611 0.038059387\n",
      "Epoch  930 / 1000\n",
      "Progress:  15.64189 0.038700987\n",
      "Epoch  931 / 1000\n",
      "Progress:  15.547003 0.039496202\n",
      "Epoch  932 / 1000\n",
      "Progress:  15.694664 0.039380375\n",
      "Epoch  933 / 1000\n",
      "Progress:  15.730864 0.03908655\n",
      "Epoch  934 / 1000\n",
      "Progress:  15.389492 0.039142344\n",
      "Epoch  935 / 1000\n",
      "Progress:  15.58879 0.038754486\n",
      "Epoch  936 / 1000\n",
      "Progress:  15.677601 0.03904661\n",
      "Epoch  937 / 1000\n",
      "Progress:  15.570551 0.039353255\n",
      "Epoch  938 / 1000\n",
      "Progress:  15.514641 0.03880937\n",
      "Epoch  939 / 1000\n",
      "Progress:  15.695929 0.038352538\n",
      "Epoch  940 / 1000\n",
      "Progress:  15.437841 0.038832683\n",
      "Epoch  941 / 1000\n",
      "Progress:  15.729464 0.038527794\n",
      "Epoch  942 / 1000\n",
      "Progress:  15.694668 0.03665568\n",
      "Epoch  943 / 1000\n",
      "Progress:  15.690481 0.038460106\n",
      "Epoch  944 / 1000\n",
      "Progress:  15.682028 0.03883481\n",
      "Epoch  945 / 1000\n",
      "Progress:  15.232913 0.037970174\n",
      "Epoch  946 / 1000\n",
      "Progress:  15.827135 0.038602833\n",
      "Epoch  947 / 1000\n",
      "Progress:  15.594241 0.03881261\n",
      "Epoch  948 / 1000\n",
      "Progress:  15.913376 0.037277516\n",
      "Epoch  949 / 1000\n",
      "Progress:  15.306157 0.038065802\n",
      "Epoch  950 / 1000\n",
      "Progress:  15.743767 0.03686231\n",
      "Epoch  951 / 1000\n",
      "Progress:  15.623051 0.037492454\n",
      "Epoch  952 / 1000\n",
      "Progress:  15.640628 0.036707893\n",
      "Epoch  953 / 1000\n",
      "Progress:  15.446068 0.03782742\n",
      "Epoch  954 / 1000\n",
      "Progress:  15.806608 0.037954222\n",
      "Epoch  955 / 1000\n",
      "Progress:  15.870041 0.039650414\n",
      "Epoch  956 / 1000\n",
      "Progress:  15.728947 0.03720529\n",
      "Epoch  957 / 1000\n",
      "Progress:  15.740098 0.03782218\n",
      "Epoch  958 / 1000\n",
      "Progress:  15.366763 0.03786671\n",
      "Epoch  959 / 1000\n",
      "Progress:  15.774782 0.038147684\n",
      "Epoch  960 / 1000\n",
      "Progress:  15.380819 0.037863854\n",
      "Epoch  961 / 1000\n",
      "Progress:  15.710011 0.037730563\n",
      "Epoch  962 / 1000\n",
      "Progress:  15.766694 0.037765045\n",
      "Epoch  963 / 1000\n",
      "Progress:  15.671833 0.03691029\n",
      "Epoch  964 / 1000\n",
      "Progress:  15.735657 0.03856374\n",
      "Epoch  965 / 1000\n",
      "Progress:  15.500771 0.037131693\n",
      "Epoch  966 / 1000\n",
      "Progress:  15.658125 0.03725612\n",
      "Epoch  967 / 1000\n",
      "Progress:  15.509121 0.036563437\n",
      "Epoch  968 / 1000\n",
      "Progress:  15.7393055 0.036826886\n",
      "Epoch  969 / 1000\n",
      "Progress:  15.449332 0.03623483\n",
      "Epoch  970 / 1000\n",
      "Progress:  15.620764 0.037150856\n",
      "Epoch  971 / 1000\n",
      "Progress:  15.71139 0.037582584\n",
      "Epoch  972 / 1000\n",
      "Progress:  15.883083 0.036550652\n",
      "Epoch  973 / 1000\n",
      "Progress:  16.142883 0.037459962\n",
      "Epoch  974 / 1000\n",
      "Progress:  15.638093 0.03696907\n",
      "Epoch  975 / 1000\n",
      "Progress:  15.945583 0.037541386\n",
      "Epoch  976 / 1000\n",
      "Progress:  15.672567 0.036451902\n",
      "Epoch  977 / 1000\n",
      "Progress:  15.694352 0.036553923\n",
      "Epoch  978 / 1000\n",
      "Progress:  15.668286 0.03475501\n",
      "Epoch  979 / 1000\n",
      "Progress:  15.532296 0.035515264\n",
      "Epoch  980 / 1000\n",
      "Progress:  15.869928 0.035330985\n",
      "Epoch  981 / 1000\n",
      "Progress:  15.790579 0.03605168\n",
      "Epoch  982 / 1000\n",
      "Progress:  15.579905 0.036178898\n",
      "Epoch  983 / 1000\n",
      "Progress:  15.933472 0.036960464\n",
      "Epoch  984 / 1000\n",
      "Progress:  15.738379 0.03628051\n",
      "Epoch  985 / 1000\n",
      "Progress:  15.644623 0.035873327\n",
      "Epoch  986 / 1000\n",
      "Progress:  15.800488 0.035515662\n",
      "Epoch  987 / 1000\n",
      "Progress:  15.590851 0.03628391\n",
      "Epoch  988 / 1000\n",
      "Progress:  15.832334 0.03604791\n",
      "Epoch  989 / 1000\n",
      "Progress:  15.750479 0.035684813\n",
      "Epoch  990 / 1000\n",
      "Progress:  15.817955 0.036686894\n",
      "Epoch  991 / 1000\n",
      "Progress:  15.794939 0.035584703\n",
      "Epoch  992 / 1000\n",
      "Progress:  15.791777 0.03673313\n",
      "Epoch  993 / 1000\n",
      "Progress:  15.807674 0.035538476\n",
      "Epoch  994 / 1000\n",
      "Progress:  15.849194 0.03457633\n",
      "Epoch  995 / 1000\n",
      "Progress:  16.019505 0.034896474\n",
      "Epoch  996 / 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:  15.933777 0.03596059\n",
      "Epoch  997 / 1000\n",
      "Progress:  16.234394 0.03587774\n",
      "Epoch  998 / 1000\n",
      "Progress:  15.918175 0.03585711\n",
      "Epoch  999 / 1000\n",
      "Progress:  15.9748125 0.034994043\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \n",
    "    importlib.reload(gan)\n",
    "    \n",
    "    NLLLoss = torch.nn.NLLLoss()\n",
    "    nr_src_langs = len(languages['src'])\n",
    "    nr_trgt_langs = len(languages['trgt'])\n",
    "    nr_langs = nr_src_langs + nr_trgt_langs\n",
    "    print('Nr source languages:', nr_src_langs)\n",
    "    print('Nr target languages:', len(languages['trgt'])) \n",
    "    print('\\n', languages)\n",
    "    \n",
    "    if avg_grads:\n",
    "        avg_factor = 1/nr_src_langs\n",
    "        print('Decoder gradient averaging factor:', avg_factor, \"\\n\")\n",
    "    \n",
    "    # Get bilingual dictionary for evaluating train loss or at least testing\n",
    "    dicts = dict()\n",
    "    #TODO\n",
    "\n",
    "    # Set up model architecture\n",
    "    net = gan.GAN(embedding_dim, internal_dim, output_dim, languages['src'])\n",
    "\n",
    "    # Get optimizers; 1 per source language of encoder and 1 for discriminator\n",
    "    optimizers = {'gen': {}}\n",
    "    for lang in languages['src']:\n",
    "        optimizers['gen'][lang] = torch.optim.Adam([{'params': net.generator.encoders[lang].parameters()},\n",
    "                                                    {'params': net.generator.decoder.parameters()}],\n",
    "                                                    lr=0.000001, betas=(0.9, 0.999), eps=1e-08, \n",
    "                                                    weight_decay=0, amsgrad=False)\n",
    "    optimizers['dis'] = torch.optim.Adam(net.discriminator.parameters(),\n",
    "                                         lr=0.0001, betas=(0.9, 0.999), eps=1e-08, \n",
    "                                         weight_decay=0, amsgrad=False)\n",
    "    \n",
    "    # Train\n",
    "    train_loss_gen, train_loss_dis = [], []\n",
    "    eval_loss = [] # TODO: To be populated...\n",
    "    last_loss = -1\n",
    "    \n",
    "    es = EarlyStopping(patience=10) #patience = amount of epochs the loss has to stop decreasing in a row for it to early stop\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch ', epoch, '/', epochs)\n",
    "        loss_gen, loss_dis = 0., 0.\n",
    "\n",
    "        # Train #\n",
    "        for batch in range(num_minibatches):\n",
    "            #print('Epoch ', epoch, ', Batch ', batch, '/', num_minibatches)\n",
    "            \n",
    "            # Update discriminator #\n",
    "            net.discriminator.train()\n",
    "            net.generator.eval()\n",
    "            net.discriminator.zero_grad()\n",
    "            \n",
    "            # Retrieve data\n",
    "            x = get_train_data(languages, vocabs, batch_size)#.to(device) \n",
    "\n",
    "            # Init data-storage\n",
    "            y_preds = torch.zeros([nr_langs*batch_size, 2])\n",
    "            y_true = torch.zeros([nr_langs*batch_size]).long()\n",
    "            \n",
    "            y_true[0:batch_size] = real_label  # First elements are target embeddings\n",
    "\n",
    "            \n",
    "            # All-real minibatch\n",
    "            x_real = x[languages['trgt'][0]]  # Extract all-real data\n",
    "            y_preds[0:batch_size] = net.discriminator(x_real)\n",
    "            \n",
    "            # All-fake minibatches - One minibatch per source language\n",
    "            for i, language in enumerate(languages['src']):\n",
    "                idx_from = batch_size*i+batch_size*nr_trgt_langs\n",
    "                idx_to = batch_size*(i+1)+batch_size*nr_trgt_langs\n",
    "                x_trans = net.generator(x[language], language)  # Generate fake data aka translate\n",
    "                y_preds[idx_from:idx_to] = net.discriminator(x_trans)\n",
    "            #print('Preds:', y_preds)\n",
    "            \n",
    "            # Loss proportional to discriminator's probability of correctly distinguishing TP and FP\n",
    "            loss = NLLLoss(torch.log(y_preds+0.0000001), y_true)  # NLLLoss needs log(prob_distribution); adding small amount to avoid log(0)\n",
    "            loss.backward()    # Compute gradients only for discriminator\n",
    "            loss_dis += loss\n",
    "            \n",
    "            # Weight update for discriminator\n",
    "            optimizers['dis'].step() \n",
    "\n",
    "            \n",
    "            # Update generator #\n",
    "            net.generator.train()\n",
    "            net.discriminator.eval()\n",
    "            net.generator.zero_grad()\n",
    "            \n",
    "            # Retrieve data\n",
    "            x = get_train_data(languages, vocabs, batch_size)#.to(device)\n",
    "            \n",
    "            # All-real minibatch\n",
    "            x_real = x[languages['trgt'][0]]  # Extract all-real data\n",
    "            y_true = torch.full((batch_size,), fake_label).long()#.to(device)  # Pretend true targets were fake\n",
    "            y_pred = net.discriminator(x_real)\n",
    "            # Loss proportional to discriminator's probability of misclassifying TP and FP\n",
    "            loss_real = NLLLoss(torch.log(y_pred+0.0000001), y_true)  # NLLLoss needs log(prob_distribution); adding small amount to avoid log(0)\n",
    "            \n",
    "            # All-fake minibatches - One minibatch per source language\n",
    "            y_true = torch.full((batch_size,), real_label).long()#.to(device) # Try to fool the discriminator\n",
    "            for language in languages['src']:\n",
    "                x_src = x[language]\n",
    "                x_trans = net.generator(x_src, language)\n",
    "                y_pred = net.discriminator(x_trans)\n",
    "                # Loss proportional to discriminator's probability of misclassifying TP and FP\n",
    "                loss = NLLLoss(torch.log(y_pred+0.0000001), y_true) + loss_real  # Add loss for real-misclassification here\n",
    "                loss.backward(retain_graph=True)    # Compute gradients only for discriminator\n",
    "                loss_gen += loss\n",
    "            \n",
    "            # Perform weight updates\n",
    "            for language in languages['src']:\n",
    "                optimizers['gen'][language].step()\n",
    "        \n",
    "        # Document accumulated losses per epoch\n",
    "        train_loss_gen.append(loss_gen.detach().numpy())\n",
    "        train_loss_dis.append(loss_dis.detach().numpy())\n",
    "        \n",
    "        #print('Mean: ', mean_param(net.generator.decoder))\n",
    "        print('Progress: ', loss_gen.detach().numpy(), \n",
    "                            loss_dis.detach().numpy())\n",
    "        \n",
    "        # Evaluation step\n",
    "        if epoch > 50 and epoch % eval_frequency is 0:\n",
    "            evaluation(net.generator, languages, source_vocabs, eval_words, source_full_vocabs, target_full_vocabs, dictionaries, neighbors, N)\n",
    "\n",
    "        if early_stop: # if early stopping is enabled or not\n",
    "            if es.step(loss_gen.detach()): # using the real loss of the generator for now, maybe use something else later? e.g. evaluation loss?\n",
    "                print('early stopping')\n",
    "                break  # early stop criterion is met, stop the loop now\n",
    "        \n",
    "        # Save checkpoints\n",
    "        #print(loss_real_total_g.detach().numpy(), loss_fake_total_g.detach().numpy())\n",
    "        \n",
    "#        save = checkpoint_frequency > 0 and epoch % checkpoint_frequency == 0 and \\\n",
    "#            last_loss > loss_real_total_g+loss_fake_total_g  # Provisional: save when loss of generator has improved\n",
    "#        last_loss = loss_real_total_g+loss_fake_total_g\n",
    "#        save_checkpoint({'epoch': epoch,\n",
    "#                         'model_state_dict': net.state_dict(),\n",
    "#                         'optimizer_state_dicts': \n",
    "#                             {**{lang: optimizers['gen'][lang].state_dict() for lang in languages['src']}, \n",
    "#                              **{languages['trgt'][0]: optimizers['dis']}\n",
    "#                            },\n",
    "#                         'losses': {'train_loss_real_d': train_loss_real_d[-1],\n",
    "#                                    'train_loss_fake_d': train_loss_fake_d[-1],\n",
    "#                                    'train_loss_real_g': train_loss_real_g[-1],\n",
    "#                                    'train_loss_fake_g': train_loss_fake_g[-1],},\n",
    "#                         }, save)\n",
    "\n",
    "    # Final testing\n",
    "#     testing(net.generator, languages, test_words, source_full_vocabs, target_full_vocabs, dictionaries, neighbors, N)\n",
    "\n",
    "    # Store model\n",
    "    torch.save(net.state_dict(), final_state_path + 'final_model%d.pt' % epoch)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # execute only if run as a script\n",
    "    main()\n",
    "    print('Done.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
