{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "import torch\n",
    "import math\n",
    "from model import gan\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "\n",
    "# Unique Naming\n",
    "from datetime import datetime\n",
    "import random, string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_string(length=10):\n",
    "    \"\"\"\n",
    "        Generate a random string of given length. For safely storing produced images.\n",
    "    \"\"\"\n",
    "    letters = string.ascii_lowercase\n",
    "    return ''.join(random.choice(letters) for i in range(length))\n",
    "\n",
    "\n",
    "def get_model_id():\n",
    "    \"\"\"\n",
    "        Creates folder with unique ID in which everything related to a particular testrun can be saved.\n",
    "    :return: Unique folder identifier\n",
    "    \"\"\"\n",
    "    # Construct testrun identifier\n",
    "    TIME_STAMP = datetime.now().strftime(\"%Y_%d_%m__%H_%M_%S__%f_\")\n",
    "    model_folder_id = TIME_STAMP + '_' + random_string() + '/'\n",
    "\n",
    "    try:\n",
    "        os.mkdirs(model_folder_id)\n",
    "    except Exception as e:\n",
    "        print('Exception occurred: ', e)\n",
    "\n",
    "    return model_folder_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "### VARIABLES & ADMINISTRATIVE STUFF ###\n",
    "# System\n",
    "#dataset_path = '/media/daniel/Elements/FastText_Data/'  # In case dataset is stored somewhere else, e.g. on hard-drive\n",
    "dataset_path = '/media/daniel/Elements/FastText_Data/'  #data in same directory\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "# Network\n",
    "embedding_dim = 300\n",
    "internal_dim = 300\n",
    "hidden = 300\n",
    "\n",
    "# Train hyperparameters\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "vocab_size = 2000\n",
    "num_minibatches = vocab_size // batch_size\n",
    "real_label, fake_label = 1, 0\n",
    "languages = {'src': ['de', 'nl'], 'trgt': 'en'}  # Target language to be indicated in last position\n",
    "checkpoint_frequency = 0  # 0 == Off; i > 0 == actual checkpoint frequency in epochs\n",
    "avg_dec_grads = True  # Boolean indicating whether to average the grads of decoder accumulated over nr of source languages by nr of source langs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model ID: 2020_03_06__23_19_13__517510__liaooyyryt/\n"
     ]
    }
   ],
   "source": [
    "# Set up saving paths\n",
    "\n",
    "data_storage_path = './'\n",
    "\n",
    "model_id = get_model_id()\n",
    "\n",
    "checkpoint_path = data_storage_path + model_id + 'Checkpoint/'\n",
    "final_state_path = data_storage_path + model_id + 'Final/'\n",
    "\n",
    "try:\n",
    "    if checkpoint_frequency > 0:\n",
    "        os.mkdir(checkpoint_path)\n",
    "    os.mkdir(final_state_path)\n",
    "except Exception as e:\n",
    "    raise Warning('Exception occurred: Cound not create dirs! Exception:\\n', e)\n",
    "    \n",
    "print('Model ID:', model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaned_vocab(vocab, vocab_size):\n",
    "    # Remove all punctuation tokens while valid nr of tokens is insufficient yet for having full vocab size\n",
    "    # TODO & possibly reserve testing vocab\n",
    "\n",
    "    # Return clean & restricted vocab\n",
    "    words = vocab.words[:vocab_size]              # Y (labels)\n",
    "    vects = [vocab[word] for word in words]       # X (input data)\n",
    "\n",
    "    return vects, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lang_to_vocab(lang_type, lang_id, vocab_size, vocabs):\n",
    "    # Get dataset\n",
    "    if dataset_path == './':\n",
    "        fasttext.util.download_model(lang_id)  # Download word embedding vector data if not available\n",
    "    vocab = fasttext.load_model(dataset_path + 'cc.' + lang_id + '.300.bin')  # Load language data\n",
    "\n",
    "    # Add train data (embedding-vectors) and labels (words) to vocab\n",
    "    x, y = cleaned_vocab(vocab, vocab_size)\n",
    "    vocabs[lang_type][lang_id] = {'x': torch.tensor(x), 'y': y}\n",
    "\n",
    "    return vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab(languages):\n",
    "    nr_src_langs = len(languages)\n",
    "    vocabs = {'src': {}, 'trgt': {}}\n",
    "    \n",
    "    for language in languages['src']:\n",
    "        vocabs = add_lang_to_vocab('src', language, vocab_size, vocabs)\n",
    "        \n",
    "    language = languages['trgt']\n",
    "    vocabs = add_lang_to_vocab('trgt', language, vocab_size, vocabs)\n",
    "\n",
    "    print('Successfully loaded language models.')\n",
    "    return vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded language models.\n"
     ]
    }
   ],
   "source": [
    "    # load vocab\n",
    "    vocabs = load_vocab(languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(data, save):\n",
    "    if save:\n",
    "        torch.save(data, checkpoint_path + 'checkpoint_%d.pt' % data['epoch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_param(model):\n",
    "    return torch.mean(torch.cat([param.data.view(-1) for param in model.parameters()], 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5607, 0.3826, 0.8992, 0.9056, 0.5622, 0.6645, 0.8006],\n",
      "        [0.5582, 0.1138, 0.7248, 0.0861, 0.7134, 0.7503, 0.4936],\n",
      "        [0.1320, 0.3971, 0.7478, 0.3828, 0.2057, 0.8433, 0.3996],\n",
      "        [0.3900, 0.3583, 0.4438, 0.8527, 0.4797, 0.2626, 0.8606],\n",
      "        [0.0792, 0.0668, 0.0232, 0.9301, 0.7002, 0.2058, 0.5192]])\n",
      "tensor([0, 4, 1])\n",
      "tensor([[0.5607, 0.3826, 0.8992, 0.9056, 0.5622, 0.6645, 0.8006],\n",
      "        [0.0792, 0.0668, 0.0232, 0.9301, 0.7002, 0.2058, 0.5192],\n",
      "        [0.5582, 0.1138, 0.7248, 0.0861, 0.7134, 0.7503, 0.4936]])\n"
     ]
    }
   ],
   "source": [
    "x_t = torch.rand([5,7])\n",
    "print(x_t)\n",
    "indices = torch.LongTensor(3).random_(0, len(x_t))\n",
    "print(indices)\n",
    "print(x_t[indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_sample(lang, vocab, batch_size):\n",
    "    \"\"\"\n",
    "    Thiss function draws batch_size-many training samples at random \n",
    "    from a vocab corresponding to queried language.  \n",
    "    \"\"\"\n",
    "    indices = torch.LongTensor(batch_size).random_(0, len(vocab))\n",
    "    return vocab[indices]\n",
    "\n",
    "\n",
    "def get_train_data(languages, vocabs, batch_size):\n",
    "    \"\"\"\n",
    "    Returns one set of samples datapoints form a vocabulary for each provided language.\n",
    "    \"\"\"\n",
    "    samples = {lang: get_dataset_sample(lang, vocab['src'][lang]) for lang in languages['src']}\n",
    "    samples[languages['trgt']] = get_dataset_sample(lang, vocab['trgt'][languages['trgt']])\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nr source languages: 2\n",
      "Nr target languages: 1\n",
      "Decoder gradient averaging factor: 0.5\n",
      "Gan\n",
      "Epoch  0 / 100\n",
      "Mean:  tensor(0.0003)\n",
      "Progress:  349.92917 279.79453 321.1089\n",
      "Epoch  1 / 100\n",
      "Mean:  tensor(-1.6334e-05)\n",
      "Progress:  350.18982 96.84667 300.5612\n",
      "Epoch  2 / 100\n",
      "Mean:  tensor(-0.0002)\n",
      "Progress:  349.85635 120.904205 324.34152\n",
      "Epoch  3 / 100\n",
      "Mean:  tensor(-0.0003)\n",
      "Progress:  349.24692 127.83351 323.16083\n",
      "Epoch  4 / 100\n",
      "Mean:  tensor(-0.0002)\n",
      "Progress:  348.79453 113.63088 305.19528\n",
      "Epoch  5 / 100\n",
      "Mean:  tensor(-0.0003)\n",
      "Progress:  349.00992 95.34325 293.73938\n",
      "Epoch  6 / 100\n",
      "Mean:  tensor(-0.0002)\n",
      "Progress:  349.2224 103.67438 300.47552\n",
      "Epoch  7 / 100\n",
      "Mean:  tensor(-0.0004)\n",
      "Progress:  349.47415 102.670715 298.74066\n",
      "Epoch  8 / 100\n",
      "Mean:  tensor(-0.0005)\n",
      "Progress:  349.46805 119.14551 318.5368\n",
      "Epoch  9 / 100\n",
      "Mean:  tensor(-0.0004)\n",
      "Progress:  348.9474 117.397934 323.54987\n",
      "Epoch  10 / 100\n",
      "Mean:  tensor(-0.0004)\n",
      "Progress:  348.09763 123.50196 323.78452\n",
      "Epoch  11 / 100\n",
      "Mean:  tensor(-0.0005)\n",
      "Progress:  347.43954 108.10123 317.72516\n",
      "Epoch  12 / 100\n",
      "Mean:  tensor(-0.0005)\n",
      "Progress:  346.91257 124.342285 331.57904\n",
      "Epoch  13 / 100\n",
      "Mean:  tensor(-0.0006)\n",
      "Progress:  346.29886 118.4216 325.25168\n",
      "Epoch  14 / 100\n",
      "Mean:  tensor(-0.0005)\n",
      "Progress:  345.9716 113.142296 331.6287\n",
      "Epoch  15 / 100\n",
      "Mean:  tensor(-0.0005)\n",
      "Progress:  345.2662 118.99058 332.6545\n",
      "Epoch  16 / 100\n",
      "Mean:  tensor(-0.0004)\n",
      "Progress:  345.09238 98.78049 322.22714\n",
      "Epoch  17 / 100\n",
      "Mean:  tensor(-0.0004)\n",
      "Progress:  345.9095 114.92456 342.25186\n",
      "Epoch  18 / 100\n",
      "Mean:  tensor(-0.0004)\n",
      "Progress:  347.5343 96.922935 331.74377\n",
      "Epoch  19 / 100\n",
      "Mean:  tensor(-0.0004)\n",
      "Progress:  349.6229 93.385735 335.01794\n",
      "Epoch  20 / 100\n",
      "Mean:  tensor(-0.0004)\n",
      "Progress:  352.90247 85.87305 343.276\n",
      "Epoch  21 / 100\n",
      "Mean:  tensor(-0.0004)\n",
      "Progress:  360.0245 83.636215 347.59357\n",
      "Epoch  22 / 100\n",
      "Mean:  tensor(-0.0004)\n",
      "Progress:  369.48715 72.52041 355.14786\n",
      "Epoch  23 / 100\n",
      "Mean:  tensor(-0.0005)\n",
      "Progress:  381.91986 56.555195 352.2598\n",
      "Epoch  24 / 100\n",
      "Mean:  tensor(-0.0004)\n",
      "Progress:  398.62982 50.978245 359.18964\n",
      "Epoch  25 / 100\n",
      "Mean:  tensor(-0.0006)\n",
      "Progress:  414.33798 38.83532 374.79062\n",
      "Epoch  26 / 100\n",
      "Mean:  tensor(-0.0006)\n",
      "Progress:  434.00488 32.101994 384.00598\n",
      "Epoch  27 / 100\n",
      "Mean:  tensor(-0.0007)\n",
      "Progress:  448.74857 20.568188 398.0359\n",
      "Epoch  28 / 100\n",
      "Mean:  tensor(-0.0006)\n",
      "Progress:  463.4453 17.059439 409.75458\n",
      "Epoch  29 / 100\n",
      "Mean:  tensor(-0.0007)\n",
      "Progress:  479.9878 15.623811 416.88\n",
      "Epoch  30 / 100\n",
      "Mean:  tensor(-0.0006)\n",
      "Progress:  491.0167 11.769884 435.2257\n",
      "Epoch  31 / 100\n",
      "Mean:  tensor(-0.0006)\n",
      "Progress:  503.09235 9.674861 432.80734\n",
      "Epoch  32 / 100\n",
      "Mean:  tensor(-0.0007)\n",
      "Progress:  512.4715 6.9269543 456.3342\n",
      "Epoch  33 / 100\n",
      "Mean:  tensor(-0.0006)\n",
      "Progress:  523.003 6.697877 465.26367\n",
      "Epoch  34 / 100\n",
      "Mean:  tensor(-0.0007)\n",
      "Progress:  528.8074 5.832534 461.2195\n",
      "Epoch  35 / 100\n",
      "Mean:  tensor(-0.0009)\n",
      "Progress:  536.1784 4.4887886 481.22556\n",
      "Epoch  36 / 100\n",
      "Mean:  tensor(-0.0009)\n",
      "Progress:  546.3561 4.0119944 488.4509\n",
      "Epoch  37 / 100\n",
      "Mean:  tensor(-0.0008)\n",
      "Progress:  551.9999 3.9942408 485.00812\n",
      "Epoch  38 / 100\n",
      "Mean:  tensor(-0.0008)\n",
      "Progress:  555.3819 2.6209433 507.4842\n",
      "Epoch  39 / 100\n",
      "Mean:  tensor(-0.0008)\n",
      "Progress:  561.5875 2.3824055 514.15405\n",
      "Epoch  40 / 100\n",
      "Mean:  tensor(-0.0008)\n",
      "Progress:  566.4725 1.8265302 528.66113\n",
      "Epoch  41 / 100\n",
      "Mean:  tensor(-0.0008)\n",
      "Progress:  569.1072 1.5248551 534.7699\n",
      "Epoch  42 / 100\n",
      "Mean:  tensor(-0.0009)\n",
      "Progress:  572.856 1.2589204 546.8198\n",
      "Epoch  43 / 100\n",
      "Mean:  tensor(-0.0008)\n",
      "Progress:  576.77246 1.2397698 555.386\n",
      "Epoch  44 / 100\n",
      "Mean:  tensor(-0.0008)\n",
      "Progress:  579.0897 0.9511132 568.391\n",
      "Epoch  45 / 100\n",
      "Mean:  tensor(-0.0008)\n",
      "Progress:  581.01654 0.94080323 566.7411\n",
      "Epoch  46 / 100\n",
      "Mean:  tensor(-0.0008)\n",
      "Progress:  583.2371 0.8363797 569.4812\n",
      "Epoch  47 / 100\n",
      "Mean:  tensor(-0.0008)\n",
      "Progress:  586.3942 1.015912 561.0749\n",
      "Epoch  48 / 100\n",
      "Mean:  tensor(-0.0008)\n",
      "Progress:  589.8937 0.89336574 572.94244\n",
      "Epoch  49 / 100\n",
      "Mean:  tensor(-0.0008)\n",
      "Progress:  592.58875 1.0476342 565.4165\n",
      "Epoch  50 / 100\n",
      "Mean:  tensor(-0.0008)\n",
      "Progress:  595.169 0.8186068 574.34094\n",
      "Epoch  51 / 100\n",
      "Mean:  tensor(-0.0008)\n",
      "Progress:  597.99945 0.9569586 566.17737\n",
      "Epoch  52 / 100\n",
      "Mean:  tensor(-0.0009)\n",
      "Progress:  600.87714 0.9378658 577.104\n",
      "Epoch  53 / 100\n",
      "Mean:  tensor(-0.0008)\n",
      "Progress:  605.5242 1.1195974 564.1233\n",
      "Epoch  54 / 100\n",
      "Mean:  tensor(-0.0008)\n",
      "Progress:  607.9909 0.86995125 570.66815\n",
      "Epoch  55 / 100\n",
      "Mean:  tensor(-0.0009)\n",
      "Progress:  610.53845 0.621613 585.255\n",
      "Epoch  56 / 100\n",
      "Mean:  tensor(-0.0009)\n",
      "Progress:  614.78357 0.8194371 580.5275\n",
      "Epoch  57 / 100\n",
      "Mean:  tensor(-0.0009)\n",
      "Progress:  618.42413 0.7545531 586.6406\n",
      "Epoch  58 / 100\n",
      "Mean:  tensor(-0.0009)\n",
      "Progress:  619.75757 0.6847306 577.48456\n",
      "Epoch  59 / 100\n",
      "Mean:  tensor(-0.0010)\n",
      "Progress:  622.0849 0.60643184 592.2987\n",
      "Epoch  60 / 100\n",
      "Mean:  tensor(-0.0009)\n",
      "Progress:  626.12823 0.5611599 605.49426\n",
      "Epoch  61 / 100\n",
      "Mean:  tensor(-0.0008)\n",
      "Progress:  628.57513 0.6931591 587.8952\n",
      "Epoch  62 / 100\n",
      "Mean:  tensor(-0.0008)\n",
      "Progress:  630.43805 0.5051066 598.03577\n",
      "Epoch  63 / 100\n",
      "Mean:  tensor(-0.0008)\n",
      "Progress:  634.1514 0.43869346 608.74133\n",
      "Epoch  64 / 100\n",
      "Mean:  tensor(-0.0007)\n",
      "Progress:  637.7533 0.6173144 599.22656\n",
      "Epoch  65 / 100\n",
      "Mean:  tensor(-0.0008)\n",
      "Progress:  639.80615 0.5627779 601.85236\n",
      "Epoch  66 / 100\n",
      "Mean:  tensor(-0.0008)\n",
      "Progress:  641.7966 0.29094848 634.98206\n",
      "Epoch  67 / 100\n",
      "Mean:  tensor(-0.0008)\n",
      "Progress:  644.1146 0.3704655 619.2184\n",
      "Epoch  68 / 100\n",
      "Mean:  tensor(-0.0008)\n",
      "Progress:  647.1983 0.38121474 615.8657\n",
      "Epoch  69 / 100\n",
      "Mean:  tensor(-0.0008)\n",
      "Progress:  650.0865 0.39388874 622.5649\n",
      "Epoch  70 / 100\n",
      "Mean:  tensor(-0.0007)\n",
      "Progress:  651.32947 0.45001858 605.2831\n",
      "Epoch  71 / 100\n",
      "Mean:  tensor(-0.0007)\n",
      "Progress:  653.5578 0.43111935 618.63837\n",
      "Epoch  72 / 100\n",
      "Mean:  tensor(-0.0007)\n",
      "Progress:  659.1712 0.40514076 627.185\n",
      "Epoch  73 / 100\n",
      "Mean:  tensor(-0.0006)\n",
      "Progress:  662.3325 0.4012142 615.88934\n",
      "Epoch  74 / 100\n",
      "Mean:  tensor(-0.0007)\n",
      "Progress:  662.9038 0.25922486 648.08\n",
      "Epoch  75 / 100\n",
      "Mean:  tensor(-0.0008)\n",
      "Progress:  665.07983 0.3508812 629.8311\n",
      "Epoch  76 / 100\n",
      "Mean:  tensor(-0.0007)\n",
      "Progress:  668.07086 0.25964954 642.3848\n",
      "Epoch  77 / 100\n",
      "Mean:  tensor(-0.0007)\n",
      "Progress:  670.711 0.31442454 630.6332\n",
      "Epoch  78 / 100\n",
      "Mean:  tensor(-0.0007)\n",
      "Progress:  673.0942 0.27693728 638.0955\n",
      "Epoch  79 / 100\n",
      "Mean:  tensor(-0.0008)\n",
      "Progress:  675.2212 0.2582908 645.92426\n",
      "Epoch  80 / 100\n",
      "Mean:  tensor(-0.0007)\n",
      "Progress:  677.83923 0.28432935 641.31775\n",
      "Epoch  81 / 100\n",
      "Mean:  tensor(-0.0007)\n",
      "Progress:  680.44763 0.39148232 623.23944\n",
      "Epoch  82 / 100\n",
      "Mean:  tensor(-0.0007)\n",
      "Progress:  684.3476 0.26083195 641.92444\n",
      "Epoch  83 / 100\n",
      "Mean:  tensor(-0.0010)\n",
      "Progress:  688.46075 0.29274103 640.58203\n",
      "Epoch  84 / 100\n",
      "Mean:  tensor(-0.0010)\n",
      "Progress:  690.6496 0.2164731 649.78796\n",
      "Epoch  85 / 100\n",
      "Mean:  tensor(-0.0007)\n",
      "Progress:  691.1901 0.24015719 649.6604\n",
      "Epoch  86 / 100\n",
      "Mean:  tensor(-0.0006)\n",
      "Progress:  694.67993 0.24531744 653.94775\n",
      "Epoch  87 / 100\n",
      "Mean:  tensor(-0.0008)\n",
      "Progress:  698.6479 0.19674987 660.27045\n",
      "Epoch  88 / 100\n",
      "Mean:  tensor(-0.0010)\n",
      "Progress:  699.8782 0.14708394 675.6698\n",
      "Epoch  89 / 100\n",
      "Mean:  tensor(-0.0010)\n",
      "Progress:  700.4754 0.140113 680.4757\n",
      "Epoch  90 / 100\n",
      "Mean:  tensor(-0.0009)\n",
      "Progress:  702.7746 0.24337538 661.19586\n",
      "Epoch  91 / 100\n",
      "Mean:  tensor(-0.0009)\n",
      "Progress:  706.7116 0.123084195 691.59607\n",
      "Epoch  92 / 100\n",
      "Mean:  tensor(-0.0011)\n",
      "Progress:  709.1384 0.19877443 667.4191\n",
      "Epoch  93 / 100\n",
      "Mean:  tensor(-0.0012)\n",
      "Progress:  710.2555 0.17188865 674.78796\n",
      "Epoch  94 / 100\n",
      "Mean:  tensor(-0.0011)\n",
      "Progress:  712.73663 0.20320335 656.1334\n",
      "Epoch  95 / 100\n",
      "Mean:  tensor(-0.0010)\n",
      "Progress:  717.3387 0.16999394 663.88495\n",
      "Epoch  96 / 100\n",
      "Mean:  tensor(-0.0010)\n",
      "Progress:  719.86755 0.15148163 675.73444\n",
      "Epoch  97 / 100\n",
      "Mean:  tensor(-0.0010)\n",
      "Progress:  719.9751 0.16601165 672.7897\n",
      "Epoch  98 / 100\n",
      "Mean:  tensor(-0.0008)\n",
      "Progress:  723.65765 0.16362654 671.249\n",
      "Epoch  99 / 100\n",
      "Mean:  tensor(-0.0008)\n",
      "Progress:  728.10614 0.12161341 685.0932\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \n",
    "    NLLLoss = torch.nn.NLLLoss()\n",
    "    nr_src_langs = len(vocabs['src'])\n",
    "    print('Nr source languages:', nr_src_langs)\n",
    "    print('Nr target languages:', len(vocabs['trgt']))\n",
    "    \n",
    "    if avg_dec_grads:\n",
    "        avg_factor = 1/nr_src_langs\n",
    "        print('Decoder gradient averaging factor:', avg_factor)\n",
    "    \n",
    "    # Get bilingual dictionary for evaluating train loss or at least testing\n",
    "    dicts = dict()\n",
    "    #TODO\n",
    "\n",
    "    # Set up model architecture\n",
    "    net = gan.GAN(embedding_dim, internal_dim, hidden, languages['src'])\n",
    "\n",
    "    # Get optimizers; 1 per source language and 1 for target language\n",
    "    optims_g = {}\n",
    "    for language in languages['src']:\n",
    "\n",
    "        #params = net.generator.encoders[language].parameters() + net.generator.decoder.parameters()\n",
    "        optims_g[language] = torch.optim.Adam([{'params': net.generator.encoders[language].parameters()},\n",
    "                                               {'params': net.generator.decoder.parameters()}],\n",
    "                                              lr=0.0001, betas=(0.9, 0.999), eps=1e-08, \n",
    "                                              weight_decay=0, amsgrad=False)\n",
    "\n",
    "    optim_d = torch.optim.Adam(net.discriminator.parameters(), \n",
    "                               lr=0.0001, betas=(0.9, 0.999), eps=1e-08, \n",
    "                               weight_decay=0, amsgrad=False)\n",
    "\n",
    "    # Train\n",
    "    train_loss_real_d, train_loss_fake_d, train_loss_g = [], [], []\n",
    "    eval_loss = [] # To be populated...\n",
    "    last_loss = -1\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch ', epoch, '/', epochs)\n",
    "        loss_real_total_d, loss_fake_total_d, loss_total_g = 0., 0., 0.\n",
    "\n",
    "        # Shuffle data #\n",
    "        # Source languages\n",
    "        for lang in languages['src']:\n",
    "            vocabs['src'][lang]['x'], vocabs['src'][lang]['y'] = shuffle(np.array(vocabs['src'][lang]['x']), np.array(vocabs['src'][lang]['y']))\n",
    "            vocabs['src'][lang]['x'] = torch.from_numpy(vocabs['src'][lang]['x'])\n",
    "            #vocabs['src'][lang]['y'] = torch.from_numpy(vocabs['src'][lang]['y']) # We don't really need that except for eval\n",
    "        # Target language\n",
    "        lang = languages['trgt'] # Retrieve language id\n",
    "        vocabs['trgt'][lang]['x'], vocabs['trgt'][lang]['y'] = shuffle(np.array(vocabs['trgt'][lang]['x']), np.array(vocabs['trgt'][lang]['y']))\n",
    "        vocabs['trgt'][lang]['x'] = torch.from_numpy(vocabs['trgt'][lang]['x'])\n",
    "\n",
    "        # Train #\n",
    "        for batch in range(num_minibatches):\n",
    "            #print('Epoch ', epoch, ', Batch ', batch, '/', num_minibatches)\n",
    "\n",
    "            # Update discriminator #\n",
    "            net.discriminator.train()\n",
    "            net.generator.eval()\n",
    "            # All-real minibatch\n",
    "            net.discriminator.zero_grad()\n",
    "            x = vocabs['trgt'][languages['trgt']]['x'][batch * batch_size:(batch + 1) * batch_size]#.to(device)\n",
    "            y_true = torch.full((batch_size,), real_label).long() #device=device  # TODO: could probs be converted to long straight away when reading in, already...\n",
    "            y_pred = net.discriminator(x)\n",
    "            # Loss proportional to discriminator's probability of correctly distinguishing TP and FP\n",
    "            loss_real = NLLLoss(torch.log(y_pred+0.0000001), y_true)  # NLLLoss needs log(prob_distribution); adding small amount to avoid log(0)\n",
    "            loss_real.backward()\n",
    "            loss_real_total_d += loss_real\n",
    "\n",
    "            # One minibatch per source language\n",
    "            translations = {}\n",
    "            loss_fake_batch_avg = 0.\n",
    "            for language in languages['src']:\n",
    "                # All-real minibatch\n",
    "                net.discriminator.zero_grad()\n",
    "                x = vocabs['src'][language]['x'][batch * batch_size:(batch + 1) * batch_size]#.to(device)\n",
    "                x = net.generator(x, language)\n",
    "                translations[language] = x\n",
    "                y_true = torch.full((batch_size,), fake_label).long() #, device=device\n",
    "                y_pred = net.discriminator(x.detach())      # Detach to avoid computing grads for generator\n",
    "                # Loss proportional to discriminator's probability of correctly distinguishing TP and FP\n",
    "                loss_fake = NLLLoss(torch.log(y_pred+0.0000001), y_true)  # NLLLoss needs log(prob_distribution); adding small amount to avoid log(0)\n",
    "                loss_fake_batch_avg += loss_fake\n",
    "                loss_fake.backward()    # Compute gradients only for discriminator\n",
    "            optim_d.step()              # Weight update\n",
    "            loss_fake_total_d += (loss_fake_batch_avg/nr_src_langs)\n",
    "\n",
    "            # Update generator #\n",
    "            net.generator.train()\n",
    "            net.discriminator.eval()\n",
    "            # Compute gradients\n",
    "            loss_fake_batch_avg = 0.\n",
    "            for language in languages['src']:\n",
    "                net.generator.encoders[language].zero_grad()\n",
    "                x = translations[language]\n",
    "                y_true = torch.full((batch_size,), real_label).long() #device=device\n",
    "                y_pred = net.discriminator(x)\n",
    "                # Loss proportional to discriminator's probability of confusing TP and FP\n",
    "                loss_fake = NLLLoss(torch.log(y_pred+0.0000001), y_true)  # NLLLoss needs log(prob_distribution); adding small amount to avoid log(0)\n",
    "                loss_fake_batch_avg += loss_fake\n",
    "                loss_fake.backward()\n",
    "            loss_total_g += (loss_fake_batch_avg / nr_src_langs)\n",
    "            \n",
    "            \n",
    "            # TODO: possibly average decoder's gradients over nr of src languages\n",
    "            if avg_dec_grads:\n",
    "                for p in net.generator.decoder.parameters():\n",
    "                    p.grad *= avg_factor\n",
    "            \n",
    "            # Perform weight updates\n",
    "            for language in languages['src']:\n",
    "                optims_g[language].step()\n",
    "            #print(loss_real_total_d, loss_fake_total_d, loss_total_g)\n",
    "        # Document accumulated losses per epoch\n",
    "        train_loss_real_d.append(loss_real_total_d)\n",
    "        train_loss_fake_d.append(loss_fake_total_d)\n",
    "        train_loss_g.append(loss_total_g)\n",
    "        \n",
    "        print('Mean: ', mean_param(net.generator.decoder))\n",
    "        print('Progress: ', loss_real_total_d.detach().numpy(), \n",
    "              loss_fake_total_d.detach().numpy(), loss_total_g.detach().numpy())\n",
    "        \n",
    "        # TODO: Similarity metric-based evaluation per epoch?\n",
    "        \n",
    "        # Save checkpoints\n",
    "        save = last_loss > train_loss_g[-1]  # Provisional: save when loss of generator has improved\n",
    "        last_loss = train_loss_g[-1]\n",
    "        save_checkpoint({'epoch': epoch,\n",
    "                         'model_state_dict': net.state_dict(),\n",
    "                         'optimizer_state_dicts': \n",
    "                             {**{lang: optims_g[lang].state_dict() for lang in languages['src']}, \n",
    "                              **{lang: optim_d.state_dict() for lang in languages['trgt']}\n",
    "                             },\n",
    "                         'losses': {'train_loss_real_d': train_loss_real_d[-1],\n",
    "                                    'train_loss_fake_d': train_loss_fake_d[-1],\n",
    "                                    'train_loss_g': train_loss_g[-1],},\n",
    "                         }, save)\n",
    "\n",
    "    # TODO: Final evaluation\n",
    "\n",
    "    # Store model\n",
    "    torch.save(net.state_dict(), final_state_path + 'final_model%d.pt' % epoch)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # execute only if run as a script\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
