{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "import torch\n",
    "import math\n",
    "from model import gan\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "\n",
    "# Unique Naming\n",
    "from datetime import datetime\n",
    "import random, string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_string(length=10):\n",
    "    \"\"\"\n",
    "        Generate a random string of given length. For safely storing produced images.\n",
    "    \"\"\"\n",
    "    letters = string.ascii_lowercase\n",
    "    return ''.join(random.choice(letters) for i in range(length))\n",
    "\n",
    "\n",
    "def get_model_id():\n",
    "    \"\"\"\n",
    "        Creates folder with unique ID in which everything related to a particular testrun can be saved.\n",
    "    :return: Unique folder identifier\n",
    "    \"\"\"\n",
    "    # Construct testrun identifier\n",
    "    TIME_STAMP = datetime.now().strftime(\"%Y_%d_%m__%H_%M_%S__%f_\")\n",
    "    model_folder_id = TIME_STAMP + '_' + random_string() + '/'\n",
    "\n",
    "    try:\n",
    "        os.mkdirs(model_folder_id)\n",
    "    except Exception as e:\n",
    "        print('Exception occurred: ', e)\n",
    "\n",
    "    return model_folder_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### VARIABLES & ADMINISTRATIVE STUFF ###\n",
    "# System\n",
    "#dataset_path = '/media/daniel/Elements/FastText_Data/'  # In case dataset is stored somewhere else, e.g. on hard-drive\n",
    "dataset_path = ''  #data in same directory\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "# Network\n",
    "embedding_dim = 300\n",
    "internal_dim = 300\n",
    "hidden = 300\n",
    "\n",
    "# Train hyperparameters\n",
    "epochs = 5\n",
    "batch_size = 32\n",
    "vocab_size = 2000\n",
    "num_minibatches = vocab_size // batch_size\n",
    "real_label, fake_label = 1, 0\n",
    "languages = {'src': ['de', 'nl'], 'trgt': ['en']}  # Target language to be indicated in last position\n",
    "checkpoint_frequency = 0  # 0 == Off; i > 0 == actual checkpoint frequency in epochs\n",
    "avg_grads = True  # Boolean indicating whether to average the grads of decoder & discriminator accumulated over nr of source languages by nr of source langs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de\n",
      "nl\n",
      "en\n",
      "{'trgt': ['en'], 'src': ['de', 'nl']}\n"
     ]
    }
   ],
   "source": [
    "# Changed the way languages are stored. \n",
    "# For easy access to complete set of all included languages, just concatenate lists\n",
    "languages = {'src': ['de', 'nl'], 'trgt': ['en']}\n",
    "\n",
    "for lang in languages['src']+languages['trgt']:\n",
    "    print(lang)\n",
    "print(languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception occurred:  module 'os' has no attribute 'mkdirs'\n",
      "Created: ./2020_04_06__02_04_29__163672__mfpcenzavm/Final/\n",
      "Model ID: 2020_04_06__02_04_29__163672__mfpcenzavm/\n"
     ]
    }
   ],
   "source": [
    "# Set up saving paths\n",
    "\n",
    "data_storage_path = './'\n",
    "\n",
    "model_id = get_model_id()\n",
    "\n",
    "checkpoint_path = data_storage_path + model_id + 'Checkpoint/'\n",
    "final_state_path = data_storage_path + model_id + 'Final/'\n",
    "\n",
    "try:\n",
    "    if checkpoint_frequency > 0:\n",
    "        os.makedirs(checkpoint_path)\n",
    "        print('Created:', checkpoint_path)\n",
    "    os.makedirs(final_state_path)\n",
    "    print('Created:', final_state_path)\n",
    "except Exception as e:\n",
    "    raise Warning('Exception occurred: Cound not create dirs! Exception:', e)\n",
    "    \n",
    "print('Model ID:', model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaned_vocab(vocab, vocab_size):\n",
    "    # Remove all punctuation tokens while valid nr of tokens is insufficient yet for having full vocab size\n",
    "    # TODO & possibly reserve testing vocab\n",
    "\n",
    "    # Return clean & restricted vocab\n",
    "    words = vocab.words[:vocab_size]              # Y (labels)\n",
    "    vects = [vocab[word] for word in words]       # X (input data)\n",
    "\n",
    "    return vects, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lang_to_vocab(lang_id, vocab_size, vocabs):\n",
    "    # Get dataset\n",
    "    if dataset_path == './':\n",
    "        fasttext.util.download_model(lang_id)  # Download word embedding vector data if not available\n",
    "    vocab = fasttext.load_model(dataset_path + 'cc.' + lang_id + '.300.bin')  # Load language data\n",
    "\n",
    "    # Add train data (embedding-vectors) and labels (words) to vocab\n",
    "    x, y = cleaned_vocab(vocab, vocab_size)\n",
    "    vocabs[lang_id] = {'x': torch.tensor(x), 'y': y}\n",
    "\n",
    "    return vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab(languages):\n",
    "    vocabs = {}\n",
    "    \n",
    "    for language in languages['src']+languages['trgt']:\n",
    "        vocabs = add_lang_to_vocab(language, vocab_size, vocabs)\n",
    "\n",
    "    print('Successfully loaded language models.')\n",
    "    return vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded language models.\n"
     ]
    }
   ],
   "source": [
    "    # load vocab\n",
    "    vocabs = load_vocab(languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define training related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(data, save):\n",
    "    if save:\n",
    "        torch.save(data, checkpoint_path + 'checkpoint_%d.pt' % data['epoch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_param(model):\n",
    "    return torch.mean(torch.cat([param.data.view(-1) for param in model.parameters()], 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_sample(lang, vocab, batch_size, include_y=False):\n",
    "    \"\"\"\n",
    "    Thiss function draws batch_size-many training samples at random \n",
    "    from a vocab corresponding to queried language.  \n",
    "    \"\"\"\n",
    "    indices = torch.LongTensor(batch_size).random_(0, len(vocab))\n",
    "    if include_y:\n",
    "        return vocab['x'][indices], vocab['y'][indices]\n",
    "    return vocab['x'][indices]\n",
    "\n",
    "\n",
    "def get_train_data(languages, vocabs, batch_size, include_y=False):\n",
    "    \"\"\"\n",
    "    Returns one set of samples datapoints form a vocabulary for each provided language.\n",
    "    \"\"\"\n",
    "    x, y = {}, {}\n",
    "    \n",
    "    # Source languages\n",
    "    for lang in languages['src']+languages['trgt']:\n",
    "        if include_y:\n",
    "            x[lang], y[lang] = get_dataset_sample(lang, vocabs[lang], batch_size, include_y)\n",
    "        else:\n",
    "            x[lang] = get_dataset_sample(lang, vocabs[lang], batch_size)\n",
    "    \n",
    "    # Return\n",
    "    if include_y:\n",
    "        return x, y\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nr source languages: 2\n",
      "Nr target languages: 1\n",
      "{'trgt': ['en'], 'src': ['de', 'nl']}\n",
      "Decoder gradient averaging factor: 0.5\n",
      "Epoch  0 / 5\n",
      "Progress:  249.00888 94.55626 221.33014 176.41458\n",
      "221.33014 176.41458\n",
      "Epoch  1 / 5\n",
      "Progress:  68.60198 26.276072 45.138794 142.06871\n",
      "45.138794 142.06871\n",
      "Epoch  2 / 5\n",
      "Progress:  41.870083 37.457222 48.166256 60.8237\n",
      "48.166256 60.8237\n",
      "Epoch  3 / 5\n",
      "Progress:  43.026512 40.902798 44.953365 53.800385\n",
      "44.953365 53.800385\n",
      "Epoch  4 / 5\n",
      "Progress:  41.301746 43.934948 47.35086 51.10363\n",
      "47.35086 51.10363\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \n",
    "    NLLLoss = torch.nn.NLLLoss()\n",
    "    nr_src_langs = len(languages['src'])\n",
    "    print('Nr source languages:', nr_src_langs)\n",
    "    print('Nr target languages:', len(languages['trgt']))\n",
    "    \n",
    "    print(languages)\n",
    "    \n",
    "    if avg_grads:\n",
    "        avg_factor = 1/nr_src_langs\n",
    "        print('Decoder gradient averaging factor:', avg_factor)\n",
    "    \n",
    "    # Get bilingual dictionary for evaluating train loss or at least testing\n",
    "    dicts = dict()\n",
    "    #TODO\n",
    "\n",
    "    # Set up model architecture\n",
    "    net = gan.GAN(embedding_dim, internal_dim, hidden, languages['src'])\n",
    "\n",
    "    # Get optimizers; 1 per source language of encoder and 1 for discriminator\n",
    "    optimizers = {'gen': {}}\n",
    "    for lang in languages['src']:\n",
    "        optimizers['gen'][lang] = torch.optim.Adam([{'params': net.generator.encoders[lang].parameters()},\n",
    "                                                    {'params': net.generator.decoder.parameters()}],\n",
    "                                                    lr=0.001, betas=(0.9, 0.999), eps=1e-08, \n",
    "                                                    weight_decay=0, amsgrad=False)\n",
    "    optimizers['dis'] = torch.optim.Adam(net.discriminator.parameters(),\n",
    "                                         lr=0.001, betas=(0.9, 0.999), eps=1e-08, \n",
    "                                         weight_decay=0, amsgrad=False)\n",
    "    \n",
    "    # Train\n",
    "    train_loss_real_d, train_loss_fake_d = [], []\n",
    "    train_loss_real_g, train_loss_fake_g = [], []\n",
    "    eval_loss = [] # TODO: To be populated...\n",
    "    last_loss = -1\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch ', epoch, '/', epochs)\n",
    "        loss_real_total_d, loss_fake_total_d, loss_real_total_g, loss_fake_total_g = 0., 0., 0., 0.\n",
    "\n",
    "        # Train #\n",
    "        for batch in range(num_minibatches):\n",
    "            #print('Epoch ', epoch, ', Batch ', batch, '/', num_minibatches)\n",
    "            \n",
    "            # Update discriminator #\n",
    "            net.discriminator.train()\n",
    "            net.generator.eval()\n",
    "            net.discriminator.zero_grad()\n",
    "            \n",
    "            x = get_train_data(languages, vocabs, batch_size)#.to(device)\n",
    "            \n",
    "            # All-fake minibatches - One minibatch per source language\n",
    "            loss_fake_batch_avg = 0.\n",
    "            y_true = torch.full((batch_size,), fake_label).long()#.to(device)\n",
    "            for language in languages['src']:\n",
    "                x_fake = x[lang]\n",
    "                x_trans = net.generator(x_fake, language)\n",
    "                y_pred = net.discriminator(x_trans.detach())      # Detach to avoid computing grads for generator\n",
    "                \n",
    "                # Loss proportional to discriminator's probability of correctly distinguishing TP and FP\n",
    "                loss_fake = NLLLoss(torch.log(y_pred+0.0000001), y_true)  # NLLLoss needs log(prob_distribution); adding small amount to avoid log(0)\n",
    "                loss_fake.backward()    # Compute gradients only for discriminator\n",
    "                loss_fake_batch_avg += loss_fake\n",
    "            \n",
    "            loss_fake_total_d += (loss_fake_batch_avg/nr_src_langs)\n",
    "            \n",
    "            # Possibly average discriminators's gradients over nr of src languages \n",
    "            # (--> ensures all-fake loss's contribution is equal to that of all-real data)\n",
    "            if avg_grads:\n",
    "                for p in net.discriminator.parameters():\n",
    "                    p.grad *= avg_factor\n",
    "            \n",
    "            # All-real minibatch\n",
    "            x_real = x[languages['trgt'][0]]  # Extract all-real data\n",
    "            y_true = torch.full((batch_size,), real_label).long()#.to(device)\n",
    "            y_pred = net.discriminator(x_real)\n",
    "            \n",
    "            # Loss proportional to discriminator's probability of correctly distinguishing TP and FP\n",
    "            loss_real = NLLLoss(torch.log(y_pred+0.0000001), y_true)  # NLLLoss needs log(prob_distribution); adding small amount to avoid log(0)\n",
    "            loss_real.backward()\n",
    "            loss_real_total_d += loss_real\n",
    "                    \n",
    "            optimizers['dis'].step() # Weight update for discriminator\n",
    "\n",
    "            # Update generator #\n",
    "            net.generator.train()\n",
    "            net.discriminator.eval()\n",
    "            net.generator.zero_grad()\n",
    "            \n",
    "            x = get_train_data(languages, vocabs, batch_size)#.to(device)\n",
    "            \n",
    "            # All-fake minibatches - One minibatch per source language\n",
    "            loss_fake_batch_avg = 0.\n",
    "            y_true = torch.full((batch_size,), real_label).long()#.to(device) # Try to fool the discriinator\n",
    "            for language in languages['src']:\n",
    "                x_fake = x[lang]\n",
    "                x_trans = net.generator(x_fake, language)\n",
    "                y_pred = net.discriminator(x_trans)      # Detach to avoid computing grads for generator\n",
    "                \n",
    "                # Loss proportional to discriminator's probability of misclassifying TP and FP\n",
    "                loss_fake = NLLLoss(torch.log(y_pred+0.0000001), y_true)  # NLLLoss needs log(prob_distribution); adding small amount to avoid log(0)\n",
    "                loss_fake.backward()    # Compute gradients only for discriminator\n",
    "                loss_fake_batch_avg += loss_fake\n",
    "            \n",
    "            loss_fake_total_g += (loss_fake_batch_avg/nr_src_langs)\n",
    "            \n",
    "            # Possibly average decoder's gradients over nr of src languages \n",
    "            # (--> ensures that decoder isn't trained as many times as there are source langs per train step)\n",
    "            if avg_grads:\n",
    "                for p in net.generator.decoder.parameters():\n",
    "                    p.grad *= avg_factor\n",
    "            \n",
    "            # All-real minibatch\n",
    "            x_real = x[languages['trgt'][0]]  # Extract all-real data\n",
    "            y_true = torch.full((batch_size,), fake_label).long()#.to(device)  # Pretend true targets were fake\n",
    "            y_pred = net.discriminator(x_real)\n",
    "            \n",
    "            # Loss proportional to discriminator's probability of misclassifying TP and FP\n",
    "            loss_real = NLLLoss(torch.log(y_pred+0.0000001), y_true)  # NLLLoss needs log(prob_distribution); adding small amount to avoid log(0)\n",
    "            loss_real.backward()\n",
    "            loss_real_total_g += loss_real\n",
    "                    \n",
    "            # Perform weight updates\n",
    "            for language in languages['src']:\n",
    "                optimizers['gen'][language].step()\n",
    "            \n",
    "            #print(loss_real_total_d, loss_fake_total_d, loss_real_total_g, loss_fake_total_g)\n",
    "        \n",
    "        # Document accumulated losses per epoch\n",
    "        train_loss_real_d.append(loss_real_total_d)\n",
    "        train_loss_fake_d.append(loss_fake_total_d)\n",
    "        train_loss_real_g.append(loss_real_total_g)\n",
    "        train_loss_fake_g.append(loss_fake_total_g)\n",
    "        \n",
    "        #print('Mean: ', mean_param(net.generator.decoder))\n",
    "        print('Progress: ', loss_real_total_d.detach().numpy(), \n",
    "                            loss_fake_total_d.detach().numpy(),\n",
    "                            loss_real_total_g.detach().numpy(), \n",
    "                            loss_fake_total_g.detach().numpy())\n",
    "        \n",
    "        # TODO: Similarity metric-based evaluation per epoch?\n",
    "        \n",
    "        # Save checkpoints\n",
    "        print(loss_real_total_g.detach().numpy(), loss_fake_total_g.detach().numpy())\n",
    "        save = checkpoint_frequency > 0 and epoch % checkpoint_frequency == 0 and \\\n",
    "            last_loss > loss_real_total_g+loss_fake_total_g  # Provisional: save when loss of generator has improved\n",
    "        last_loss = loss_real_total_g+loss_fake_total_g\n",
    "        save_checkpoint({'epoch': epoch,\n",
    "                         'model_state_dict': net.state_dict(),\n",
    "                         'optimizer_state_dicts': \n",
    "                             {**{lang: optimizers['gen'][lang].state_dict() for lang in languages['src']}, \n",
    "                              **{languages['trgt'][0]: optimizers['dis']}\n",
    "                             },\n",
    "                         'losses': {'train_loss_real_d': train_loss_real_d[-1],\n",
    "                                    'train_loss_fake_d': train_loss_fake_d[-1],\n",
    "                                    'train_loss_real_g': train_loss_real_g[-1],\n",
    "                                    'train_loss_fake_g': train_loss_fake_g[-1],},\n",
    "                         }, save)\n",
    "\n",
    "    # TODO: Final evaluation\n",
    "\n",
    "    # Store model\n",
    "    torch.save(net.state_dict(), final_state_path + 'final_model%d.pt' % epoch)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # execute only if run as a script\n",
    "    main()\n",
    "    print('Done.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
