{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import fasttext.util\n",
    "import torch\n",
    "import math\n",
    "from model import gan\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### VARIABLES & ADMINISTRATIVE STUFF ###\n",
    "# System\n",
    "#dataset_path = '/media/daniel/Elements/FastText_Data/'  # In case dataset is stored somewhere else, e.g. on hard-drive\n",
    "dataset_path = '/media/daniel/Elements/FastText_Data/'  #data in same directory\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "# Network\n",
    "embedding_dim = 300\n",
    "internal_dim = 300\n",
    "hidden = 300\n",
    "\n",
    "# Train hyperparameters\n",
    "epochs = 2\n",
    "batch_size = 32\n",
    "vocab_size = 2000\n",
    "num_minibatches = vocab_size // batch_size\n",
    "real_label, fake_label = 1, 0\n",
    "languages = {'src': ['de', 'nl'], 'trgt': 'en'}  # Target language to be indicated in last position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaned_vocab(vocab, vocab_size):\n",
    "    # Remove all punctuation tokens while valid nr of tokens is insufficient yet for having full vocab size\n",
    "    # TODO & possibly reserve testing vocab\n",
    "\n",
    "    # Return clean & restricted vocab\n",
    "    words = vocab.words[:vocab_size]              # Y (labels)\n",
    "    vects = [vocab[word] for word in words]       # X (input data)\n",
    "\n",
    "    return vects, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lang_to_vocab(lang_type, lang_id, vocab_size, vocabs):\n",
    "    # Get dataset\n",
    "    if dataset_path == './':\n",
    "        fasttext.util.download_model(lang_id)  # Download word embedding vector data if not available\n",
    "    vocab = fasttext.load_model(dataset_path + 'cc.' + lang_id + '.300.bin')  # Load language data\n",
    "\n",
    "    # Add train data (embedding-vectors) and labels (words) to vocab\n",
    "    x, y = cleaned_vocab(vocab, vocab_size)\n",
    "    vocabs[lang_type][lang_id] = {'x': torch.tensor(x), 'y': y}\n",
    "\n",
    "    return vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab(languages):\n",
    "    nr_src_langs = len(languages)\n",
    "    vocabs = {'src': {}, 'trgt': {}}\n",
    "    \n",
    "    for language in languages['src']:\n",
    "        vocabs = add_lang_to_vocab('src', language, vocab_size, vocabs)\n",
    "        \n",
    "    language = languages['trgt']\n",
    "    vocabs = add_lang_to_vocab('trgt', language, vocab_size, vocabs)\n",
    "\n",
    "    print('Successfully loaded language models.')\n",
    "    return vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded language models.\n"
     ]
    }
   ],
   "source": [
    "    # load vocab\n",
    "    vocabs = load_vocab(languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nr source languages: 2\n",
      "Nr target languages: 1\n",
      "Epoch  0 / 10\n",
      "Epoch  0 , Batch  0 / 62\n",
      "Epoch  0 , Batch  1 / 62\n",
      "Epoch  0 , Batch  2 / 62\n",
      "Epoch  0 , Batch  3 / 62\n",
      "Epoch  0 , Batch  4 / 62\n",
      "Epoch  0 , Batch  5 / 62\n",
      "Epoch  0 , Batch  6 / 62\n",
      "Epoch  0 , Batch  7 / 62\n",
      "Epoch  0 , Batch  8 / 62\n",
      "Epoch  0 , Batch  9 / 62\n",
      "Epoch  0 , Batch  10 / 62\n",
      "Epoch  0 , Batch  11 / 62\n",
      "Epoch  0 , Batch  12 / 62\n",
      "Epoch  0 , Batch  13 / 62\n",
      "Epoch  0 , Batch  14 / 62\n",
      "Epoch  0 , Batch  15 / 62\n",
      "Epoch  0 , Batch  16 / 62\n",
      "Epoch  0 , Batch  17 / 62\n",
      "Epoch  0 , Batch  18 / 62\n",
      "Epoch  0 , Batch  19 / 62\n",
      "Epoch  0 , Batch  20 / 62\n",
      "Epoch  0 , Batch  21 / 62\n",
      "Epoch  0 , Batch  22 / 62\n",
      "Epoch  0 , Batch  23 / 62\n",
      "Epoch  0 , Batch  24 / 62\n",
      "Epoch  0 , Batch  25 / 62\n",
      "Epoch  0 , Batch  26 / 62\n",
      "Epoch  0 , Batch  27 / 62\n",
      "Epoch  0 , Batch  28 / 62\n",
      "Epoch  0 , Batch  29 / 62\n",
      "Epoch  0 , Batch  30 / 62\n",
      "Epoch  0 , Batch  31 / 62\n",
      "Epoch  0 , Batch  32 / 62\n",
      "Epoch  0 , Batch  33 / 62\n",
      "Epoch  0 , Batch  34 / 62\n",
      "Epoch  0 , Batch  35 / 62\n",
      "Epoch  0 , Batch  36 / 62\n",
      "Epoch  0 , Batch  37 / 62\n",
      "Epoch  0 , Batch  38 / 62\n",
      "Epoch  0 , Batch  39 / 62\n",
      "Epoch  0 , Batch  40 / 62\n",
      "Epoch  0 , Batch  41 / 62\n",
      "Epoch  0 , Batch  42 / 62\n",
      "Epoch  0 , Batch  43 / 62\n",
      "Epoch  0 , Batch  44 / 62\n",
      "Epoch  0 , Batch  45 / 62\n",
      "Epoch  0 , Batch  46 / 62\n",
      "Epoch  0 , Batch  47 / 62\n",
      "Epoch  0 , Batch  48 / 62\n",
      "Epoch  0 , Batch  49 / 62\n",
      "Epoch  0 , Batch  50 / 62\n",
      "Epoch  0 , Batch  51 / 62\n",
      "Epoch  0 , Batch  52 / 62\n",
      "Epoch  0 , Batch  53 / 62\n",
      "Epoch  0 , Batch  54 / 62\n",
      "Epoch  0 , Batch  55 / 62\n",
      "Epoch  0 , Batch  56 / 62\n",
      "Epoch  0 , Batch  57 / 62\n",
      "Epoch  0 , Batch  58 / 62\n",
      "Epoch  0 , Batch  59 / 62\n",
      "Epoch  0 , Batch  60 / 62\n",
      "Epoch  0 , Batch  61 / 62\n",
      "Progress:  tensor(352.4573, grad_fn=<AddBackward0>) tensor(151.6003, grad_fn=<AddBackward0>) tensor(348.9939, grad_fn=<AddBackward0>)\n",
      "Epoch  1 / 10\n",
      "Epoch  1 , Batch  0 / 62\n",
      "Epoch  1 , Batch  1 / 62\n",
      "Epoch  1 , Batch  2 / 62\n",
      "Epoch  1 , Batch  3 / 62\n",
      "Epoch  1 , Batch  4 / 62\n",
      "Epoch  1 , Batch  5 / 62\n",
      "Epoch  1 , Batch  6 / 62\n",
      "Epoch  1 , Batch  7 / 62\n",
      "Epoch  1 , Batch  8 / 62\n",
      "Epoch  1 , Batch  9 / 62\n",
      "Epoch  1 , Batch  10 / 62\n",
      "Epoch  1 , Batch  11 / 62\n",
      "Epoch  1 , Batch  12 / 62\n",
      "Epoch  1 , Batch  13 / 62\n",
      "Epoch  1 , Batch  14 / 62\n",
      "Epoch  1 , Batch  15 / 62\n",
      "Epoch  1 , Batch  16 / 62\n",
      "Epoch  1 , Batch  17 / 62\n",
      "Epoch  1 , Batch  18 / 62\n",
      "Epoch  1 , Batch  19 / 62\n",
      "Epoch  1 , Batch  20 / 62\n",
      "Epoch  1 , Batch  21 / 62\n",
      "Epoch  1 , Batch  22 / 62\n",
      "Epoch  1 , Batch  23 / 62\n",
      "Epoch  1 , Batch  24 / 62\n",
      "Epoch  1 , Batch  25 / 62\n",
      "Epoch  1 , Batch  26 / 62\n",
      "Epoch  1 , Batch  27 / 62\n",
      "Epoch  1 , Batch  28 / 62\n",
      "Epoch  1 , Batch  29 / 62\n",
      "Epoch  1 , Batch  30 / 62\n",
      "Epoch  1 , Batch  31 / 62\n",
      "Epoch  1 , Batch  32 / 62\n",
      "Epoch  1 , Batch  33 / 62\n",
      "Epoch  1 , Batch  34 / 62\n",
      "Epoch  1 , Batch  35 / 62\n",
      "Epoch  1 , Batch  36 / 62\n",
      "Epoch  1 , Batch  37 / 62\n",
      "Epoch  1 , Batch  38 / 62\n",
      "Epoch  1 , Batch  39 / 62\n",
      "Epoch  1 , Batch  40 / 62\n",
      "Epoch  1 , Batch  41 / 62\n",
      "Epoch  1 , Batch  42 / 62\n",
      "Epoch  1 , Batch  43 / 62\n",
      "Epoch  1 , Batch  44 / 62\n",
      "Epoch  1 , Batch  45 / 62\n",
      "Epoch  1 , Batch  46 / 62\n",
      "Epoch  1 , Batch  47 / 62\n",
      "Epoch  1 , Batch  48 / 62\n",
      "Epoch  1 , Batch  49 / 62\n",
      "Epoch  1 , Batch  50 / 62\n",
      "Epoch  1 , Batch  51 / 62\n",
      "Epoch  1 , Batch  52 / 62\n",
      "Epoch  1 , Batch  53 / 62\n",
      "Epoch  1 , Batch  54 / 62\n",
      "Epoch  1 , Batch  55 / 62\n",
      "Epoch  1 , Batch  56 / 62\n",
      "Epoch  1 , Batch  57 / 62\n",
      "Epoch  1 , Batch  58 / 62\n",
      "Epoch  1 , Batch  59 / 62\n",
      "Epoch  1 , Batch  60 / 62\n",
      "Epoch  1 , Batch  61 / 62\n",
      "Progress:  tensor(nan, grad_fn=<AddBackward0>) tensor(nan, grad_fn=<AddBackward0>) tensor(nan, grad_fn=<AddBackward0>)\n",
      "Epoch  2 / 10\n",
      "Epoch  2 , Batch  0 / 62\n",
      "Epoch  2 , Batch  1 / 62\n",
      "Epoch  2 , Batch  2 / 62\n",
      "Epoch  2 , Batch  3 / 62\n",
      "Epoch  2 , Batch  4 / 62\n",
      "Epoch  2 , Batch  5 / 62\n",
      "Epoch  2 , Batch  6 / 62\n",
      "Epoch  2 , Batch  7 / 62\n",
      "Epoch  2 , Batch  8 / 62\n",
      "Epoch  2 , Batch  9 / 62\n",
      "Epoch  2 , Batch  10 / 62\n",
      "Epoch  2 , Batch  11 / 62\n",
      "Epoch  2 , Batch  12 / 62\n",
      "Epoch  2 , Batch  13 / 62\n",
      "Epoch  2 , Batch  14 / 62\n",
      "Epoch  2 , Batch  15 / 62\n",
      "Epoch  2 , Batch  16 / 62\n",
      "Epoch  2 , Batch  17 / 62\n",
      "Epoch  2 , Batch  18 / 62\n",
      "Epoch  2 , Batch  19 / 62\n",
      "Epoch  2 , Batch  20 / 62\n",
      "Epoch  2 , Batch  21 / 62\n",
      "Epoch  2 , Batch  22 / 62\n",
      "Epoch  2 , Batch  23 / 62\n",
      "Epoch  2 , Batch  24 / 62\n",
      "Epoch  2 , Batch  25 / 62\n",
      "Epoch  2 , Batch  26 / 62\n",
      "Epoch  2 , Batch  27 / 62\n",
      "Epoch  2 , Batch  28 / 62\n",
      "Epoch  2 , Batch  29 / 62\n",
      "Epoch  2 , Batch  30 / 62\n",
      "Epoch  2 , Batch  31 / 62\n",
      "Epoch  2 , Batch  32 / 62\n",
      "Epoch  2 , Batch  33 / 62\n",
      "Epoch  2 , Batch  34 / 62\n",
      "Epoch  2 , Batch  35 / 62\n",
      "Epoch  2 , Batch  36 / 62\n",
      "Epoch  2 , Batch  37 / 62\n",
      "Epoch  2 , Batch  38 / 62\n",
      "Epoch  2 , Batch  39 / 62\n",
      "Epoch  2 , Batch  40 / 62\n",
      "Epoch  2 , Batch  41 / 62\n",
      "Epoch  2 , Batch  42 / 62\n",
      "Epoch  2 , Batch  43 / 62\n",
      "Epoch  2 , Batch  44 / 62\n",
      "Epoch  2 , Batch  45 / 62\n",
      "Epoch  2 , Batch  46 / 62\n",
      "Epoch  2 , Batch  47 / 62\n",
      "Epoch  2 , Batch  48 / 62\n",
      "Epoch  2 , Batch  49 / 62\n",
      "Epoch  2 , Batch  50 / 62\n",
      "Epoch  2 , Batch  51 / 62\n",
      "Epoch  2 , Batch  52 / 62\n",
      "Epoch  2 , Batch  53 / 62\n",
      "Epoch  2 , Batch  54 / 62\n",
      "Epoch  2 , Batch  55 / 62\n",
      "Epoch  2 , Batch  56 / 62\n",
      "Epoch  2 , Batch  57 / 62\n",
      "Epoch  2 , Batch  58 / 62\n",
      "Epoch  2 , Batch  59 / 62\n",
      "Epoch  2 , Batch  60 / 62\n",
      "Epoch  2 , Batch  61 / 62\n",
      "Progress:  tensor(nan, grad_fn=<AddBackward0>) tensor(nan, grad_fn=<AddBackward0>) tensor(nan, grad_fn=<AddBackward0>)\n",
      "Epoch  3 / 10\n",
      "Epoch  3 , Batch  0 / 62\n",
      "Epoch  3 , Batch  1 / 62\n",
      "Epoch  3 , Batch  2 / 62\n",
      "Epoch  3 , Batch  3 / 62\n",
      "Epoch  3 , Batch  4 / 62\n",
      "Epoch  3 , Batch  5 / 62\n",
      "Epoch  3 , Batch  6 / 62\n",
      "Epoch  3 , Batch  7 / 62\n",
      "Epoch  3 , Batch  8 / 62\n",
      "Epoch  3 , Batch  9 / 62\n",
      "Epoch  3 , Batch  10 / 62\n",
      "Epoch  3 , Batch  11 / 62\n",
      "Epoch  3 , Batch  12 / 62\n",
      "Epoch  3 , Batch  13 / 62\n",
      "Epoch  3 , Batch  14 / 62\n",
      "Epoch  3 , Batch  15 / 62\n",
      "Epoch  3 , Batch  16 / 62\n",
      "Epoch  3 , Batch  17 / 62\n",
      "Epoch  3 , Batch  18 / 62\n",
      "Epoch  3 , Batch  19 / 62\n",
      "Epoch  3 , Batch  20 / 62\n",
      "Epoch  3 , Batch  21 / 62\n",
      "Epoch  3 , Batch  22 / 62\n",
      "Epoch  3 , Batch  23 / 62\n",
      "Epoch  3 , Batch  24 / 62\n",
      "Epoch  3 , Batch  25 / 62\n",
      "Epoch  3 , Batch  26 / 62\n",
      "Epoch  3 , Batch  27 / 62\n",
      "Epoch  3 , Batch  28 / 62\n",
      "Epoch  3 , Batch  29 / 62\n",
      "Epoch  3 , Batch  30 / 62\n",
      "Epoch  3 , Batch  31 / 62\n",
      "Epoch  3 , Batch  32 / 62\n",
      "Epoch  3 , Batch  33 / 62\n",
      "Epoch  3 , Batch  34 / 62\n",
      "Epoch  3 , Batch  35 / 62\n",
      "Epoch  3 , Batch  36 / 62\n",
      "Epoch  3 , Batch  37 / 62\n",
      "Epoch  3 , Batch  38 / 62\n",
      "Epoch  3 , Batch  39 / 62\n",
      "Epoch  3 , Batch  40 / 62\n",
      "Epoch  3 , Batch  41 / 62\n",
      "Epoch  3 , Batch  42 / 62\n",
      "Epoch  3 , Batch  43 / 62\n",
      "Epoch  3 , Batch  44 / 62\n",
      "Epoch  3 , Batch  45 / 62\n",
      "Epoch  3 , Batch  46 / 62\n",
      "Epoch  3 , Batch  47 / 62\n",
      "Epoch  3 , Batch  48 / 62\n",
      "Epoch  3 , Batch  49 / 62\n",
      "Epoch  3 , Batch  50 / 62\n",
      "Epoch  3 , Batch  51 / 62\n",
      "Epoch  3 , Batch  52 / 62\n",
      "Epoch  3 , Batch  53 / 62\n",
      "Epoch  3 , Batch  54 / 62\n",
      "Epoch  3 , Batch  55 / 62\n",
      "Epoch  3 , Batch  56 / 62\n",
      "Epoch  3 , Batch  57 / 62\n",
      "Epoch  3 , Batch  58 / 62\n",
      "Epoch  3 , Batch  59 / 62\n",
      "Epoch  3 , Batch  60 / 62\n",
      "Epoch  3 , Batch  61 / 62\n",
      "Progress:  tensor(nan, grad_fn=<AddBackward0>) tensor(nan, grad_fn=<AddBackward0>) tensor(nan, grad_fn=<AddBackward0>)\n",
      "Epoch  4 / 10\n",
      "Epoch  4 , Batch  0 / 62\n",
      "Epoch  4 , Batch  1 / 62\n",
      "Epoch  4 , Batch  2 / 62\n",
      "Epoch  4 , Batch  3 / 62\n",
      "Epoch  4 , Batch  4 / 62\n",
      "Epoch  4 , Batch  5 / 62\n",
      "Epoch  4 , Batch  6 / 62\n",
      "Epoch  4 , Batch  7 / 62\n",
      "Epoch  4 , Batch  8 / 62\n",
      "Epoch  4 , Batch  9 / 62\n",
      "Epoch  4 , Batch  10 / 62\n",
      "Epoch  4 , Batch  11 / 62\n",
      "Epoch  4 , Batch  12 / 62\n",
      "Epoch  4 , Batch  13 / 62\n",
      "Epoch  4 , Batch  14 / 62\n",
      "Epoch  4 , Batch  15 / 62\n",
      "Epoch  4 , Batch  16 / 62\n",
      "Epoch  4 , Batch  17 / 62\n",
      "Epoch  4 , Batch  18 / 62\n",
      "Epoch  4 , Batch  19 / 62\n",
      "Epoch  4 , Batch  20 / 62\n",
      "Epoch  4 , Batch  21 / 62\n",
      "Epoch  4 , Batch  22 / 62\n",
      "Epoch  4 , Batch  23 / 62\n",
      "Epoch  4 , Batch  24 / 62\n",
      "Epoch  4 , Batch  25 / 62\n",
      "Epoch  4 , Batch  26 / 62\n",
      "Epoch  4 , Batch  27 / 62\n",
      "Epoch  4 , Batch  28 / 62\n",
      "Epoch  4 , Batch  29 / 62\n",
      "Epoch  4 , Batch  30 / 62\n",
      "Epoch  4 , Batch  31 / 62\n",
      "Epoch  4 , Batch  32 / 62\n",
      "Epoch  4 , Batch  33 / 62\n",
      "Epoch  4 , Batch  34 / 62\n",
      "Epoch  4 , Batch  35 / 62\n",
      "Epoch  4 , Batch  36 / 62\n",
      "Epoch  4 , Batch  37 / 62\n",
      "Epoch  4 , Batch  38 / 62\n",
      "Epoch  4 , Batch  39 / 62\n",
      "Epoch  4 , Batch  40 / 62\n",
      "Epoch  4 , Batch  41 / 62\n",
      "Epoch  4 , Batch  42 / 62\n",
      "Epoch  4 , Batch  43 / 62\n",
      "Epoch  4 , Batch  44 / 62\n",
      "Epoch  4 , Batch  45 / 62\n",
      "Epoch  4 , Batch  46 / 62\n",
      "Epoch  4 , Batch  47 / 62\n",
      "Epoch  4 , Batch  48 / 62\n",
      "Epoch  4 , Batch  49 / 62\n",
      "Epoch  4 , Batch  50 / 62\n",
      "Epoch  4 , Batch  51 / 62\n",
      "Epoch  4 , Batch  52 / 62\n",
      "Epoch  4 , Batch  53 / 62\n",
      "Epoch  4 , Batch  54 / 62\n",
      "Epoch  4 , Batch  55 / 62\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  4 , Batch  56 / 62\n",
      "Epoch  4 , Batch  57 / 62\n",
      "Epoch  4 , Batch  58 / 62\n",
      "Epoch  4 , Batch  59 / 62\n",
      "Epoch  4 , Batch  60 / 62\n",
      "Epoch  4 , Batch  61 / 62\n",
      "Progress:  tensor(nan, grad_fn=<AddBackward0>) tensor(nan, grad_fn=<AddBackward0>) tensor(nan, grad_fn=<AddBackward0>)\n",
      "Epoch  5 / 10\n",
      "Epoch  5 , Batch  0 / 62\n",
      "Epoch  5 , Batch  1 / 62\n",
      "Epoch  5 , Batch  2 / 62\n",
      "Epoch  5 , Batch  3 / 62\n",
      "Epoch  5 , Batch  4 / 62\n",
      "Epoch  5 , Batch  5 / 62\n",
      "Epoch  5 , Batch  6 / 62\n",
      "Epoch  5 , Batch  7 / 62\n",
      "Epoch  5 , Batch  8 / 62\n",
      "Epoch  5 , Batch  9 / 62\n",
      "Epoch  5 , Batch  10 / 62\n",
      "Epoch  5 , Batch  11 / 62\n",
      "Epoch  5 , Batch  12 / 62\n",
      "Epoch  5 , Batch  13 / 62\n",
      "Epoch  5 , Batch  14 / 62\n",
      "Epoch  5 , Batch  15 / 62\n",
      "Epoch  5 , Batch  16 / 62\n",
      "Epoch  5 , Batch  17 / 62\n",
      "Epoch  5 , Batch  18 / 62\n",
      "Epoch  5 , Batch  19 / 62\n",
      "Epoch  5 , Batch  20 / 62\n",
      "Epoch  5 , Batch  21 / 62\n",
      "Epoch  5 , Batch  22 / 62\n",
      "Epoch  5 , Batch  23 / 62\n",
      "Epoch  5 , Batch  24 / 62\n",
      "Epoch  5 , Batch  25 / 62\n",
      "Epoch  5 , Batch  26 / 62\n",
      "Epoch  5 , Batch  27 / 62\n",
      "Epoch  5 , Batch  28 / 62\n",
      "Epoch  5 , Batch  29 / 62\n",
      "Epoch  5 , Batch  30 / 62\n",
      "Epoch  5 , Batch  31 / 62\n",
      "Epoch  5 , Batch  32 / 62\n",
      "Epoch  5 , Batch  33 / 62\n",
      "Epoch  5 , Batch  34 / 62\n",
      "Epoch  5 , Batch  35 / 62\n",
      "Epoch  5 , Batch  36 / 62\n",
      "Epoch  5 , Batch  37 / 62\n",
      "Epoch  5 , Batch  38 / 62\n",
      "Epoch  5 , Batch  39 / 62\n",
      "Epoch  5 , Batch  40 / 62\n",
      "Epoch  5 , Batch  41 / 62\n",
      "Epoch  5 , Batch  42 / 62\n",
      "Epoch  5 , Batch  43 / 62\n",
      "Epoch  5 , Batch  44 / 62\n",
      "Epoch  5 , Batch  45 / 62\n",
      "Epoch  5 , Batch  46 / 62\n",
      "Epoch  5 , Batch  47 / 62\n",
      "Epoch  5 , Batch  48 / 62\n",
      "Epoch  5 , Batch  49 / 62\n",
      "Epoch  5 , Batch  50 / 62\n",
      "Epoch  5 , Batch  51 / 62\n",
      "Epoch  5 , Batch  52 / 62\n",
      "Epoch  5 , Batch  53 / 62\n",
      "Epoch  5 , Batch  54 / 62\n",
      "Epoch  5 , Batch  55 / 62\n",
      "Epoch  5 , Batch  56 / 62\n",
      "Epoch  5 , Batch  57 / 62\n",
      "Epoch  5 , Batch  58 / 62\n",
      "Epoch  5 , Batch  59 / 62\n",
      "Epoch  5 , Batch  60 / 62\n",
      "Epoch  5 , Batch  61 / 62\n",
      "Progress:  tensor(nan, grad_fn=<AddBackward0>) tensor(nan, grad_fn=<AddBackward0>) tensor(nan, grad_fn=<AddBackward0>)\n",
      "Epoch  6 / 10\n",
      "Epoch  6 , Batch  0 / 62\n",
      "Epoch  6 , Batch  1 / 62\n",
      "Epoch  6 , Batch  2 / 62\n",
      "Epoch  6 , Batch  3 / 62\n",
      "Epoch  6 , Batch  4 / 62\n",
      "Epoch  6 , Batch  5 / 62\n",
      "Epoch  6 , Batch  6 / 62\n",
      "Epoch  6 , Batch  7 / 62\n",
      "Epoch  6 , Batch  8 / 62\n",
      "Epoch  6 , Batch  9 / 62\n",
      "Epoch  6 , Batch  10 / 62\n",
      "Epoch  6 , Batch  11 / 62\n",
      "Epoch  6 , Batch  12 / 62\n",
      "Epoch  6 , Batch  13 / 62\n",
      "Epoch  6 , Batch  14 / 62\n",
      "Epoch  6 , Batch  15 / 62\n",
      "Epoch  6 , Batch  16 / 62\n",
      "Epoch  6 , Batch  17 / 62\n",
      "Epoch  6 , Batch  18 / 62\n",
      "Epoch  6 , Batch  19 / 62\n",
      "Epoch  6 , Batch  20 / 62\n",
      "Epoch  6 , Batch  21 / 62\n",
      "Epoch  6 , Batch  22 / 62\n",
      "Epoch  6 , Batch  23 / 62\n",
      "Epoch  6 , Batch  24 / 62\n",
      "Epoch  6 , Batch  25 / 62\n",
      "Epoch  6 , Batch  26 / 62\n",
      "Epoch  6 , Batch  27 / 62\n",
      "Epoch  6 , Batch  28 / 62\n",
      "Epoch  6 , Batch  29 / 62\n",
      "Epoch  6 , Batch  30 / 62\n",
      "Epoch  6 , Batch  31 / 62\n",
      "Epoch  6 , Batch  32 / 62\n",
      "Epoch  6 , Batch  33 / 62\n",
      "Epoch  6 , Batch  34 / 62\n",
      "Epoch  6 , Batch  35 / 62\n",
      "Epoch  6 , Batch  36 / 62\n",
      "Epoch  6 , Batch  37 / 62\n",
      "Epoch  6 , Batch  38 / 62\n",
      "Epoch  6 , Batch  39 / 62\n",
      "Epoch  6 , Batch  40 / 62\n",
      "Epoch  6 , Batch  41 / 62\n",
      "Epoch  6 , Batch  42 / 62\n",
      "Epoch  6 , Batch  43 / 62\n",
      "Epoch  6 , Batch  44 / 62\n",
      "Epoch  6 , Batch  45 / 62\n",
      "Epoch  6 , Batch  46 / 62\n",
      "Epoch  6 , Batch  47 / 62\n",
      "Epoch  6 , Batch  48 / 62\n",
      "Epoch  6 , Batch  49 / 62\n",
      "Epoch  6 , Batch  50 / 62\n",
      "Epoch  6 , Batch  51 / 62\n",
      "Epoch  6 , Batch  52 / 62\n",
      "Epoch  6 , Batch  53 / 62\n",
      "Epoch  6 , Batch  54 / 62\n",
      "Epoch  6 , Batch  55 / 62\n",
      "Epoch  6 , Batch  56 / 62\n",
      "Epoch  6 , Batch  57 / 62\n",
      "Epoch  6 , Batch  58 / 62\n",
      "Epoch  6 , Batch  59 / 62\n",
      "Epoch  6 , Batch  60 / 62\n",
      "Epoch  6 , Batch  61 / 62\n",
      "Progress:  tensor(nan, grad_fn=<AddBackward0>) tensor(nan, grad_fn=<AddBackward0>) tensor(nan, grad_fn=<AddBackward0>)\n",
      "Epoch  7 / 10\n",
      "Epoch  7 , Batch  0 / 62\n",
      "Epoch  7 , Batch  1 / 62\n",
      "Epoch  7 , Batch  2 / 62\n",
      "Epoch  7 , Batch  3 / 62\n",
      "Epoch  7 , Batch  4 / 62\n",
      "Epoch  7 , Batch  5 / 62\n",
      "Epoch  7 , Batch  6 / 62\n",
      "Epoch  7 , Batch  7 / 62\n",
      "Epoch  7 , Batch  8 / 62\n",
      "Epoch  7 , Batch  9 / 62\n",
      "Epoch  7 , Batch  10 / 62\n",
      "Epoch  7 , Batch  11 / 62\n",
      "Epoch  7 , Batch  12 / 62\n",
      "Epoch  7 , Batch  13 / 62\n",
      "Epoch  7 , Batch  14 / 62\n",
      "Epoch  7 , Batch  15 / 62\n",
      "Epoch  7 , Batch  16 / 62\n",
      "Epoch  7 , Batch  17 / 62\n",
      "Epoch  7 , Batch  18 / 62\n",
      "Epoch  7 , Batch  19 / 62\n",
      "Epoch  7 , Batch  20 / 62\n",
      "Epoch  7 , Batch  21 / 62\n",
      "Epoch  7 , Batch  22 / 62\n",
      "Epoch  7 , Batch  23 / 62\n",
      "Epoch  7 , Batch  24 / 62\n",
      "Epoch  7 , Batch  25 / 62\n",
      "Epoch  7 , Batch  26 / 62\n",
      "Epoch  7 , Batch  27 / 62\n",
      "Epoch  7 , Batch  28 / 62\n",
      "Epoch  7 , Batch  29 / 62\n",
      "Epoch  7 , Batch  30 / 62\n",
      "Epoch  7 , Batch  31 / 62\n",
      "Epoch  7 , Batch  32 / 62\n",
      "Epoch  7 , Batch  33 / 62\n",
      "Epoch  7 , Batch  34 / 62\n",
      "Epoch  7 , Batch  35 / 62\n",
      "Epoch  7 , Batch  36 / 62\n",
      "Epoch  7 , Batch  37 / 62\n",
      "Epoch  7 , Batch  38 / 62\n",
      "Epoch  7 , Batch  39 / 62\n",
      "Epoch  7 , Batch  40 / 62\n",
      "Epoch  7 , Batch  41 / 62\n",
      "Epoch  7 , Batch  42 / 62\n",
      "Epoch  7 , Batch  43 / 62\n",
      "Epoch  7 , Batch  44 / 62\n",
      "Epoch  7 , Batch  45 / 62\n",
      "Epoch  7 , Batch  46 / 62\n",
      "Epoch  7 , Batch  47 / 62\n",
      "Epoch  7 , Batch  48 / 62\n",
      "Epoch  7 , Batch  49 / 62\n",
      "Epoch  7 , Batch  50 / 62\n",
      "Epoch  7 , Batch  51 / 62\n",
      "Epoch  7 , Batch  52 / 62\n",
      "Epoch  7 , Batch  53 / 62\n",
      "Epoch  7 , Batch  54 / 62\n",
      "Epoch  7 , Batch  55 / 62\n",
      "Epoch  7 , Batch  56 / 62\n",
      "Epoch  7 , Batch  57 / 62\n",
      "Epoch  7 , Batch  58 / 62\n",
      "Epoch  7 , Batch  59 / 62\n",
      "Epoch  7 , Batch  60 / 62\n",
      "Epoch  7 , Batch  61 / 62\n",
      "Progress:  tensor(nan, grad_fn=<AddBackward0>) tensor(nan, grad_fn=<AddBackward0>) tensor(nan, grad_fn=<AddBackward0>)\n",
      "Epoch  8 / 10\n",
      "Epoch  8 , Batch  0 / 62\n",
      "Epoch  8 , Batch  1 / 62\n",
      "Epoch  8 , Batch  2 / 62\n",
      "Epoch  8 , Batch  3 / 62\n",
      "Epoch  8 , Batch  4 / 62\n",
      "Epoch  8 , Batch  5 / 62\n",
      "Epoch  8 , Batch  6 / 62\n",
      "Epoch  8 , Batch  7 / 62\n",
      "Epoch  8 , Batch  8 / 62\n",
      "Epoch  8 , Batch  9 / 62\n",
      "Epoch  8 , Batch  10 / 62\n",
      "Epoch  8 , Batch  11 / 62\n",
      "Epoch  8 , Batch  12 / 62\n",
      "Epoch  8 , Batch  13 / 62\n",
      "Epoch  8 , Batch  14 / 62\n",
      "Epoch  8 , Batch  15 / 62\n",
      "Epoch  8 , Batch  16 / 62\n",
      "Epoch  8 , Batch  17 / 62\n",
      "Epoch  8 , Batch  18 / 62\n",
      "Epoch  8 , Batch  19 / 62\n",
      "Epoch  8 , Batch  20 / 62\n",
      "Epoch  8 , Batch  21 / 62\n",
      "Epoch  8 , Batch  22 / 62\n",
      "Epoch  8 , Batch  23 / 62\n",
      "Epoch  8 , Batch  24 / 62\n",
      "Epoch  8 , Batch  25 / 62\n",
      "Epoch  8 , Batch  26 / 62\n",
      "Epoch  8 , Batch  27 / 62\n",
      "Epoch  8 , Batch  28 / 62\n",
      "Epoch  8 , Batch  29 / 62\n",
      "Epoch  8 , Batch  30 / 62\n",
      "Epoch  8 , Batch  31 / 62\n",
      "Epoch  8 , Batch  32 / 62\n",
      "Epoch  8 , Batch  33 / 62\n",
      "Epoch  8 , Batch  34 / 62\n",
      "Epoch  8 , Batch  35 / 62\n",
      "Epoch  8 , Batch  36 / 62\n",
      "Epoch  8 , Batch  37 / 62\n",
      "Epoch  8 , Batch  38 / 62\n",
      "Epoch  8 , Batch  39 / 62\n",
      "Epoch  8 , Batch  40 / 62\n",
      "Epoch  8 , Batch  41 / 62\n",
      "Epoch  8 , Batch  42 / 62\n",
      "Epoch  8 , Batch  43 / 62\n",
      "Epoch  8 , Batch  44 / 62\n",
      "Epoch  8 , Batch  45 / 62\n",
      "Epoch  8 , Batch  46 / 62\n",
      "Epoch  8 , Batch  47 / 62\n",
      "Epoch  8 , Batch  48 / 62\n",
      "Epoch  8 , Batch  49 / 62\n",
      "Epoch  8 , Batch  50 / 62\n",
      "Epoch  8 , Batch  51 / 62\n",
      "Epoch  8 , Batch  52 / 62\n",
      "Epoch  8 , Batch  53 / 62\n",
      "Epoch  8 , Batch  54 / 62\n",
      "Epoch  8 , Batch  55 / 62\n",
      "Epoch  8 , Batch  56 / 62\n",
      "Epoch  8 , Batch  57 / 62\n",
      "Epoch  8 , Batch  58 / 62\n",
      "Epoch  8 , Batch  59 / 62\n",
      "Epoch  8 , Batch  60 / 62\n",
      "Epoch  8 , Batch  61 / 62\n",
      "Progress:  tensor(nan, grad_fn=<AddBackward0>) tensor(nan, grad_fn=<AddBackward0>) tensor(nan, grad_fn=<AddBackward0>)\n",
      "Epoch  9 / 10\n",
      "Epoch  9 , Batch  0 / 62\n",
      "Epoch  9 , Batch  1 / 62\n",
      "Epoch  9 , Batch  2 / 62\n",
      "Epoch  9 , Batch  3 / 62\n",
      "Epoch  9 , Batch  4 / 62\n",
      "Epoch  9 , Batch  5 / 62\n",
      "Epoch  9 , Batch  6 / 62\n",
      "Epoch  9 , Batch  7 / 62\n",
      "Epoch  9 , Batch  8 / 62\n",
      "Epoch  9 , Batch  9 / 62\n",
      "Epoch  9 , Batch  10 / 62\n",
      "Epoch  9 , Batch  11 / 62\n",
      "Epoch  9 , Batch  12 / 62\n",
      "Epoch  9 , Batch  13 / 62\n",
      "Epoch  9 , Batch  14 / 62\n",
      "Epoch  9 , Batch  15 / 62\n",
      "Epoch  9 , Batch  16 / 62\n",
      "Epoch  9 , Batch  17 / 62\n",
      "Epoch  9 , Batch  18 / 62\n",
      "Epoch  9 , Batch  19 / 62\n",
      "Epoch  9 , Batch  20 / 62\n",
      "Epoch  9 , Batch  21 / 62\n",
      "Epoch  9 , Batch  22 / 62\n",
      "Epoch  9 , Batch  23 / 62\n",
      "Epoch  9 , Batch  24 / 62\n",
      "Epoch  9 , Batch  25 / 62\n",
      "Epoch  9 , Batch  26 / 62\n",
      "Epoch  9 , Batch  27 / 62\n",
      "Epoch  9 , Batch  28 / 62\n",
      "Epoch  9 , Batch  29 / 62\n",
      "Epoch  9 , Batch  30 / 62\n",
      "Epoch  9 , Batch  31 / 62\n",
      "Epoch  9 , Batch  32 / 62\n",
      "Epoch  9 , Batch  33 / 62\n",
      "Epoch  9 , Batch  34 / 62\n",
      "Epoch  9 , Batch  35 / 62\n",
      "Epoch  9 , Batch  36 / 62\n",
      "Epoch  9 , Batch  37 / 62\n",
      "Epoch  9 , Batch  38 / 62\n",
      "Epoch  9 , Batch  39 / 62\n",
      "Epoch  9 , Batch  40 / 62\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  9 , Batch  41 / 62\n",
      "Epoch  9 , Batch  42 / 62\n",
      "Epoch  9 , Batch  43 / 62\n",
      "Epoch  9 , Batch  44 / 62\n",
      "Epoch  9 , Batch  45 / 62\n",
      "Epoch  9 , Batch  46 / 62\n",
      "Epoch  9 , Batch  47 / 62\n",
      "Epoch  9 , Batch  48 / 62\n",
      "Epoch  9 , Batch  49 / 62\n",
      "Epoch  9 , Batch  50 / 62\n",
      "Epoch  9 , Batch  51 / 62\n",
      "Epoch  9 , Batch  52 / 62\n",
      "Epoch  9 , Batch  53 / 62\n",
      "Epoch  9 , Batch  54 / 62\n",
      "Epoch  9 , Batch  55 / 62\n",
      "Epoch  9 , Batch  56 / 62\n",
      "Epoch  9 , Batch  57 / 62\n",
      "Epoch  9 , Batch  58 / 62\n",
      "Epoch  9 , Batch  59 / 62\n",
      "Epoch  9 , Batch  60 / 62\n",
      "Epoch  9 , Batch  61 / 62\n",
      "Progress:  tensor(nan, grad_fn=<AddBackward0>) tensor(nan, grad_fn=<AddBackward0>) tensor(nan, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \n",
    "    NLLLoss = torch.nn.NLLLoss()\n",
    "    nr_src_langs = len(vocabs['src'])\n",
    "    print('Nr source languages:', nr_src_langs)\n",
    "    print('Nr target languages:', len(vocabs['trgt']))\n",
    "    \n",
    "    # Get bilingual dictionary for evaluating train loss or at least testing\n",
    "    dicts = dict()\n",
    "    #TODO\n",
    "\n",
    "    # Set up model architecture\n",
    "    net = gan.GAN(embedding_dim, internal_dim, hidden, languages['src'])\n",
    "\n",
    "    # Get optimizers; 1 per source language and 1 for target language\n",
    "    optims_g = {}\n",
    "    for language in languages['src']:\n",
    "\n",
    "        #params = net.generator.encoders[language].parameters() + net.generator.decoder.parameters()\n",
    "        optims_g[language] = torch.optim.Adam([{'params': net.generator.encoders[language].parameters()},{'params': net.generator.decoder.parameters()}],lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "\n",
    "    optim_d = torch.optim.Adam(net.discriminator.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "\n",
    "    # Train\n",
    "    train_loss_real_d, train_loss_fake_d, train_loss_g = [], [], []\n",
    "    eval_loss = [], []\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch ', epoch, '/', epochs)\n",
    "        loss_real_total_d, loss_fake_total_d, loss_total_g = 0., 0., 0.\n",
    "\n",
    "        # Shuffle data #\n",
    "        # Source languages\n",
    "        for lang in languages['src']:\n",
    "            vocabs['src'][lang]['x'], vocabs['src'][lang]['y'] = shuffle(np.array(vocabs['src'][lang]['x']), np.array(vocabs['src'][lang]['y']))\n",
    "            vocabs['src'][lang]['x'] = torch.from_numpy(vocabs['src'][lang]['x'])\n",
    "            #vocabs['src'][lang]['y'] = torch.from_numpy(vocabs['src'][lang]['y']) # We don't really need that except for eval\n",
    "        # Target language\n",
    "        lang = languages['trgt'] # Retrieve language id\n",
    "        vocabs['trgt'][lang]['x'], vocabs['trgt'][lang]['y'] = shuffle(np.array(vocabs['trgt'][lang]['x']), np.array(vocabs['trgt'][lang]['y']))\n",
    "        vocabs['trgt'][lang]['x'] = torch.from_numpy(vocabs['trgt'][lang]['x'])\n",
    "\n",
    "        # Train #\n",
    "        for batch in range(num_minibatches):\n",
    "            print('Epoch ', epoch, ', Batch ', batch, '/', num_minibatches)\n",
    "\n",
    "            # Update discriminator #\n",
    "            # All-real minibatch\n",
    "            net.discriminator.zero_grad()\n",
    "            x = vocabs['trgt'][languages['trgt']]['x'][batch * batch_size:(batch + 1) * batch_size]#.to(device)\n",
    "            y_true = torch.full((batch_size,), real_label).long() #device=device  # TODO: could probs be converted to long straight away when reading in, already...\n",
    "            y_pred = net.discriminator(x)\n",
    "            # Loss proportional to discriminator's probability of correctly distinguishing TP and FP\n",
    "            loss_real = NLLLoss(torch.log(y_pred), y_true)  # NLLLoss needs log(prob_distribution)\n",
    "            loss_real.backward()\n",
    "            loss_real_total_d += loss_real\n",
    "\n",
    "            # One minibatch per source language\n",
    "            translations = {}\n",
    "            loss_fake_batch_avg = 0.\n",
    "            for language in languages['src']:\n",
    "                # All-real minibatch\n",
    "                net.discriminator.zero_grad()\n",
    "                x = vocabs['src'][language]['x'][batch * batch_size:(batch + 1) * batch_size]#.to(device)\n",
    "                x = net.generator(x, language)\n",
    "                translations[language] = x\n",
    "                y_true = torch.full((batch_size,), fake_label).long() #, device=device\n",
    "                y_pred = net.discriminator(x.detach())      # Detach to avoid computing grads for generator\n",
    "                # Loss proportional to discriminator's probability of correctly distinguishing TP and FP\n",
    "                loss_fake = NLLLoss(torch.log(y_pred), y_true)  # NLLLoss needs log(prob_distribution)\n",
    "                loss_fake_batch_avg += loss_fake\n",
    "                loss_fake.backward()    # Compute gradients only for discriminator\n",
    "            optim_d.step()              # Weight update\n",
    "            loss_fake_total_d += (loss_fake_batch_avg/nr_src_langs)\n",
    "\n",
    "            # Update generator #\n",
    "            # Compute gradients\n",
    "            loss_fake_batch_avg = 0.\n",
    "            for language in languages['src']:\n",
    "                net.generator.encoders[language].zero_grad()\n",
    "                x = translations[language]\n",
    "                y_true = torch.full((batch_size,), real_label).long() #device=device\n",
    "                y_pred = net.discriminator(x)\n",
    "                #loss_fake = net.loss(y_pred, y_true, 'gen')\n",
    "                # Loss proportional to discriminator's probability of confusing TP and FP\n",
    "                loss_fake = NLLLoss(torch.log(y_pred), y_true)\n",
    "                loss_fake_batch_avg += loss_fake\n",
    "                loss_fake.backward()\n",
    "            loss_total_g += (loss_fake_batch_avg / nr_src_langs)\n",
    "\n",
    "            # TODO: possibly average decoder's gradients\n",
    "\n",
    "            # Perform weight updates\n",
    "            for language in languages['src']:\n",
    "                optims_g[language].step()\n",
    "\n",
    "        # Document accumulated losses per epoch\n",
    "        train_loss_real_d.append(loss_real_total_d)\n",
    "        train_loss_fake_d.append(loss_fake_total_d)\n",
    "        train_loss_g.append(loss_total_g)\n",
    "        \n",
    "        print('Progress: ', loss_real_total_d, loss_fake_total_d, loss_total_g)\n",
    "\n",
    "        # TODO: Similarity metric-based evaluation per epoch?\n",
    "\n",
    "    # TODO: Final evaluation\n",
    "\n",
    "    # TODO: Store model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # execute only if run as a script\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
