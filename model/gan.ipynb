{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, D_in, H, D_out, src_languages):\n",
    "        super(GAN, self).__init__()\n",
    "        embedding_dim = D_in\n",
    "        internal_dim = H\n",
    "        hidden = D_out\n",
    "        \n",
    "        self.generator = Generator(embedding_dim, hidden, internal_dim, src_languages)\n",
    "        self.discriminator = Discriminator(embedding_dim, internal_dim, hidden)\n",
    "        \n",
    "        self.NLLLoss = torch.nn.NLLLoss()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        raise Warning('Calling GAN DIRECTLY NOT IMPLEMENTED!')\n",
    "        \n",
    "        \n",
    "    def loss(self, y_pred, targets, net):\n",
    "        \"\"\"\n",
    "            Loss function according to https://arxiv.org/pdf/1710.04087.pdf\n",
    "            \n",
    "        :param y_pred: Per minibatch element, probability distribution over TP (=1=word embedding native\n",
    "                       to target space) and FP (=0=word embedding produced by generator)\n",
    "        :param targets: Per minibatch element, binary indication of TP or FP wrt. to ground-truth\n",
    "        :param net: Loss to be computed for 'gen' vs 'dis'\n",
    "\n",
    "        :return: Loss for generator or discriminator\n",
    "        \"\"\"\n",
    "\n",
    "        if net == 'dis':\n",
    "            # Loss proportional to discriminator's probability of correctly distinguishing TP and FP\n",
    "            loss = self.NLLLoss(torch.log(y_pred), targets)  # NLLLoss needs log(prob_distribution)\n",
    "        else:\n",
    "            # Loss proportional to discriminator's probability of confusing TP and FP\n",
    "            targets_inverse = torch.ones(list(targets.shape)[0]) - targets\n",
    "            loss = self.NLLLoss(torch.log(y_pred), targets_inverse)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, D_in, H, D_out, languages):\n",
    "        super(Generator, self).__init__()\n",
    "        self.D_out = D_out\n",
    "        \n",
    "        embedding_dim = D_in\n",
    "        internal_dim = H\n",
    "        hidden = D_out\n",
    "\n",
    "        self.encoders = {}\n",
    "        for language in languages:\n",
    "            self.encoders[language] = encoder.FeedForwardEncoder(embedding_dim, hidden, internal_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = decoder.FeedForwardDecoder(internal_dim, hidden, embedding_dim)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, batch_language):\n",
    "        \"\"\"\n",
    "            Generator's forward pass.\n",
    "        :param x: Word embedding vectors from a single language space\n",
    "        :param batch_language: Indication from which language's embedding space word embedding vectors are provided in x\n",
    "        :return: Translations of x in target language's embedding space\n",
    "        \"\"\"\n",
    "        x = self.encoders[batch_language](x)  # Feed src-lang's embedding vectors through lang's respective encoder\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, D_in, H, D_out=2):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.w1 = torch.nn.Linear(D_in, H)\n",
    "        self.w2 = torch.nn.Linear(H, D_out)\n",
    "        self.activation = torch.nn.functional.relu\n",
    "        self.softmax = torch.nn.Softmax(dim=1)  # Take SM along dimension=1, whereas dim=0 == Batch-size\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.w1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.w2(x)\n",
    "        x = self.softmax(x)      # Create probability distribution over embedding being TP or FP\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
